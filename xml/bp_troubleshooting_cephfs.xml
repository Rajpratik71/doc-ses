<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="bp-troubleshooting-cephfs">
 <title>Troubleshooting &ceph; File System</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <sect1 xml:id="bp-troubleshooting-slow-ops">
   <title>Slow or Stuck Operations</title>
   <para>
     If you are experiencing apparent hung operations, the first task is
     to identify where the problem is occurring: in the client, the MDS,
     or the network connecting them. Start by looking to see if either
     side has stuck operations and narrow it down from there.
   </para>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-rados-health">
   <title>RADOS Health</title>
   <para>
     If part of the CephFS metadata or data pools is unavailable and
     CephFS is not responding, it is probably because RADOS itself is
     unhealthy. <!-- See RADOS troubleshooting -->
   </para>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-mds">
   <title>The MDS</title>
   <para>
     If an operation is hung inside the MDS, it will eventually show
     up in <command>ceph health</command>, identifying “slow requests are blocked”. It
     may also identify clients as “failing to respond” or misbehaving in
     other ways. If the MDS identifies specific clients as misbehaving,
     you should investigate why they are doing so. Generally, it will
     be the result of:</para>
     <itemizedlist>
       <listitem>
         <para>
           Overloading the system
         </para>
       </listitem>
       <listitem>
         <para>
            Running an older (misbehaving) client
          </para>
        </listitem>
        <listitem>
          <para>
            Underlying RADOS issues
          </para>
        </listitem>
      </itemizedlist>
   <sect2 xml:id="mds-slow-requests">
     <title>MDS Slow Requests</title>
     <para>
       You can list current operations via the admin socket by running
       the following command from the MDS host:
     </para>
<screen>
ceph daemon mds.<replaceable>NAME</replaceable> dump_ops_in_flight
</screen>
     <para>
        Identify the stuck commands and examine why they are stuck.
        Usually the last event will have been an attempt to gather
        locks, or sending the operation off to the MDS log. If it is
        waiting on the OSDs, fix them. If operations are stuck on a
        specific inode, you probably have a client holding caps which
        prevent others from using it. This can be because the client is trying
        to flush out dirty data or because you have encountered a bug
        in CephFS’ distributed file lock code (the file “capabilities”
        [“caps”] system).
     </para>
     <para>
       If it is a result of a bug in the capabilities code, restarting
       the MDS is likely to resolve the problem.
     </para>
     <para>
       If there are no slow requests reported on the MDS, and it is not
       reporting that clients are misbehaving, either the client has
       a problem or its requests are not reaching the MDS.
     </para>
   </sect2>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-disconnected-remounted-fs">
   <title>Disconnected and Remounted File System</title>
   <para>
     Because CephFS has a consistent cache, if your network connection
     is disrupted for a long enough time the client will be forcibly
     disconnected from the system. At this point, the kernel client
     is in a bind: it cannot safely write back dirty data, and many
     applications do not handle IO errors correctly on <literal>close()</literal>.
     At the moment, the kernel client will remount the FS, but
     outstanding filesystem IO may or may not be satisfied. In these
     cases, you may need to reboot your client system.
   </para>
   <para>
     You can identify you are in this situation if <filename>dmesg/kern.log</filename>
     report something like:
   </para>
<screen>
  Jul 20 08:14:38 teuthology kernel: [3677601.123718] ceph: mds0 closed our session
  Jul 20 08:14:38 teuthology kernel: [3677601.128019] ceph: mds0 reconnect start
  Jul 20 08:14:39 teuthology kernel: [3677602.093378] ceph: mds0 reconnect denied
  Jul 20 08:14:39 teuthology kernel: [3677602.098525] ceph:  dropping dirty+flushing Fw state for ffff8802dc150518 1099935956631
  Jul 20 08:14:39 teuthology kernel: [3677602.107145] ceph:  dropping dirty+flushing Fw state for ffff8801008e8518 1099935946707
  Jul 20 08:14:39 teuthology kernel: [3677602.196747] libceph: mds0 172.21.5.114:6812 socket closed (con state OPEN)
  Jul 20 08:14:40 teuthology kernel: [3677603.126214] libceph: mds0 172.21.5.114:6812 connection reset
  Jul 20 08:14:40 teuthology kernel: [3677603.132176] libceph: reset on mds0
</screen>
   <para>
     This is an area of ongoing work to improve the behavior. Kernels will
     soon be reliably issuing error codes to in-progress IO, although
     your application(s) may not deal with them well. In the longer-term,
     we hope to allow reconnect and reclaim of data in cases where
     it will not violate POSIX semantics.
   </para>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-mounting">
   <title>Mounting</title>
   <sect2 xml:id="mount-5-error">
     <title>Mount I/O error</title>
     <para>
       A <literal>mount 5</literal> (EIO, I/O error) error typically occurs if
       a MDS server is laggy or if it crashed. Ensure at least one MDS is up
       and running, and the cluster is <literal>active + healthy</literal>.
     </para>
   </sect2>
   <sect2 xml:id="mount-12-error">
     <title>Mount Out of Memory Error</title>
     <para>
       A <literal>mount 12</literal> error (ENOMEM, out of memory) with
       <literal>cannot allocate memory</literal> usually
       occurs if you have a version mismatch between the &ceph; Client version
       and the &ceph; Storage Cluster version. Check the versions using:
     </para>
<screen>
ceph -v
</screen>
     <para>
       If the &ceph; Client is behind the &ceph; cluster, try to upgrade it:
     </para>
<screen>
sudo zypper up
sudo zypper in ceph-common
</screen>
     <para>
       You may need to uninstall, autoclean and autoremove <command>ceph-common</command>
       and then reinstall it so that you have the latest version.
     </para>
   </sect2>
 </sect1>
</chapter>
