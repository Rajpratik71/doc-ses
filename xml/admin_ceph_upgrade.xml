<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-ceph-upgrade">
 <title>Upgrading from Previous Releases</title>
 <para>
  This chapter introduces steps to upgrade &productname; &prevproductnumber; to
  version &productnumber;. Upgrading from &prevproductnumber; to &productnumber;
  means the following:
 </para>
 <itemizedlist>
   <listitem>
     <para>
       Upgrading from Ceph Nautilus to Octopus.
     </para>
   </listitem>
   <listitem>
     <para>
       Switching from installing and running &ceph; via RPM packages
       to running in containers.
     </para>
   </listitem>
   <listitem>
     <para>
      Complete removal of &deepsea; and replacing with <literal>ceph-salt</literal>
      and &cephadm;.
     </para>
   </listitem>
 </itemizedlist>
 <warning>
   <para>
     The upgrade information in this chapter <emphasis>only</emphasis> applies
     to upgrades from &deepsea; to &cephadm;. Do not attempt to follow these
     instructions if you want to deploy &productname; on the &casp; platform.
   </para>
 </warning>
  <para>
   Upgrading from &productname; versions older than &prevproductnumber; is not
   supported. You first need to upgrade to the latest version of &productname;
   &prevproductnumber; and then follow the steps in this chapter.
  </para>

 <sect1 xml:id="before-upgrade">
   <title>Before Upgrading</title>
   <para>
     Before upgrading, ensure you read through the following sections to
     ensure you understand all tasks that need to be executed.
   </para>
   <sect2 xml:id="upgrade-consider-points">
    <title>Points to Consider</title>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis>Read the release notes</emphasis> - there you can find
       additional information on changes since the previous release of
       &productname;. Check the release notes to see whether:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Your hardware needs special considerations.
        </para>
       </listitem>
       <listitem>
        <para>
         Any used software packages have changed significantly.
        </para>
       </listitem>
       <listitem>
        <para>
         Special precautions are necessary for your installation.
        </para>
       </listitem>
      </itemizedlist>
      <para>
       The release notes also provide information that could not make it into the
       manual on time. They also contain notes about known issues.
      </para>
      <para>
       After having installed the package <package>release-notes-ses</package>,
       find the release notes locally in the directory
       <filename>/usr/share/doc/release-notes</filename> or online at
       <link xlink:href="https://www.suse.com/releasenotes/"/>.
      </para>
     </listitem>
     <listitem>
      <para>
       Your password must be changed to meet &productname; &productnumber;
       requirements. Ensure you change the username and password on
       <emphasis>all</emphasis> initiators as well.
      </para>
     </listitem>
     <listitem>
      <para>
       In case old RBD kernel clients (older than &prevcephos;) are being used,
       refer to <xref linkend="rbd-old-clients-map" />. We recommend upgrading
       old RBD kernel clients if possible.
      </para>
     </listitem>
     <listitem>
      <para>
       The cluster upgrade may take a long time&mdash;approximately the time it
       takes to upgrade one machine multiplied by the number of cluster nodes.
      </para>
     </listitem>
     <listitem>
      <para>
       You have to upgrade the &smaster; first, and replace &deepsea; with
       <literal>ceph-salt</literal> and &cephadm; after.  You will be unable to
       start using the &cephadm; orchestrator module until at least all &mgr;'s
       are upgraded.
      </para>
     </listitem>
     <listitem>
      <para>
       The upgrade from using Nautilus RPMs to Octopus Containers needs to happen all
       in one step. This means upgrade an entire node at a time, not one daemon
       at a time.
      </para>
     </listitem>
     <listitem>
      <para>
        The OSD migration from FileStore to Bluestore <emphasis>needs</emphasis>
        to happen before the upgrade as it is unsupported in &productname;
        &productnumber;. See <xref linkend="filestore2bluestore"/> for
        more information.
      </para>
    </listitem>
    <listitem>
      <para>
        If you are running an older cluster that still uses <literal>ceph-disk</literal>
        OSDs, you <emphasis>need</emphasis> to switch to <literal>ceph-volume</literal> before
        the upgrade.
      </para>
    </listitem>
    </itemizedlist>
  </sect2>

   <sect2 xml:id="upgrade-backup-config-data">
     <title>Backing Up Cluster Configuration and Data</title>
     <para>
       This chapter explains which parts of the &ceph; cluster you should back
       up in before you continue with the upgrade.
     </para>
     <sect3 xml:id="upgrade-backup-ceph">
       <title>Back Up &ceph; Configuration</title>
       <para>
        Back up the <filename>/etc/ceph</filename> directory. It contains crucial
        cluster configuration. You will need the backup of
        <filename>/etc/ceph</filename> for example when you need to replace the
        &adm;.
       </para>
     </sect3>
     <sect3 xml:id="upgrade-backup-salt">
       <title>Back Up &salt; Configuration</title>
       <para>
        You need to back up the <filename>/etc/salt/</filename> directory. It
        contains the &salt; configuration files, for example the &smaster; key and
        accepted client keys.
       </para>
       <para>
        The &salt; files are not strictly required for backing up the &adm;, but
        make redeploying the &salt; cluster easier. If there is no backup of these
        files, the &salt; minions need to be registered again at the new &adm;.
       </para>

       <note>
        <title>Security of the Salt Master Private Key</title>
        <para>
         Make sure that the backup of the &smaster; private key is stored in a safe
         location. The &smaster; key can be used to manipulate all cluster nodes.
        </para>
       </note>

       <para>
        After restoring the <filename>/etc/salt</filename> directory from a backup,
        restart the &salt; services:
       </para>

     <screen>&prompt.smaster;<command>systemctl</command> restart salt-master
     &prompt.smaster;<command>systemctl</command> restart salt-minion</screen>
     </sect3>
     <sect3 xml:id="upgrade-backup-deepsea">
       <title>Back Up &deepsea; Configuration</title>

       <para>
        All files required by &deepsea; are stored in
        <filename>/srv/pillar/</filename>, <filename>/srv/salt/</filename> and
        <filename>/etc/salt/master.d</filename>.
       </para>

       <para>
        If you need to redeploy the &adm;, install the &deepsea; package on the new
        node and move the backed up data back into the directories. &deepsea; can
        then be used again without any further changes being required. Before using
        &deepsea; again, make sure that all &salt; minions are correctly registered
        on the &adm;.
       </para>
     </sect3>
     <sect3 xml:id="upgrade-backup-config-files">
       <title>Back Up Custom Configurations</title>

       <itemizedlist>
        <listitem>
         <para>
          &prometheus; data and customization.
         </para>
        </listitem>
        <listitem>
         <para>
          &grafana; customization.
         </para>
        </listitem>
        <listitem>
         <para>
          Verify that you have a record of existing &oa; users so that you can
          create new accounts for these users in the &dashboard;.
         </para>
        </listitem>
        <listitem>
         <para>
          Manual changes to <filename>ceph.conf</filename> outside of &deepsea;.
         </para>
        </listitem>
        <listitem>
         <para>
          Manual changes to the &iscsi; configuration outside of &deepsea;.
         </para>
        </listitem>
        <listitem>
         <para>
          &ceph; keys.
         </para>
        </listitem>
        <listitem>
         <para>
          &crushmap; and CRUSH rules. Save the decompiled &crushmap; including CRUSH
          rules into <filename>crushmap-backup.txt</filename> by running the
          following command:
         </para>
     <screen>&prompt.cephuser;ceph osd getcrushmap | crushtool -d - -o crushmap-backup.txt</screen>
        </listitem>
        <listitem>
         <para>
          &sgw; configuration. If you are using a single gateway, backup
          <filename>/etc/samba/smb.conf</filename>. If you are using HA setup,
          backup also CTDB and Pacemaker configuration files. Refer to
          <xref linkend="cha-ses-cifs"/> for details on what configuration is used
          by &sgw;s.
         </para>
        </listitem>
        <listitem>
         <para>
          &ganesha; configuration. Only needed when using HA setup. Refer to
          <xref linkend="cha-ceph-nfsganesha"/> for details on what configuration is
          used by &ganesha;.
         </para>
        </listitem>
       </itemizedlist>
     </sect3>
   </sect2>

   <sect2 xml:id="verify-previous-upgrade">
     <title>Verify Steps From Previous Upgrade</title>
     <para>
      In case you previously upgraded from version 5, verify that the upgrade to
      version 6 was completed successfully:
     </para>
       <para>
        Check for the existence of the
        <filename>/srv/salt/ceph/configuration/files/ceph.conf.import</filename>
        file.
      </para>
       <para>
        This file is created by the engulf process during the upgrade from &productname; 5 to 6.
        The <option>configuration_init: default-import</option> option is
        set in <filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename>.
      </para>
       <para>
        If <option>configuration_init</option> is still set to
        <option>default-import</option>, the cluster is using
        <filename>ceph.conf.import</filename> as its configuration file and not
        &deepsea;'s default <filename>ceph.conf</filename> which is compiled
        from files in <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/</filename>.
      </para>
       <para>
        Therefore, you need to inspect <filename>ceph.conf.import</filename> for
        any custom configuration, and possibly move the configuration to one of
        the files in <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/</filename>.
      </para>
       <para>
        Then remove the <option>configuration_init: default-import</option> line
        from <filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename>.
       </para>


<!-- Notes: This section should also contain information on:
1. installing latest SES 6 updates (before check upgrade steps)
2. Checking the cluster state (before check upgrade steps)
3. Ensuring you have enough disk space -->
   </sect2>
 </sect1>

 <sect1 xml:id="upgrade-salt-master">
   <title>Upgrading &smaster;</title>
 </sect1>

 <sect1 xml:id="upgrade-mon-mgr-nodes">
   <title>Upgrading the &mon; and &mgr; Nodes</title>
 </sect1>

 <sect1 xml:id="upgrade-osd-nodes">
   <title>Upgrading the OSD Nodes</title>
 </sect1>

 <sect1 xml:id="upgrade-gateways">
   <title>Upgrading the Gateways</title>

   <sect2 xml:id="upgrade-mds">
     <title>Upgrading the &mds;</title>
   </sect2>

   <sect2 xml:id="upgrade-igw">
     <title>Upgrading the &igw;</title>
   </sect2>

   <sect2 xml:id="upgrade-ogw">
     <title>Upgrading the &ogw;</title>
   </sect2>

   <sect2 xml:id="upgrade-ganesha">
     <title>Upgrading &ganesha;</title>
     <para>
       The following demonstrates how to migrate an existing &ganesha; service
       running &ceph; Nautilus to an &ganesha; container running &ceph; Octopus.
     </para>
     <warning>
       <para>
         The following documentation requires you to have already successfully
         upgraded the core &ceph; services.
       </para>
     </warning>
     <para>
       &ganesha; stores additional per-daemon config and export configs in a
       &rados; pool. The configured &rados; pool can be found on the <literal>watch_url</literal>
       line of the <literal>RADOS_URLS</literal> block in the <filename>ganesha.conf</filename>
       file.
     </para>
     <para>
       Before attempting any migration, we strongly recommend to make a copy
       of the export and daemon config objects located in the RADOS pool
       To locate the configured RADOS pool, run the following command:
     </para>
<screen>&prompt.cephuser;grep -A5 RADOS_URLS /etc/ganesha/ganesha.conf</screen>
     <para>
       To list the contents of the RADOS pool:
     </para>
<screen>&prompt.cephuser;rados --pool cephfs_data --namespace ganesha ls | sort
  conf-node3
  export-1
  export-2
  export-3
  export-4</screen>
     <para>
       To copy the RADOS objects:
     </para>
<screen>&prompt.cephuser;RADOS_ARGS="--pool cephfs_data --namespace ganesha"
&prompt.cephuser;OBJS=$(rados $RADOS_ARGS ls)
&prompt.cephuser;for obj in $OBJS; do rados $RADOS_ARGS get $obj $obj; done
&prompt.cephuser;ls -lah
total 40K
drwxr-xr-x 2 root root 4.0K Sep 8 03:30 .
drwx------ 9 root root 4.0K Sep 8 03:23 ..
-rw-r--r-- 1 root root 90 Sep 8 03:30 conf-node2
-rw-r--r-- 1 root root 90 Sep 8 03:30 conf-node3
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-1
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-2
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-3
-rw-r--r-- 1 root root 358 Sep 8 03:30 export-4</screen>
     <para>
       On a per node basis, an existing &ganesha; service needs to be stopped
       and then replaced with a container managed by &cephadm;.
     </para>
     <procedure>
       <step>
         <para>
           Stop and disable the existing &ganesha; service:
         </para>
<screen>&prompt.cephuser;systemctl stop nfs-ganesha
&prompt.cephuser;systemctl disable nfs-ganesha
</screen>
       </step>
       <step>
         <para>
           After the existing &ganesha; service has been stopped, a new one
           can be deployed in a container using &cephadm;. To do so, you need
           to create a service specification that contains a <literal>service_id</literal>
           that will be used to identify this new NFS cluster, the hostname of
           the node we are migrating listed as a host in the placement specification,
           and the RADOS pool and namespace that contains the configured NFS
           export objects.
         </para>
         <para>
           Deploy a new &ganesha; container by creating a new placement
           specification. For information on creating a placement specification
           see <xref linkend="cephadm-service-and-placement-specs"/>.
         </para>
       </step>
       <step>
         <para>
           Apply the placement specification:
         </para>
<screen>&prompt.cephuser;ceph orch apply -i <replaceable>FILENAME</replaceable>.yaml</screen>
       </step>
       <step>
         <para>
           Confirm the &ganesha; daemon is running on the host:
         </para>
<screen>&prompt.cephuser;ceph orch ps --daemon_type nfs
NAME           HOST   STATUS         REFRESHED  AGE  VERSION  IMAGE NAME                                                            IMAGE ID      CONTAINER ID
nfs.foo.node2  node2  running (26m)  8m ago     27m  3.3      registry.suse.de/devel/storage/7.0/containers/ses/7/ceph/ceph:latest  8b4be7c42abd  c8b75d7c8f0d</screen>
       </step>
     </procedure>
     <para>
       The existing exports can be migrated in two different ways:
     </para>
     <itemizedlist>
       <listitem>
         <para>
           Manually re-created or re-assigned using the &dashboard;.
         </para>
<!-- depends on https://github.com/ceph/ceph/pull/36948 -->
       </listitem>
       <listitem>
         <para>
           Manually copy the contents of each per-daemon &rados; object into the
           newly created &ganesha; common configuration.
         </para>
       </listitem>
     </itemizedlist>
     <procedure>
       <title>Manually Copying Exports to &ganesha; Common Configuration File</title>
       <step>
         <para>
           Determine the list of per-daemon RADOS objects:
         </para>
<screen>&prompt.cephuser;RADOS_ARGS="--pool cephfs_data --namespace ganesha"
&prompt.cephuser;DAEMON_OBJS=$(rados $RADOS_ARGS ls | grep 'conf-')</screen>
       </step>
       <step>
         <para>
           Make a copy of the per-daemon &rados; objects:
         </para>
<screen>&prompt.cephuser;for obj in $DAEMON_OBJS; do rados $RADOS_ARGS get $obj $obj; done
&prompt.cephuser;ls -lah
total 20K
drwxr-xr-x 2 root root 4.0K Sep 8 16:51 .
drwxr-xr-x 3 root root 4.0K Sep 8 16:47 ..
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-nfs.foo
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-node2
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-node3</screen>
       </step>
       <step>
         <para>
           Sort and merge into a single list of exports:
         </para>
<screen>&prompt.cephuser;cat conf-* | sort -u > conf-nfs.foo
&prompt.cephuser;cat conf-nfs.foo
%url "rados://cephfs_data/ganesha/export-1"
%url "rados://cephfs_data/ganesha/export-2"
%url "rados://cephfs_data/ganesha/export-3"
%url "rados://cephfs_data/ganesha/export-4"</screen>
       </step>
       <step>
         <para>
           Write the new &ganesha; common configuration file:
         </para>
<screen>&prompt.cephuser;rados $RADOS_ARGS put conf-nfs.foo conf-nfs.foo</screen>
       </step>
       <step>
         <para>
           Notify the &ganesha; daemon:
         </para>
<screen>&prompt.cephuser;rados $RADOS_ARGS notify conf-nfs.foo conf-nfs.foo</screen>
         <note>
           <para>
             This action will cause the daemon to reload the configuration.
           </para>
         </note>
       </step>
     </procedure>
     <para>
       After the service has been successfully migrated, the Nautilus based
       &ganesha; service can be removed.
     </para>
     <procedure>
       <step>
         <para>
           Remove &ganesha;:
         </para>
<screen>&prompt.cephuser;zypper rm nfs-ganesha
Reading installed packages...
Resolving package dependencies...
The following 5 packages are going to be REMOVED:
  nfs-ganesha nfs-ganesha-ceph nfs-ganesha-rados-grace nfs-ganesha-rados-urls nfs-ganesha-rgw
5 packages to remove.
After the operation, 308.9 KiB will be freed.
Continue? [y/n/v/...? shows all options] (y): y
(1/5) Removing nfs-ganesha-ceph-2.8.3+git0.d504d374e-3.3.1.x86_64 .................................................................................................................................................................................................................................................................................................[done]
(2/5) Removing nfs-ganesha-rgw-2.8.3+git0.d504d374e-3.3.1.x86_64 ..................................................................................................................................................................................................................................................................................................[done]
(3/5) Removing nfs-ganesha-rados-urls-2.8.3+git0.d504d374e-3.3.1.x86_64 ...........................................................................................................................................................................................................................................................................................[done]
(4/5) Removing nfs-ganesha-rados-grace-2.8.3+git0.d504d374e-3.3.1.x86_64 ..........................................................................................................................................................................................................................................................................................[done]
(5/5) Removing nfs-ganesha-2.8.3+git0.d504d374e-3.3.1.x86_64 ......................................................................................................................................................................................................................................................................................................[done]
Additional rpm output:
warning: /etc/ganesha/ganesha.conf saved as /etc/ganesha/ganesha.conf.rpmsave</screen>
       </step>
       <step>
         <para>
           Remove the legacy cluster settings from the &dashboard;:
         </para>
<screen>&prompt.cephuser;ceph dashboard get-ganesha-clusters-rados-pool-namespace
&prompt.cephuser;ceph dashboard reset-ganesha-clusters-rados-pool-namespace
&prompt.cephuser;ceph dashboard get-ganesha-clusters-rados-pool-namespace
&prompt.cephuser;ceph dashboard set-ganesha-clusters-rados-pool-namespace cluster2:pool2/ns2
</screen>
       </step>
     </procedure>
   </sect2>
 </sect1>

 <sect1 xml:id="upgrade-post-cleanup">
   <title>Post-upgrade Clean-up</title>
<screen>
  # ceph versions
  # ceph osd require-osd-release octopus
  # ceph mgr module enable pg_autoscaler
  # ceph mgr module enable telemetry
  # ceph telemetry on
  # ceph osd set require-min-compat-client luminous
  # ceph balancer mode upmap
  # ceph balancer on
  [...]
</screen>
 </sect1>


<!-- I think this can go somewhere better in the admin guide -->

 <sect1 xml:id="filestore2bluestore">
 <title>OSD Migration to &bluestore;</title>

 <para>
  OSD &bluestore; is a new back-end for the OSD daemons. It is the default
  option since &productname; 5. Compared to &filestore;, which stores objects
  as files in an XFS file system, &bluestore; can deliver increased
  performance because it stores objects directly on the underlying block
  device. &bluestore; also enables other features, such as built-in
  compression and EC overwrites, that are unavailable with &filestore;.
 </para>

 <para>
  Specifically for &bluestore;, an OSD has a 'wal' (Write Ahead Log) device
  and a 'db' (RocksDB database) device. The RocksDB database holds the
  metadata for a &bluestore; OSD. These two devices will reside on the same
  device as an OSD by default, but either can be placed on different, for
  example faster, media.
 </para>

 <para>
  In &productname; 5, both &filestore; and &bluestore; are supported and it is
  possible for &filestore; and &bluestore; OSDs to co-exist in a single
  cluster. During the &productname; upgrade procedure, &filestore; OSDs are
  not automatically converted to &bluestore;. Be aware that the
  &bluestore;-specific features will not be available on OSDs that have not
  been migrated to &bluestore;.
 </para>

 <para>
  Before converting to &bluestore;, the OSDs need to be running &productname;
  5. The conversion is a slow process as all data gets re-written twice.
  Though the migration process can take a long time to complete, there is no
  cluster outage and all clients can continue accessing the cluster during
  this period. However, do expect lower performance for the duration of the
  migration. This is caused by rebalancing and backfilling of cluster data.
 </para>

 <para>
  Use the following procedure to migrate &filestore; OSDs to &bluestore;:
 </para>

 <tip>
  <title>Turn Off Safety Measures</title>
  <para>
   &salt; commands needed for running the migration are blocked by safety
   measures. In order to turn these precautions off, run the following
   command:
  </para>
<screen>
&prompt.smaster;salt-run disengage.safety
</screen>
  <para>
   Rebuild the nodes before continuing:
  </para>
<screen>
&prompt.smaster; salt-run rebuild.node <replaceable>TARGET</replaceable>
</screen>
  <para>
   You can also choose to rebuild each node individually. For example:
  </para>
<screen>
&prompt.smaster; salt-run rebuild.node data1.ceph
</screen>
  <para>
   The <literal>rebuild.node</literal> always removes and recreates all OSDs
   on the node.
  </para>
  <important>
   <para>
    If one OSD fails to convert, re-running the rebuild destroys the already
    converted &bluestore; OSDs. Instead of re-running the rebuild, you can
    run:
   </para>
<screen>
&prompt.smaster;salt-run disks.deploy <replaceable>TARGET</replaceable>
</screen>
  </important>
 </tip>

 <para>
  After the migration to &bluestore;, the object count will remain the same
  and disk usage will be nearly the same.
 </para>
</sect1>

</chapter>
