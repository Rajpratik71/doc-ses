<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-ceph-upgrade">
 <title>Upgrading from Previous Releases</title>
 <para>
  This chapter introduces steps to upgrade &productname; &prevproductnumber; to
  version &productnumber;. Upgrading from &prevproductnumber; to &productnumber;
  means the following:
 </para>
 <itemizedlist>
   <listitem>
     <para>
       Upgrading from Ceph Nautilus to Octopus.
     </para>
   </listitem>
   <listitem>
     <para>
       Switching from installing and running &ceph; via RPM packages
       to running in containers.
     </para>
   </listitem>
   <listitem>
     <para>
      Complete removal of &deepsea; and replacing with <literal>ceph-salt</literal>
      and &cephadm;.
     </para>
   </listitem>
 </itemizedlist>

 <note>
  <title>Upgrade from Older Releases Not Supported</title>
  <para>
   Upgrading from &productname; versions older than &prevproductnumber; is not
   supported. You first need to upgrade to the latest version of &productname;
   &prevproductnumber; and then follow the steps in this chapter.
  </para>
 </note>

 <sect1 xml:id="before-upgrade">
   <title>Before Upgrading</title>
   <para>
     Before upgrading, ensure you read through the following sections to
     ensure you understand all tasks that need to be executed.
   </para>
   <sect2 xml:id="upgrade-consider-points">
    <title>Points to Consider</title>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis>Read the release notes</emphasis> - there you can find
       additional information on changes since the previous release of
       &productname;. Check the release notes to see whether:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Your hardware needs special considerations.
        </para>
       </listitem>
       <listitem>
        <para>
         Any used software packages have changed significantly.
        </para>
       </listitem>
       <listitem>
        <para>
         Special precautions are necessary for your installation.
        </para>
       </listitem>
      </itemizedlist>
      <para>
       The release notes also provide information that could not make it into the
       manual on time. They also contain notes about known issues.
      </para>
      <para>
       After having installed the package <package>release-notes-ses</package>,
       find the release notes locally in the directory
       <filename>/usr/share/doc/release-notes</filename> or online at
       <link xlink:href="https://www.suse.com/releasenotes/"/>.
      </para>
     </listitem>
     <listitem>
      <para>
       Your password must be changed to meet &productname; &productnumber;
       requirements. Ensure you change the username and password on
       <emphasis>all</emphasis> initiators as well.
      </para>
     </listitem>

  <!-- will need swiftgist to review the above and ensure these are still the steps
  to verify upgrade from 5 -> 6 -->
     <listitem>
      <para>
       In case old RBD kernel clients (older than &prevcephos;) are being used,
       refer to <xref linkend="rbd-old-clients-map" />. We recommend upgrading
       old RBD kernel clients if possible.
      </para>
     </listitem>
     <listitem>
      <para>
       The cluster upgrade may take a long time&mdash;approximately the time it
       takes to upgrade one machine multiplied by the number of cluster nodes.
      </para>
  <!-- verify this -->
     </listitem>
     <listitem>
      <para>
       You have to upgrade the &smaster; first, and replace &deepsea; with
       <literal>ceph-salt</literal> and &cephadm; after.  You will be unable to
       start using the &cephadm; orchestrator module until at least all &mgr;'s
       are upgraded.
      </para>
     </listitem>
     <listitem>
      <para>
       The upgrade from Nautilus RPMs to Octopus Containers needs to happen all
       in one step. This means upgrade an entire node at a time, not one daemon
       at a time.
      </para>
     </listitem>
     <listitem>
      <para>
        The OSD migration from FileStore to Bluestore <emphasis>needs</emphasis>
        to happen before the upgrade as it is unsupported in &productname;
        &productnumber;. See <xref linkend="filestore2bluestore"/> for
        more information.
      </para>
    </listitem>
    </itemizedlist>
  </sect2>

   <sect2 xml:id="upgrade-backup-config-files">
     <title>Backup Config Files</title>
   </sect2>

   <sect2 xml:id="verify-previous-upgrade">
     <title>Verify Steps From Previous Upgrade</title>
     <para>
      In case you previously upgraded from version 5, verify that the upgrade to
      version 6 was completed successfully:
     </para>
     <note>
       <para>
         If you are running an older cluster tha tstill uses <literal>ceph-disk</literal>
         OSDs, we recommend you switch to <literal>ceph-volume</literal> either
         before or during the upgrade.
       </para>
     </note>
       <para>
        Check for the existence of the file
       </para>
 <screen>/srv/salt/ceph/configuration/files/ceph.conf.import</screen>
       <para>
        It is created by the engulf process during the upgrade from SES 5 to 6.
        Also, the <option>configuration_init: default-import</option> option is
        set in the file
       </para>
 <screen>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</screen>
       <para>
        If <option>configuration_init</option> is still set to
        <option>default-import</option>, the cluster is using
        <filename>ceph.conf.import</filename> as its configuration file and not
        &deepsea;'s default <filename>ceph.conf</filename> which is compiled
        from files in
       </para>
 <screen>/srv/salt/ceph/configuration/files/ceph.conf.d/</screen>
       <para>
        Therefore you need to inspect <filename>ceph.conf.import</filename> for
        any custom configuration, and possibly move the configuration to one of
        the files in
       </para>
 <screen>/srv/salt/ceph/configuration/files/ceph.conf.d/</screen>
       <para>
        Then remove the <option>configuration_init: default-import</option> line
        from
       </para>
 <screen>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</screen>


<!-- Notes: This section should also contain information on:
1. installing latest SES 6 updates (before check upgrade steps)
2. Checking the cluster state (before check upgrade steps)
3. Ensuring you have enough disk space -->
   </sect2>
 </sect1>

 <sect1 xml:id="upgrade-salt-master">
   <title>Upgrading &smaster;</title>
 </sect1>

 <sect1 xml:id="upgrade-mon-mgr-nodes">
   <title>Upgrading the &mon; and &mgr; Nodes</title>
 </sect1>

 <sect1 xml:id="upgrade-osd-nodes">
   <title>Upgrading the OSD Nodes</title>
 </sect1>

 <sect1 xml:id="upgrade-gateways">
   <title>Upgrading the Gateways</title>

   <sect2 xml:id="upgrade-mds">
     <title>Upgrading the &mds;</title>
   </sect2>

   <sect2 xml:id="upgrade-igw">
     <title>Upgrading the &igw;</title>
   </sect2>

   <sect2 xml:id="upgrade-ogw">
     <title>Upgrading the &ogw;</title>
   </sect2>

   <sect2 xml:id="upgrade-ganesha">
     <title>Upgrading &ganesha;</title>
   </sect2>

 </sect1>

 <sect1 xml:id="upgrade-post-cleanup">
   <title>Post-upgrade Clean-up</title>
<screen>
  # ceph versions
  # ceph osd require-osd-release octopus
  # ceph mgr module enable pg_autoscaler
  # ceph mgr module enable telemetry
  # ceph telemetry on
  # ceph osd set require-min-compat-client luminous
  # ceph balancer mode upmap
  # ceph balancer on
  [...]
</screen>
 </sect1>


<!-- I think this can go somewhere better in the admin guide -->

 <sect1 xml:id="filestore2bluestore">
 <title>OSD Migration to &bluestore;</title>

 <para>
  OSD &bluestore; is a new back-end for the OSD daemons. It is the default
  option since &productname; 5. Compared to &filestore;, which stores objects
  as files in an XFS file system, &bluestore; can deliver increased
  performance because it stores objects directly on the underlying block
  device. &bluestore; also enables other features, such as built-in
  compression and EC overwrites, that are unavailable with &filestore;.
 </para>

 <para>
  Specifically for &bluestore;, an OSD has a 'wal' (Write Ahead Log) device
  and a 'db' (RocksDB database) device. The RocksDB database holds the
  metadata for a &bluestore; OSD. These two devices will reside on the same
  device as an OSD by default, but either can be placed on different, for
  example faster, media.
 </para>

 <para>
  In &productname; 5, both &filestore; and &bluestore; are supported and it is
  possible for &filestore; and &bluestore; OSDs to co-exist in a single
  cluster. During the &productname; upgrade procedure, &filestore; OSDs are
  not automatically converted to &bluestore;. Be aware that the
  &bluestore;-specific features will not be available on OSDs that have not
  been migrated to &bluestore;.
 </para>

 <para>
  Before converting to &bluestore;, the OSDs need to be running &productname;
  5. The conversion is a slow process as all data gets re-written twice.
  Though the migration process can take a long time to complete, there is no
  cluster outage and all clients can continue accessing the cluster during
  this period. However, do expect lower performance for the duration of the
  migration. This is caused by rebalancing and backfilling of cluster data.
 </para>

 <para>
  Use the following procedure to migrate &filestore; OSDs to &bluestore;:
 </para>

 <tip>
  <title>Turn Off Safety Measures</title>
  <para>
   &salt; commands needed for running the migration are blocked by safety
   measures. In order to turn these precautions off, run the following
   command:
  </para>
<screen>
&prompt.smaster;salt-run disengage.safety
</screen>
  <para>
   Rebuild the nodes before continuing:
  </para>
<screen>
&prompt.smaster; salt-run rebuild.node <replaceable>TARGET</replaceable>
</screen>
  <para>
   You can also choose to rebuild each node individually. For example:
  </para>
<screen>
&prompt.smaster; salt-run rebuild.node data1.ceph
</screen>
  <para>
   The <literal>rebuild.node</literal> always removes and recreates all OSDs
   on the node.
  </para>
  <important>
   <para>
    If one OSD fails to convert, re-running the rebuild destroys the already
    converted &bluestore; OSDs. Instead of re-running the rebuild, you can
    run:
   </para>
<screen>
&prompt.smaster;salt-run disks.deploy <replaceable>TARGET</replaceable>
</screen>
  </important>
 </tip>

 <para>
  After the migration to &bluestore;, the object count will remain the same
  and disk usage will be nearly the same.
 </para>
</sect1>

</chapter>
