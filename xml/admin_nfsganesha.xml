<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
 xml:id="cha-ceph-nfsganesha">
<!-- ============================================================== -->
 <title>&ganesha;</title>
 <para>
  &ganesha; is an NFS server (refer to
  <link xlink:href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-admin/#cha-nfs">Sharing
  File Systems with NFS</link> ) that runs in a user address space instead of
  as part of the operating system kernel. With &ganesha;, you can plug in your
  own storage mechanism&mdash;such as &ceph;&mdash;and access it from any NFS
  client. For installation instructions, see
  <xref linkend ="deploy-cephadm-day2-service-nfs"/>.
 </para>
 <note>
  <title>&ganesha; Performance</title>
  <para>
   Because of increased protocol overhead and additional latency caused by
   extra network hops between the client and the storage, accessing &ceph; via
   an NFS Gateway may significantly reduce application performance when
   compared to native &cephfs; or &ogw; clients.
  </para>
 </note>
 <para>
   Each &ganesha; service consist of a configuration hierarchy that contains:
 </para>
 <itemizedlist>
   <listitem>
     <para>
       A bootstrap configuration
     </para>
   </listitem>
   <listitem>
     <para>
       A per-daemon or per-service configuration
     </para>
   </listitem>
   <listitem>
     <para>
       An export configuration
     </para>
   </listitem>
 </itemizedlist>
 <para>
   The bootstrap configuration is the minimal configuration to start the
   <literal>nfs-ganesha</literal> daemon within a container. Each bootstrap
   configuration will contain a <literal>%url</literal> directive that includes
   any additional configuration from the &rados; common configuration object.
   The common configuration object can include additional <literal>%url</literal>
   directives for each of the NFS exports defined in the export &rados;
   configuration objects.
 </para>

 <sect1 xml:id="ceph-nfsganesha-services">
  <title>Starting or Restarting &ganesha;</title>
  <para>
   To start the &ganesha; service, run:
  </para>
 <screen>&prompt.cephuser;ceph orch start nfs.<replaceable>SERVICE_ID</replaceable></screen>
  <para>
   To restart the entire &ganesha; service, run:
  </para>
 <screen>&prompt.cephuser;ceph orch restart nfs.<replaceable>SERVICE_ID</replaceable></screen>
  <para>
    If you only want to restart a single &ganesha; daemon, run:
  </para>
 <screen>&prompt.cephuser;ceph orch daemon restart nfs.<replaceable>SERVICE_ID</replaceable></screen>
  <para>
   When &ganesha; is started or restarted, it has a grace timeout of 90 seconds
   for NFS v4. During the grace period, new requests from clients are actively
   rejected. Hence, clients may face a slowdown of requests when NFS is in
   grace state.
  </para>
 </sect1>

 <sect1 xml:id="ceph-nfsganesha-createcephfs">
   <title>Create a &cephfs; Volume</title>
   <para>
     Execute the following to create a &cephfs; volume:
   </para>
<screen>&prompt.cephuser;ceph fs volume create <replaceable>VOLUME_NAME</replaceable></screen>
   <para>
     Confirm the volume is active:
   </para>
<screen>&prompt.cephuser;ceph fs status</screen>
 </sect1>

 <sect1 xml:id="ceph-nfsganesha-nfservice">
   <title>Create a NFS Service</title>
   <para>
     Execute the following to create a <literal>nfs-ganesha</literal> service:
   </para>
<screen>
&prompt.cephuser;ceph orch apply nfs <replaceable>SERVICE_NAME</replaceable> --pool <replaceable>POOL_NAME</replaceable> --namespace <replaceable>NAMESPACE_NAME</replaceable>
</screen>
 </sect1>

 <sect1 xml:id="ceph-nfsganesha-list-objects">
   <title>List Objects in the NFS Recovery Pool</title>
   <para>
     Execute the following to list the objects in the NFS recovery pool:
   </para>
<screen>&prompt.cephuser;rados --pool <replaceable>POOL_NAME</replaceable> --namespace <replaceable>NAMESPACE_NAME</replaceable> ls</screen>
 </sect1>

 <sect1 xml:id="ceph-nfsganesha-create-export">
   <title>Create a NFS Export</title>
   <para>
     Execute the following to create a NFS export.
   </para>
   <note>
     <para>
       The FSAL block should be modified to include the desired cephx
       user ID and secret access key.
     </para>
   </note>
<screen>&prompt.cephuser;rados --pool <replaceable>POOL_NAME</replaceable> --namespace <replaceable>NAMESPACE_NAME</replaceable> put <replaceable>export-1</replaceable> <replaceable>export-1</replaceable></screen>
   <para>
     An update may be required for the configured exports to be recognized by
     the &ganesha; service:
   </para>
 <screen>&prompt.cephuser;ceph orch restart nfs.<replaceable>SERVICE_ID</replaceable></screen>
 </sect1>

 <sect1 xml:id="ceph-nfsganesha-mount">
  <title>Mount the NFS Export</title>
  <para>
   To mount the exported NFS share on a client host, run:
  </para>
<screen>&prompt.root;<command>mount</command> -t nfs <replaceable>nfs_ganesha_server_hostname:/ /path/to/local/mountpoint</replaceable></screen>
 </sect1>

 <sect1 xml:id="ceph-nfsganesha-verify">
  <title>Verify the NFS Export</title>
  <para>
   NFS v4 will build a list of exports at the root of a pseudo file system.
   You can verify that the NFS shares are exported by mounting
   <filename>/</filename> of the &ganesha; server node:
  </para>
<screen>&prompt.root;<command>mount</command> -t nfs <replaceable>nfs_ganesha_server_hostname:/ /path/to/local/mountpoint</replaceable>
&prompt.root;<command>ls</command> <replaceable>/path/to/local/mountpoint</replaceable> cephfs rgw</screen>
  <note>
   <title>&ganesha; is v4 Only</title>
   <para>
    By default, &cephadm; will configure an NFS v4 server.
    NFS v4 does not interact with <literal>rpcbind</literal> nor the
    <literal>mountd</literal> daemon. NFS client tools such as
    <command>showmount</command> will not show any configured exports.
   </para>
 </note>
 </sect1>

 <sect1 xml:id="ceph-nfsganesha-customrole">
  <title>Multiple &ganesha; Clusters</title>
  <para>
   Multiple &ganesha; clusters can be defined. This allows for:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Separated &ganesha; clusters for accessing &ogw; and &cephfs;.
    </para>
   </listitem>
   <listitem>
    <para>
     Assigning different &ogw; users to &ganesha; clusters.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   Having different &ogw; users enables &ganesha; clusters to access different S3
   buckets. S3 buckets can be used for access control.
  </para>
  <sect2 xml:id="ganesha-rgw-supported-operations">
   <title>Supported &ogw; Operations</title>
   <para>
    The &ogw; NFS interface supports most operations on files and directories,
    with the following restrictions:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis>Links including symbolic links are not supported.</emphasis>
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>NFS access control lists (ACLs) are not supported.</emphasis>
      Unix user and group ownership and permissions <emphasis>are</emphasis>
      supported.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Directories may not be moved or renamed.</emphasis> You
      <emphasis>may</emphasis> move files between directories.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Only full, sequential write I/O is supported.</emphasis>
      All write operations should be treated as an upload. Many typical I/O
      operations, such as editing files in place, will fail because they
      perform non-sequential stores. There are file utilities that perform
      writes sequentially (for example, some versions of GNU
      <command>tar</command>), but may fail because of infrequent
      non-sequential stores. When mounting via NFS, an application's sequential
      I/O can generally be forced to perform sequential writes to the NFS
      server via synchronous mounting (the <option>-o sync</option> option).
      NFS clients that cannot mount synchronously (for example, Microsoft
      Windows*) will not be able to upload files.
     </para>
    </listitem>
    <listitem>
     <para>
      NFS &ogw; supports read-write operations only for block sizes smaller than
      4 MB.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>

 <sect1 xml:id="ceph-nfsganesha-loglevel">
  <title>Setting the Log Level</title>
  <para>
   You change the default debug level <literal>NIV_EVENT</literal> by editing
   the file <filename>/etc/sysconfig/ganesha</filename>. Replace
   <literal>NIV_EVENT</literal> with <literal>NIV_DEBUG</literal> or
   <literal>NIV_FULL_DEBUG</literal>. Increasing the log verbosity can produce
   large amounts of data in the log files.
  </para>
<screen>OPTIONS="-L /var/log/ganesha/ganesha.log -f /etc/ganesha/ganesha.conf -N NIV_EVENT"</screen>
  <para>
   A restart of the service is required when changing the log level.
  </para>
  <note>
   <para>
    &ganesha; uses &ceph; client libraries to connect to the &ceph; cluster. By
    default, client libraries do not log errors or any other output. To see
    more details about &ganesha; interacting with the &ceph; cluster (for
    example, connection issues details) logging needs to be explicitly defined
    in the <filename>ceph.conf</filename> configuration file under the
    <literal>[client]</literal> section. For example:
   </para>
<screen>[client]
	log_file = "/var/log/ceph/ceph-client.log"</screen>
  </note>
 </sect1>
</chapter>
