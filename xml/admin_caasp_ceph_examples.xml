<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
  xml:id="admin-caasp-ceph-examples">
  <!-- ============================================================== -->
  <title>&ceph; Examples</title>
  <remark>
    Content from: https://rook.io/docs/rook/v1.4/ceph-examples.html
    We don't yet have SES location links for the '.yaml' files. Those can be omitted for now.
  </remark>

  <section xml:id="ceph-examples">
    <title>Ceph Examples</title>
    <para>
      Configuration for Rook and Ceph can be configured in multiple ways
      to provide block devices, shared filesystem volumes or object
      storage in a kubernetes namespace. We have provided several examples
      to simplify storage setup, but remember there are many tunables and
      you will need to decide what settings work for your use case and
      environment.
    </para>
    <para>
      See the
      <emphasis role="strong"><link xlink:href="https://github.com/rook/rook/blob/%7B%7B%20branchName%20%7D%7D/cluster/examples/kubernetes/ceph">example
        yaml files</link></emphasis> folder for all the rook/ceph setup
      example spec files.
    </para>
    <section xml:id="common-resources">
      <title>Common Resources</title>
      <para>
        The first step to deploy Rook is to create the common resources.
        The configuration for these resources will be the same for most
        deployments. The
        <link xlink:href="https://github.com/rook/rook/blob/%7B%7B%20branchName%20%7D%7D/cluster/examples/kubernetes/ceph/common.yaml">common.yaml</link>
        sets these resources up.
      </para>
      <screen>
        kubectl create -f common.yaml
      </screen>
      <para>
        The examples all assume the operator and all Ceph daemons will be
        started in the same namespace. If you want to deploy the operator
        in a separate namespace, see the comments throughout
        <literal>common.yaml</literal>.
      </para>
    </section>
    <section xml:id="operator">
      <title>Operator</title>
      <para>
        After the common resources are created, the next step is to create
        the Operator deployment. Several spec file examples are provided
        in
        <link xlink:href="https://github.com/rook/rook/blob/%7B%7B%20branchName%20%7D%7D/cluster/examples/kubernetes/ceph/">this
          directory</link>:
      </para>
      <itemizedlist spacing="compact">
        <listitem>
          <para>
            <literal>operator.yaml</literal>: The most common settings for
            production deployments
          </para>
          <itemizedlist spacing="compact">
            <listitem>
              <para>
                <literal>kubectl create -f operator.yaml</literal>
              </para>
            </listitem>
          </itemizedlist>
        </listitem>
        <listitem>
          <para>
            <literal>operator-openshift.yaml</literal>: Includes all of
            the operator settings for running a basic Rook cluster in an
            OpenShift environment. You will also want to review the
            <link xlink:href="ceph-openshift.md">OpenShift
              Prerequisites</link> to confirm the settings.
          </para>
          <itemizedlist spacing="compact">
            <listitem>
              <para>
                <literal>oc create -f operator-openshift.yaml</literal>
              </para>
            </listitem>
          </itemizedlist>
        </listitem>
      </itemizedlist>
      <para>
        Settings for the operator are configured through environment
        variables on the operator deployment. The individual settings are
        documented in
        <link xlink:href="https://github.com/rook/rook/blob/%7B%7B%20branchName%20%7D%7D/cluster/examples/kubernetes/ceph/operator.yaml">operator.yaml</link>.
      </para>
    </section>
    <section xml:id="cluster-crd">
      <title>Cluster CRD</title>
      <para>
        Now that your operator is running, let’s create your Ceph storage
        cluster. This CR contains the most critical settings that will
        influence how the operator configures the storage. It is important
        to understand the various ways to configure the cluster. These
        examples represent a very small set of the different ways to
        configure the storage.
      </para>
      <itemizedlist spacing="compact">
        <listitem>
          <para>
            <literal>cluster.yaml</literal>: This file contains common
            settings for a production storage cluster. Requires at least
            three nodes.
          </para>
        </listitem>
        <listitem>
          <para>
            <literal>cluster-test.yaml</literal>: Settings for a test
            cluster where redundancy is not configured. Requires only a
            single node.
          </para>
        </listitem>
        <listitem>
          <para>
            <literal>cluster-on-pvc.yaml</literal>: This file contains
            common settings for backing the Ceph Mons and OSDs by PVs.
            Useful when running in cloud environments or where local PVs
            have been created for Ceph to consume.
          </para>
        </listitem>
        <listitem>
          <para>
            <literal>cluster-with-drive-groups.yaml</literal>: This file
            contains example configurations for creating advanced OSD
            layouts on nodes using Ceph Drive Groups.
            <link xlink:href="Documentation/ceph-cluster-crd.md#storage-selection-via-ceph-drive-groups">See
              docs for more</link>
          </para>
        </listitem>
        <listitem>
          <para>
            <literal>cluster-external</literal>: Connect to an
            <link xlink:href="ceph-cluster-crd.md#external-cluster">external
              Ceph cluster</link> with minimal access to monitor the health
            of the cluster and connect to the storage.
          </para>
        </listitem>
        <listitem>
          <para>
            <literal>cluster-external-management</literal>: Connect to an
            <link xlink:href="ceph-cluster-crd.md#external-cluster">external
              Ceph cluster</link> with the admin key of the external cluster
            to enable remote creation of pools and configure services such
            as an <link xlink:href="ceph-object.md">Object Store</link> or
            a <link xlink:href="ceph-filesystem.md">Shared
              Filesystem</link>.
          </para>
        </listitem>
      </itemizedlist>
      <para>
        See the <link xlink:href="ceph-cluster-crd.md">Cluster CRD</link>
        topic for more details and more examples for the settings.
      </para>
    </section>
    <section xml:id="setting-up-consumable-storage">
      <title>Setting up consumable storage</title>
      <para>
        Now we are ready to setup
        <link xlink:href="https://ceph.com/ceph-storage/block-storage/">block</link>,
        <link xlink:href="https://ceph.com/ceph-storage/file-system/">shared
          filesystem</link> or
        <link xlink:href="https://ceph.com/ceph-storage/object-storage/">object
          storage</link> in the Rook Ceph cluster. These kinds of storage
        are respectively referred to as CephBlockPool, CephFilesystem and
        CephObjectStore in the spec files.
      </para>
      <section xml:id="block-devices">
        <title>Block Devices</title>
        <para>
          Ceph can provide raw block device volumes to pods. Each example
          below sets up a storage class which can then be used to
          provision a block device in kubernetes pods. The storage class
          is defined with
          <link xlink:href="http://docs.ceph.com/docs/master/rados/operations/pools/">a
            pool</link> which defines the level of data redundancy in Ceph:
        </para>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              <literal>storageclass.yaml</literal>: This example
              illustrates replication of 3 for production scenarios and
              requires at least three nodes. Your data is replicated on
              three different kubernetes worker nodes and intermittent or
              long-lasting single node failures will not result in data
              unavailability or loss.
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>storageclass-ec.yaml</literal>: Configures erasure
              coding for data durability rather than replication.
              <link xlink:href="http://docs.ceph.com/docs/master/rados/operations/erasure-code/">Ceph’s
                erasure coding</link> is more efficient than replication so
              you can get high reliability without the 3x replication cost
              of the preceding example (but at the cost of higher
              computational encoding and decoding costs on the worker
              nodes). Erasure coding requires at least three nodes. See
              the
              <link xlink:href="ceph-pool-crd.md#erasure-coded">Erasure
                coding</link> documentation for more details.
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>storageclass-test.yaml</literal>: Replication of 1
              for test scenarios and it requires only a single node. Do
              not use this for applications that store valuable data or
              have high-availability storage requirements, since a single
              node failure can result in data loss.
            </para>
          </listitem>
        </itemizedlist>
        <para>
          The storage classes are found in different sub-directories
          depending on the driver:
        </para>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              <literal>csi/rbd</literal>: The CSI driver for block
              devices. This is the preferred driver going forward.
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>flex</literal>: The flex driver will be deprecated
              in a future release to be determined.
            </para>
          </listitem>
        </itemizedlist>
        <para>
          See the <link xlink:href="ceph-pool-crd.md">Ceph Pool CRD</link>
          topic for more details on the settings.
        </para>
      </section>
      <section xml:id="shared-filesystem">
        <title>Shared Filesystem</title>
        <para>
          Ceph filesystem (CephFS) allows the user to <quote>mount</quote>
          a shared posix-compliant folder into one or more hosts (pods in
          the container world). This storage is similar to NFS shared
          storage or CIFS shared folders, as explained
          <link xlink:href="https://ceph.com/ceph-storage/file-system/">here</link>.
        </para>
        <para>
          File storage contains multiple pools that can be configured for
          different scenarios:
        </para>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              <literal>filesystem.yaml</literal>: Replication of 3 for
              production scenarios. Requires at least three nodes.
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>filesystem-ec.yaml</literal>: Erasure coding for
              production scenarios. Requires at least three nodes.
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>filesystem-test.yaml</literal>: Replication of 1
              for test scenarios. Requires only a single node.
            </para>
          </listitem>
        </itemizedlist>
        <para>
          Dynamic provisioning is possible with the CSI driver. The
          storage class for shared filesystems is found in the
          <literal>csi/cephfs</literal> directory.
        </para>
        <para>
          See the <link xlink:href="ceph-filesystem-crd.md">Shared
            Filesystem CRD</link> topic for more details on the settings.
        </para>
      </section>
      <section xml:id="ceph-examples-object-storage">
        <title>Object Storage</title>
        <para>
          Ceph supports storing blobs of data called objects that support
          HTTP(s)-type get/put/post and delete semantics. This storage is
          similar to AWS S3 storage, for example.
        </para>
        <para>
          Object storage contains multiple pools that can be configured
          for different scenarios:
        </para>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              <literal>object.yaml</literal>: Replication of 3 for
              production scenarios. Requires at least three nodes.
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>object-openshift.yaml</literal>: Replication of 3
              with rgw in a port range valid for OpenShift. Requires at
              least three nodes.
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>object-ec.yaml</literal>: Erasure coding rather
              than replication for production scenarios. Requires at least
              three nodes.
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>object-test.yaml</literal>: Replication of 1 for
              test scenarios. Requires only a single node.
            </para>
          </listitem>
        </itemizedlist>
        <para>
          See the <link xlink:href="ceph-object-store-crd.md">Object Store
            CRD</link> topic for more details on the settings.
        </para>
      </section>
      <section xml:id="object-storage-user">
        <title>Object Storage User</title>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              <literal>object-user.yaml</literal>: Creates a simple object
              storage user and generates credentials for the S3 API
            </para>
          </listitem>
        </itemizedlist>
      </section>
      <section xml:id="object-storage-buckets">
        <title>Object Storage Buckets</title>
        <para>
          The Ceph operator also runs an object store bucket provisioner
          which can grant access to existing buckets or dynamically
          provision new buckets.
        </para>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              <link xlink:href="https://github.com/rook/rook/blob/%7B%7B%20branchName%20%7D%7D/cluster/examples/kubernetes/ceph/object-bucket-claim-retain.yaml">object-bucket-claim-retain.yaml</link>
              Creates a request for a new bucket by referencing a
              StorageClass which saves the bucket when the initiating OBC
              is deleted.
            </para>
          </listitem>
          <listitem>
            <para>
              <link xlink:href="https://github.com/rook/rook/blob/%7B%7B%20branchName%20%7D%7D/cluster/examples/kubernetes/ceph/object-bucket-claim-delete.yaml">object-bucket-claim-delete.yaml</link>
              Creates a request for a new bucket by referencing a
              StorageClass which deletes the bucket when the initiating
              OBC is deleted.
            </para>
          </listitem>
          <listitem>
            <para>
              <link xlink:href="https://github.com/rook/rook/blob/%7B%7B%20branchName%20%7D%7D/cluster/examples/kubernetes/ceph/storageclass-bucket-retain.yaml">storageclass-bucket-retain.yaml</link>
              Creates a new StorageClass which defines the Ceph Object
              Store, a region, and retains the bucket after the initiating
              OBC is deleted.
            </para>
          </listitem>
          <listitem>
            <para>
              <link xlink:href="https://github.com/rook/rook/blob/%7B%7B%20branchName%20%7D%7D/cluster/examples/kubernetes/ceph/storageclass-bucket-delete.yaml">storageclass-bucket-delete.yaml</link>
              Creates a new StorageClass which defines the Ceph Object
              Store, a region, and deletes the bucket after the initiating
              OBC is deleted.
            </para>
          </listitem>
        </itemizedlist>
      </section>
    </section>
  </section>

</chapter>
