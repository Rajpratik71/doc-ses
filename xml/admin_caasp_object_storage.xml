<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
  xml:id="admin-caasp-object-storage">
  <!-- ============================================================== -->
  <title>&objstore;</title>
  <remark>
    Sect 1 - Content from: https://rook.io/docs/rook/v1.4/ceph-object.html
    Ignore "Access External to the Cluster" for now
    Ignore "Object Multisite" for now
    Ignore "Connect to an External Object Store"

    sect 2 - https://rook.io/docs/rook/v1.4/ceph-object-store-crd.html

    Sect 3- https://rook.io/docs/rook/v1.4/ceph-object-bucket-claim.html

    Sect 4 - https://rook.io/docs/rook/v1.4/ceph-object-store-user-crd.html
  </remark>

  <section xml:id="rook-object-storage">
    <title>Object Storage</title>
    <para>
      Object storage exposes an S3 API to the storage cluster for
      applications to put and get data.
    </para>
    <section xml:id="prerequisites">
      <title>Prerequisites</title>
      <para>
        This guide assumes a Rook cluster as explained in the
        <link xlink:href="ceph-quickstart.md">Quickstart</link>.
      </para>
    </section>
    <section xml:id="configure-an-object-store">
      <title>Configure an Object Store</title>
      <para>
        Rook has the ability to either deploy an object store in
        Kubernetes or to connect to an external RGW service. Most
        commonly, the object store will be configured locally by Rook.
        Alternatively, if you have an existing Ceph cluster with Rados
        Gateways, see the
        <link linkend="connect-to-an-external-object-store">external
          section</link> to consume it from Rook.
      </para>
      <section xml:id="create-a-local-object-store">
        <title>Create a Local Object Store</title>
        <para>
          The below sample will create a
          <literal>CephObjectStore</literal> that starts the RGW service
          in the cluster with an S3 API.
        </para>
        <note>
          <para>
            <emphasis role="strong">NOTE</emphasis>: This sample requires
            <emphasis>at least 3 bluestore OSDs</emphasis>, with each OSD
            located on a <emphasis>different node</emphasis>.
          </para>
        </note>
        <para>
          The OSDs must be located on different nodes, because the
          <link xlink:href="ceph-pool-crd.md#spec"><literal>failureDomain</literal></link>
          is set to <literal>host</literal> and the
          <literal>erasureCoded</literal> chunk settings require at least
          3 different OSDs (2 <literal>dataChunks</literal> + 1
          <literal>codingChunks</literal>).
        </para>
        <para>
          See the
          <link xlink:href="ceph-object-store-crd.md#object-store-settings">Object
            Store CRD</link>, for more detail on the settings available for
          a <literal>CephObjectStore</literal>.
        </para>
        <programlisting language="yaml">
          apiVersion: ceph.rook.io/v1
          kind: CephObjectStore
          metadata:
          name: my-store
          namespace: rook-ceph
          spec:
          metadataPool:
          failureDomain: host
          replicated:
          size: 3
          dataPool:
          failureDomain: host
          erasureCoded:
          dataChunks: 2
          codingChunks: 1
          preservePoolsOnDelete: true
          gateway:
          type: s3
          sslCertificateRef:
          port: 80
          # securePort: 443
          instances: 1
          healthCheck:
          bucket:
          disabled: false
          interval: 60s
        </programlisting>
        <para>
          After the <literal>CephObjectStore</literal> is created, the
          Rook operator will then create all the pools and other resources
          necessary to start the service. This may take a minute to
          complete.
        </para>
        <programlisting>
          # Create the object store
          kubectl create -f object.yaml

          # To confirm the object store is configured, wait for the rgw pod to start
          kubectl -n rook-ceph get pod -l app=rook-ceph-rgw
        </programlisting>
      </section>
      <section xml:id="connect-to-an-external-object-store">
        <title>Connect to an External Object Store</title>
        <para>
          Rook can connect to existing RGW gateways to work in conjunction
          with the external mode of the <literal>CephCluster</literal>
          CRD. If you have an external <literal>CephCluster</literal> CR,
          you can instruct Rook to consume external gateways with the
          following:
        </para>
        <programlisting language="yaml">
          apiVersion: ceph.rook.io/v1
          kind: CephObjectStore
          metadata:
          name: external-store
          namespace: rook-ceph
          spec:
          gateway:
          port: 8080
          externalRgwEndpoints:
          - ip: 192.168.39.182
          healthCheck:
          bucket:
          enabled: true
          interval: 60s
        </programlisting>
        <para>
          You can use the existing <literal>object-external.yaml</literal>
          file. When ready the ceph-object-controller will output a
          message in the Operator log similar to this one:
        </para>
        <programlisting>
          ceph-object-controller: ceph object store gateway service running at 10.100.28.138:8080
        </programlisting>
        <para>
          You can now get and access the store via:
        </para>
        <programlisting>
          kubectl -n rook-ceph get svc -l app=rook-ceph-rgw
          NAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
          rook-ceph-rgw-my-store   ClusterIP   10.100.28.138   &lt;none&gt;        8080/TCP   6h59m
        </programlisting>
        <para>
          Any pod from your cluster can now access this endpoint:
        </para>
        <programlisting>
          curl 10.100.28.138:8080
          &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;ListAllMyBucketsResult xmlns=&quot;http://s3.amazonaws.com/doc/2006-03-01/&quot;&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName&gt;&lt;/DisplayName&gt;&lt;/Owner&gt;&lt;Buckets&gt;&lt;/Buckets&gt;&lt;/ListAllMyBucketsResult&gt;
        </programlisting>
        <para>
          It is also possible to use the internally registered DNS name:
        </para>
        <programlisting>
          curl rook-ceph-rgw-my-store.rook-ceph:8080
          &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;ListAllMyBucketsResult xmlns=&quot;http://s3.amazonaws.com/doc/2006-03-01/&quot;&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName&gt;&lt;/DisplayName&gt;&lt;/Owner&gt;&lt;Buckets&gt;&lt;/Buckets&gt;&lt;/ListAllMyBucketsResult&gt;
        </programlisting>
        <para>
          The DNS name is created with the following schema
          <literal>rook-ceph-rgw-$STORE_NAME.$NAMESPACE</literal>.
        </para>
      </section>
    </section>
    <section xml:id="create-a-bucket">
      <title>Create a Bucket</title>
      <para>
        Now that the object store is configured, next we need to create a
        bucket where a client can read and write objects. A bucket can be
        created by defining a storage class, similar to the pattern used
        by block and file storage. First, define the storage class that
        will allow object clients to create a bucket. The storage class
        defines the object storage system, the bucket retention policy,
        and other properties required by the administrator. Save the
        following as <literal>storageclass-bucket-delete.yaml</literal>
        (the example is named as such due to the <literal>Delete</literal>
        reclaim policy).
      </para>
      <programlisting language="yaml">
        apiVersion: storage.k8s.io/v1
        kind: StorageClass
        metadata:
        name: rook-ceph-bucket
        provisioner: rook-ceph.ceph.rook.io/bucket
        reclaimPolicy: Delete
        parameters:
        objectStoreName: my-store
        objectStoreNamespace: rook-ceph
        region: us-east-1
      </programlisting>
      <programlisting>
        kubectl create -f storageclass-bucket-delete.yaml
      </programlisting>
      <para>
        Based on this storage class, an object client can now request a
        bucket by creating an Object Bucket Claim (OBC). When the OBC is
        created, the Rook-Ceph bucket provisioner will create a new
        bucket. Notice that the OBC references the storage class that was
        created above. Save the following as
        <literal>object-bucket-claim-delete.yaml</literal> (the example is
        named as such due to the <literal>Delete</literal> reclaim
        policy):
      </para>
      <programlisting language="yaml">
        apiVersion: objectbucket.io/v1alpha1
        kind: ObjectBucketClaim
        metadata:
        name: ceph-bucket
        spec:
        generateBucketName: ceph-bkt
        storageClassName: rook-ceph-bucket
      </programlisting>
      <programlisting>
        kubectl create -f object-bucket-claim-delete.yaml
      </programlisting>
      <para>
        Now that the claim is created, the operator will create the bucket
        as well as generate other artifacts to enable access to the
        bucket. A secret and ConfigMap are created with the same name as
        the OBC and in the same namespace. The secret contains credentials
        used by the application pod to access the bucket. The ConfigMap
        contains bucket endpoint information and is also consumed by the
        pod. See the <link xlink:href="ceph-object-bucket-claim.md">Object
          Bucket Claim Documentation</link> for more details on the
        <literal>CephObjectBucketClaims</literal>.
      </para>
      <section xml:id="client-connections">
        <title>Client Connections</title>
        <para>
          The following commands extract key pieces of information from
          the secret and configmap:&quot;
        </para>
        <programlisting language="bash">
          #config-map, secret, OBC will part of default if no specific name space mentioned
          export AWS_HOST=$(kubectl -n default get cm ceph-bucket -o yaml | grep BUCKET_HOST | awk '{print $2}')
          export AWS_ACCESS_KEY_ID=$(kubectl -n default get secret ceph-bucket -o yaml | grep AWS_ACCESS_KEY_ID | awk '{print $2}' | base64 --decode)
          export AWS_SECRET_ACCESS_KEY=$(kubectl -n default get secret ceph-bucket -o yaml | grep AWS_SECRET_ACCESS_KEY | awk '{print $2}' | base64 --decode)
        </programlisting>
      </section>
    </section>
    <section xml:id="consume-the-object-storage">
      <title>Consume the Object Storage</title>
      <para>
        Now that you have the object store configured and a bucket
        created, you can consume the object storage from an S3 client.
      </para>
      <para>
        This section will guide you through testing the connection to the
        <literal>CephObjectStore</literal> and uploading and downloading
        from it. Run the following commands after you have connected to
        the <link xlink:href="ceph-toolbox.md">Rook toolbox</link>.
      </para>
      <section xml:id="connection-environment-variables">
        <title>Connection Environment Variables</title>
        <para>
          To simplify the s3 client commands, you will want to set the
          four environment variables for use by your client (ie. inside
          the toolbox). See above for retrieving the variables for a
          bucket created by an <literal>ObjectBucketClaim</literal>.
        </para>
        <programlisting language="bash">
          export AWS_HOST=&lt;host&gt;
          export AWS_ENDPOINT=&lt;endpoint&gt;
          export AWS_ACCESS_KEY_ID=&lt;accessKey&gt;
          export AWS_SECRET_ACCESS_KEY=&lt;secretKey&gt;
        </programlisting>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              <literal>Host</literal>: The DNS host name where the rgw
              service is found in the cluster. Assuming you are using the
              default <literal>rook-ceph</literal> cluster, it will be
              <literal>rook-ceph-rgw-my-store.rook-ceph</literal>.
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>Endpoint</literal>: The endpoint where the rgw
              service is listening. Run
              <literal>kubectl -n rook-ceph get svc rook-ceph-rgw-my-store</literal>,
              then combine the clusterIP and the port.
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>Access key</literal>: The user’s
              <literal>access_key</literal> as printed above
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>Secret key</literal>: The user’s
              <literal>secret_key</literal> as printed above
            </para>
          </listitem>
        </itemizedlist>
        <para>
          The variables for the user generated in this example might be:
        </para>
        <programlisting language="bash">
          export AWS_HOST=rook-ceph-rgw-my-store.rook-ceph
          export AWS_ENDPOINT=10.104.35.31:80
          export AWS_ACCESS_KEY_ID=XEZDB3UJ6X7HVBE7X7MA
          export AWS_SECRET_ACCESS_KEY=7yGIZON7EhFORz0I40BFniML36D2rl8CQQ5kXU6l
        </programlisting>
        <para>
          The access key and secret key can be retrieved as described in
          the section above on <link linkend="client-connections">client
            connections</link> or below in the section
          <link linkend="create-a-user">creating a user</link> if you are
          not creating the buckets with an
          <literal>ObjectBucketClaim</literal>.
        </para>
      </section>
      <section xml:id="install-s3cmd">
        <title>Install s3cmd</title>
        <para>
          To test the <literal>CephObjectStore</literal> we will install
          the <literal>s3cmd</literal> tool into the toolbox pod.
        </para>
        <programlisting>
          yum --assumeyes install s3cmd
        </programlisting>
      </section>
      <section xml:id="put-or-get-an-object">
        <title>PUT or GET an object</title>
        <para>
          Upload a file to the newly created bucket
        </para>
        <programlisting>
          echo &quot;Hello Rook&quot; &gt; /tmp/rookObj
          s3cmd put /tmp/rookObj --no-ssl --host=${AWS_HOST} --host-bucket=  s3://rookbucket
        </programlisting>
        <para>
          Download and verify the file from the bucket
        </para>
        <programlisting>
          s3cmd get s3://rookbucket/rookObj /tmp/rookObj-download --no-ssl --host=${AWS_HOST} --host-bucket=
          cat /tmp/rookObj-download
        </programlisting>
      </section>
    </section>
    <section xml:id="access-external-to-the-cluster">
      <title>Access External to the Cluster</title>
      <para>
        Rook sets up the object storage so pods will have access internal
        to the cluster. If your applications are running outside the
        cluster, you will need to setup an external service through a
        <literal>NodePort</literal>.
      </para>
      <para>
        First, note the service that exposes RGW internal to the cluster.
        We will leave this service intact and create a new service for
        external access.
      </para>
      <programlisting>
        $ kubectl -n rook-ceph get service rook-ceph-rgw-my-store
        NAME                     CLUSTER-IP   EXTERNAL-IP   PORT(S)     AGE
        rook-ceph-rgw-my-store   10.3.0.177   &lt;none&gt;        80/TCP      2m
      </programlisting>
      <para>
        Save the external service as <literal>rgw-external.yaml</literal>:
      </para>
      <programlisting language="yaml">
        apiVersion: v1
        kind: Service
        metadata:
        name: rook-ceph-rgw-my-store-external
        namespace: rook-ceph
        labels:
        app: rook-ceph-rgw
        rook_cluster: rook-ceph
        rook_object_store: my-store
        spec:
        ports:
        - name: rgw
        port: 80
        protocol: TCP
        targetPort: 80
        selector:
        app: rook-ceph-rgw
        rook_cluster: rook-ceph
        rook_object_store: my-store
        sessionAffinity: None
        type: NodePort
      </programlisting>
      <para>
        Now create the external service.
      </para>
      <programlisting>
        kubectl create -f rgw-external.yaml
      </programlisting>
      <para>
        See both rgw services running and notice what port the external
        service is running on:
      </para>
      <programlisting>
        $ kubectl -n rook-ceph get service rook-ceph-rgw-my-store rook-ceph-rgw-my-store-external
        NAME                              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
        rook-ceph-rgw-my-store            ClusterIP   10.104.82.228    &lt;none&gt;        80/TCP         4m
        rook-ceph-rgw-my-store-external   NodePort    10.111.113.237   &lt;none&gt;        80:31536/TCP   39s
      </programlisting>
      <para>
        Internally the rgw service is running on port
        <literal>80</literal>. The external port in this case is
        <literal>31536</literal>. Now you can access the
        <literal>CephObjectStore</literal> from anywhere! All you need is
        the hostname for any machine in the cluster, the external port,
        and the user credentials.
      </para>
    </section>
    <section xml:id="create-a-user">
      <title>Create a User</title>
      <para>
        If you need to create an independent set of user credentials to
        access the S3 endpoint, create a
        <literal>CephObjectStoreUser</literal>. The user will be used to
        connect to the RGW service in the cluster using the S3 API. The
        user will be independent of any object bucket claims that you
        might have created in the earlier instructions in this document.
      </para>
      <para>
        See the <link xlink:href="ceph-object-store-user-crd.md">Object
          Store User CRD</link> for more detail on the settings available
        for a <literal>CephObjectStoreUser</literal>.
      </para>
      <programlisting language="yaml">
        apiVersion: ceph.rook.io/v1
        kind: CephObjectStoreUser
        metadata:
        name: my-user
        namespace: rook-ceph
        spec:
        store: my-store
        displayName: &quot;my display name&quot;
      </programlisting>
      <para>
        When the <literal>CephObjectStoreUser</literal> is created, the
        Rook operator will then create the RGW user on the specified
        <literal>CephObjectStore</literal> and store the Access Key and
        Secret Key in a kubernetes secret in the same namespace as the
        <literal>CephObjectStoreUser</literal>.
      </para>
      <programlisting>
        # Create the object store user
        kubectl create -f object-user.yaml

        # To confirm the object store user is configured, describe the secret
        kubectl -n rook-ceph describe secret rook-ceph-object-user-my-store-my-user

        Name:       rook-ceph-object-user-my-store-my-user
        Namespace:  rook-ceph
        Labels:         app=rook-ceph-rgw
        rook_cluster=rook-ceph
        rook_object_store=my-store
        Annotations:    &lt;none&gt;

        Type:   kubernetes.io/rook

        Data
        ====
        AccessKey:  20 bytes
        SecretKey:  40 bytes
      </programlisting>
      <para>
        The AccessKey and SecretKey data fields can be mounted in a pod as
        an environment variable. More information on consuming kubernetes
        secrets can be found in the
        <link xlink:href="https://kubernetes.io/docs/concepts/configuration/secret/">K8s
          secret documentation</link>
      </para>
      <para>
        To directly retrieve the secrets:
      </para>
      <programlisting>
        kubectl -n rook-ceph get secret rook-ceph-object-user-my-store-my-user -o yaml | grep AccessKey | awk '{print $2}' | base64 --decode
        kubectl -n rook-ceph get secret rook-ceph-object-user-my-store-my-user -o yaml | grep SecretKey | awk '{print $2}' | base64 --decode
      </programlisting>
    </section>
    <section xml:id="object-multisite">
      <title>Object Multisite</title>
      <para>
        Multisite is a feature of Ceph that allows object stores to
        replicate its data over multiple Ceph clusters.
      </para>
      <para>
        Multisite also allows object stores to be independent and isloated
        from other object stores in a cluster.
      </para>
      <para>
        For more information on multisite please read the
        <link xlink:href="ceph-object-multisite.md">ceph multisite
          overview</link> for how to run it.
      </para>
    </section>
  </section>

  <section xml:id="ceph-object-store-crd">
    <title>Ceph Object Store CRD</title>
    <para>
      Rook allows creation and customization of object stores through the
      custom resource definitions (CRDs). The following settings are
      available for Ceph object stores.
    </para>
    <section xml:id="rook-objectstore-crd-sample">
      <title>Sample</title>
      <section xml:id="rook-objectstore-crd-erasure-coded">
        <title>Erasure Coded</title>
        <para>
          Erasure coded pools require the OSDs to use
          <literal>bluestore</literal> for the configured
          <link xlink:href="ceph-cluster-crd.md#osd-configuration-settings"><literal>storeType</literal></link>.
          Additionally, erasure coded pools can only be used with
          <literal>dataPools</literal>. The
          <literal>metadataPool</literal> must use a replicated pool.
        </para>
        <note>
          <para>
            <emphasis role="strong">NOTE</emphasis>: This sample requires
            <emphasis>at least 3 bluestore OSDs</emphasis>, with each OSD
            located on a <emphasis>different node</emphasis>.
          </para>
        </note>
        <para>
          The OSDs must be located on different nodes, because the
          <link xlink:href="ceph-pool-crd.md#spec"><literal>failureDomain</literal></link>
          is set to <literal>host</literal> and the
          <literal>erasureCoded</literal> chunk settings require at least
          3 different OSDs (2 <literal>dataChunks</literal> + 1
          <literal>codingChunks</literal>).
        </para>
        <programlisting language="yaml">
          apiVersion: ceph.rook.io/v1
          kind: CephObjectStore
          metadata:
          name: my-store
          namespace: rook-ceph
          spec:
          metadataPool:
          failureDomain: host
          replicated:
          size: 3
          dataPool:
          failureDomain: host
          erasureCoded:
          dataChunks: 2
          codingChunks: 1
          preservePoolsOnDelete: true
          gateway:
          type: s3
          sslCertificateRef:
          port: 80
          # securePort: 443
          instances: 1
          # A key/value list of annotations
          annotations:
          #  key: value
          placement:
          #  nodeAffinity:
          #    requiredDuringSchedulingIgnoredDuringExecution:
          #      nodeSelectorTerms:
          #      - matchExpressions:
          #        - key: role
          #          operator: In
          #          values:
          #          - rgw-node
          #  tolerations:
          #  - key: rgw-node
          #    operator: Exists
          #  podAffinity:
          #  podAntiAffinity:
          #  topologySpreadConstraints:
          resources:
          #  limits:
          #    cpu: &quot;500m&quot;
          #    memory: &quot;1024Mi&quot;
          #  requests:
          #    cpu: &quot;500m&quot;
          #    memory: &quot;1024Mi&quot;
          #zone:
          #name: zone-a
        </programlisting>
      </section>
    </section>
    <section xml:id="object-store-settings">
      <title>Object Store Settings</title>
      <section xml:id="objectstore-settings-metadata">
        <title>Metadata</title>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              <literal>name</literal>: The name of the object store to
              create, which will be reflected in the pool and other
              resource names.
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>namespace</literal>: The namespace of the Rook
              cluster where the object store is created.
            </para>
          </listitem>
        </itemizedlist>
      </section>
      <section xml:id="pools">
        <title>Pools</title>
        <para>
          The pools allow all of the settings defined in the Pool CRD
          spec. For more details, see the
          <link xlink:href="ceph-pool-crd.md">Pool CRD</link> settings. In
          the example above, there must be at least three hosts (size 3)
          and at least three devices (2 data + 1 coding chunks) in the
          cluster.
        </para>
        <para>
          When the <literal>zone</literal> section is set pools with the
          object stores name will not be created since the object-store
          will the using the pools created by the ceph-object-zone.
        </para>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              <literal>metadataPool</literal>: The settings used to create
              all of the object store metadata pools. Must use
              replication.
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>dataPool</literal>: The settings to create the
              object store data pool. Can use replication or erasure
              coding.
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>preservePoolsOnDelete</literal>: If it is set to
              <quote>true</quote> the pools used to support the object
              store will remain when the object store will be deleted.
              This is a security measure to avoid accidental loss of data.
              It is set to <quote>false</quote> by default. If not
              specified is also deemed as <quote>false</quote>.
            </para>
          </listitem>
        </itemizedlist>
      </section>
    </section>
    <section xml:id="gateway-settings">
      <title>Gateway Settings</title>
      <para>
        The gateway settings correspond to the RGW daemon settings.
      </para>
      <itemizedlist spacing="compact">
        <listitem>
          <para>
            <literal>type</literal>: <literal>S3</literal> is supported
          </para>
        </listitem>
        <listitem>
          <para>
            <literal>sslCertificateRef</literal>: If the certificate is
            not specified, SSL will not be configured. If specified, this
            is the name of the Kubernetes secret that contains the SSL
            certificate to be used for secure connections to the object
            store. Rook will look in the secret provided at the
            <literal>cert</literal> key name. The value of the
            <literal>cert</literal> key must be in the format expected by
            the
            <link xlink:href="https://docs.ceph.com/docs/master/install/ceph-deploy/install-ceph-gateway/#using-ssl-with-civetweb">RGW
              service</link>: <quote>The server key, server certificate, and
                any other CA or intermediate certificates be supplied in one
                file. Each of these items must be in pem form.</quote>
          </para>
        </listitem>
        <listitem>
          <para>
            <literal>port</literal>: The port on which the Object service
            will be reachable. If host networking is enabled, the RGW
            daemons will also listen on that port. If running on SDN, the
            RGW daemon listening port will be 8080 internally.
          </para>
        </listitem>
        <listitem>
          <para>
            <literal>securePort</literal>: The secure port on which RGW
            pods will be listening. An SSL certificate must be specified.
          </para>
        </listitem>
        <listitem>
          <para>
            <literal>instances</literal>: The number of pods that will be
            started to load balance this object store.
          </para>
        </listitem>
        <listitem>
          <para>
            <literal>externalRgwEndpoints</literal>: A list of IP
            addresses to connect to external existing Rados Gateways
            (works with external mode). This setting will be ignored if
            the <literal>CephCluster</literal> does not have
            <literal>external</literal> spec enabled. Refer to the
            <link xlink:href="ceph-cluster-crd.md#external-cluster">external
              cluster section</link> for more details.
          </para>
        </listitem>
        <listitem>
          <para>
            <literal>annotations</literal>: Key value pair list of
            annotations to add.
          </para>
        </listitem>
        <listitem>
          <para>
            <literal>labels</literal>: Key value pair list of labels to
            add.
          </para>
        </listitem>
        <listitem>
          <para>
            <literal>placement</literal>: The Kubernetes placement
            settings to determine where the RGW pods should be started in
            the cluster.
          </para>
        </listitem>
        <listitem>
          <para>
            <literal>resources</literal>: Set resource requests/limits for
            the Gateway Pod(s), see
            <link xlink:href="ceph-cluster-crd.md#resource-requirementslimits">Resource
              Requirements/Limits</link>.
          </para>
        </listitem>
        <listitem>
          <para>
            <literal>priorityClassName</literal>: Set priority class name
            for the Gateway Pod(s)
          </para>
        </listitem>
      </itemizedlist>
      <para>
        Example of external rgw endpoints to connect to:
      </para>
      <programlisting language="yaml">
        gateway:
        port: 80
        externalRgwEndpoints:
        - ip: 192.168.39.182
      </programlisting>
      <para>
        This will create a service with the endpoint
        <literal>192.168.39.182</literal> on port <literal>80</literal>,
        pointing to the Ceph object external gateway. All the other
        settings from the gateway section will be ignored, except for
        <literal>securePort</literal>.
      </para>
    </section>
    <section xml:id="zone-settings">
      <title>Zone Settings</title>
      <para>
        The <link xlink:href="ceph-object-multisite.md">zone</link>
        settings allow the object store to join custom created
        <link xlink:href="ceph-object-multisite-crd.md">ceph-object-zone</link>.
      </para>
      <itemizedlist spacing="compact">
        <listitem>
          <para>
            <literal>name</literal>: the name of the ceph-object-zone the
            object store will be in.
          </para>
        </listitem>
      </itemizedlist>
    </section>
    <section xml:id="runtime-settings">
      <title>Runtime settings</title>
      <section xml:id="mime-types">
        <title>MIME types</title>
        <para>
          Rook provides a default <literal>mime.types</literal> file for
          each Ceph object store. This file is stored in a Kubernetes
          ConfigMap with the name
          <literal>rook-ceph-rgw-&lt;STORE-NAME&gt;-mime-types</literal>.
          For most users, the default file should suffice, however, the
          option is available to users to edit the
          <literal>mime.types</literal> file in the ConfigMap as they
          desire. Users may have their own special file types, and
          particularly security conscious users may wish to pare down the
          file to reduce the possibility of a file type execution attack.
        </para>
        <para>
          Rook will not overwrite an existing
          <literal>mime.types</literal> ConfigMap so that user
          modifications will not be destroyed. If the object store is
          destroyed and recreated, the ConfigMap will also be destroyed
          and created anew.
        </para>
      </section>
    </section>
    <section xml:id="health-settings">
      <title>Health settings</title>
      <para>
        Rook-Ceph will be default monitor the state of the object store
        endpoints. The following CRD settings are available:
      </para>
      <itemizedlist spacing="compact">
        <listitem>
          <para>
            <literal>healthCheck</literal>: main object store health
            monitoring section
          </para>
        </listitem>
      </itemizedlist>
      <para>
        Here is a complete example:
      </para>
      <programlisting language="yaml">
        healthCheck:
        bucket:
        disabled: false
        interval: 60s
      </programlisting>
      <para>
        The endpoint health check procedure is the following:
      </para>
      <orderedlist numeration="arabic" spacing="compact">
        <listitem>
          <para>
            Create an S3 user
          </para>
        </listitem>
        <listitem>
          <para>
            Create a bucket with that user
          </para>
        </listitem>
        <listitem>
          <para>
            PUT the file in the object store
          </para>
        </listitem>
        <listitem>
          <para>
            GET the file from the object store
          </para>
        </listitem>
        <listitem>
          <para>
            Verify object consistency
          </para>
        </listitem>
        <listitem>
          <para>
            Update CR health status check
          </para>
        </listitem>
      </orderedlist>
      <para>
        Rook-Ceph always keeps the bucket and the user for the health
        check, it just does a PUT and GET of an s3 object since creating a
        bucket is an expensive operation.
      </para>
    </section>
  </section>

  <section xml:id="ceph-object-bucket-claim">
    <title>Ceph Object Bucket Claim</title>
    <para>
      Rook supports the creation of new buckets and access to existing
      buckets via two custom resources:
    </para>
    <itemizedlist spacing="compact">
      <listitem>
        <para>
          an <literal>Object Bucket Claim (OBC)</literal> is custom
          resource which requests a bucket (new or existing) and is
          described by a Custom Resource Definition (CRD) shown below.
        </para>
      </listitem>
      <listitem>
        <para>
          an <literal>Object Bucket (OB)</literal> is a custom resource
          automatically generated when a bucket is provisioned. It is a
          global resource, typically not visible to non-admin users, and
          contains information specific to the bucket. It is described by
          an OB CRD, also shown below.
        </para>
      </listitem>
    </itemizedlist>
    <para>
      An OBC references a storage class which is created by an
      administrator. The storage class defines whether the bucket
      requested is a new bucket or an existing bucket. It also defines the
      bucket retention policy. Users request a new or existing bucket by
      creating an OBC which is shown below. The ceph provisioner detects
      the OBC and creates a new bucket or grants access to an existing
      bucket, depending the the storage class referenced in the OBC. It
      also generates a Secret which provides credentials to access the
      bucket, and a ConfigMap which contains the bucket’s endpoint.
      Application pods consume the information in the Secret and ConfigMap
      to access the bucket. Please note that to make provisioner watch the
      cluster namespace only you need to set
      <literal>ROOK_OBC_WATCH_OPERATOR_NAMESPACE</literal> to
      <literal>true</literal> in the operator manifest, otherwise it
      watches all namespaces.
    </para>
    <section xml:id="rook-objectstore-bucket-sample">
      <title>Sample</title>
      <section xml:id="obc-custom-resource">
        <title>OBC Custom Resource</title>
        <programlisting language="yaml">
          apiVersion: objectbucket.io/v1alpha1
          kind: ObjectBucketClaim
          metadata:
          name: ceph-bucket [1]
          namespace: rook-ceph [2]
          spec:
          bucketName: [3]
          generateBucketName: photo-booth [4]
          storageClassName: rook-ceph-bucket [4]
          additionalConfig: [5]
          maxObjects: &quot;1000&quot;
          maxSize: &quot;2G&quot;
        </programlisting>
        <orderedlist numeration="arabic" spacing="compact">
          <listitem>
            <para>
              <literal>name</literal> of the
              <literal>ObjectBucketClaim</literal>. This name becomes the
              name of the Secret and ConfigMap.
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>namespace</literal>(optional) of the
              <literal>ObjectBucketClaim</literal>, which is also the
              namespace of the ConfigMap and Secret.
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>bucketName</literal> name of the
              <literal>bucket</literal>.
              <emphasis role="strong">Not</emphasis> recommended for new
              buckets since names must be unique within an entire object
              store.
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>generateBucketName</literal> value becomes the
              prefix for a randomly generated name, if supplied then
              <literal>bucketName</literal> must be empty. If both
              <literal>bucketName</literal> and
              <literal>generateBucketName</literal> are supplied then
              <literal>BucketName</literal> has precedence and
              <literal>GenerateBucketName</literal> is ignored. If both
              <literal>bucketName</literal> and
              <literal>generateBucketName</literal> are blank or omitted
              then the storage class is expected to contain the name of an
              <emphasis>existing</emphasis> bucket. It’s an error if all
              three bucket related names are blank or omitted.
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>storageClassName</literal> which defines the
              StorageClass which contains the names of the bucket
              provisioner, the object-store and specifies the bucket
              retention policy.
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>additionalConfig</literal> is an optional list of
              key-value pairs used to define attributes specific to the
              bucket being provisioned by this OBC. This information is
              typically tuned to a particular bucket provisioner and may
              limit application portability. Options supported:
            </para>
          </listitem>
        </orderedlist>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              <literal>maxObjects</literal>: The maximum number of objects
              in the bucket
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>maxSize</literal>: The maximum size of the bucket,
              please note minimum recommended value is 4K.
            </para>
          </listitem>
        </itemizedlist>
      </section>
      <section xml:id="obc-custom-resource-after-bucket-provisioning">
        <title>OBC Custom Resource after Bucket Provisioning</title>
        <programlisting language="yaml">
          apiVersion: objectbucket.io/v1alpha1
          kind: ObjectBucketClaim
          metadata:
          creationTimestamp: &quot;2019-10-18T09:54:01Z&quot;
          generation: 2
          name: ceph-bucket
          namespace: default [1]
          resourceVersion: &quot;559491&quot;
          spec:
          ObjectBucketName: obc-default-ceph-bucket [2]
          additionalConfig: null
          bucketName: photo-booth-c1178d61-1517-431f-8408-ec4c9fa50bee [3]
          cannedBucketAcl: &quot;&quot;
          ssl: false
          storageClassName: rook-ceph-bucket [4]
          versioned: false
          status:
          Phase: bound [5]
        </programlisting>
        <orderedlist numeration="arabic" spacing="compact">
          <listitem>
            <para>
              <literal>namespace</literal> where OBC got created.
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>ObjectBucketName</literal> generated OB name
              created using name space and OBC name.
            </para>
          </listitem>
          <listitem>
            <para>
              the generated (in this case), unique
              <literal>bucket name</literal> for the new bucket.
            </para>
          </listitem>
          <listitem>
            <para>
              name of the storage class from OBC got created.
            </para>
          </listitem>
          <listitem>
            <para>
              phases of bucket creation:
            </para>
            <itemizedlist spacing="compact">
              <listitem>
                <para>
                  <emphasis>Pending</emphasis>: the operator is processing
                  the request.
                </para>
              </listitem>
              <listitem>
                <para>
                  <emphasis>Bound</emphasis>: the operator finished
                  processing the request and linked the OBC and OB
                </para>
              </listitem>
              <listitem>
                <para>
                  <emphasis>Released</emphasis>: the OB has been deleted,
                  leaving the OBC unclaimed but unavailable.
                </para>
              </listitem>
              <listitem>
                <para>
                  <emphasis>Failed</emphasis>: not currently set.
                </para>
              </listitem>
            </itemizedlist>
          </listitem>
        </orderedlist>
      </section>
      <section xml:id="app-pod">
        <title>App Pod</title>
        <programlisting language="yaml">
          apiVersion: v1
          kind: Pod
          metadata:
          name: app-pod
          namespace: dev-user
          spec:
          containers:
          - name: mycontainer
          image: redis
          envFrom: [1]
          - configMapRef:
          name: ceph-bucket [2]
          - secretRef:
          name: ceph-bucket [3]
        </programlisting>
        <orderedlist numeration="arabic" spacing="compact">
          <listitem>
            <para>
              use <literal>env:</literal> if mapping of the defined key
              names to the env var names used by the app is needed.
            </para>
          </listitem>
          <listitem>
            <para>
              makes available to the pod as env variables:
              <literal>BUCKET_HOST</literal>,
              <literal>BUCKET_PORT</literal>,
              <literal>BUCKET_NAME</literal>
            </para>
          </listitem>
          <listitem>
            <para>
              makes available to the pod as env variables:
              <literal>AWS_ACCESS_KEY_ID</literal>,
              <literal>AWS_SECRET_ACCESS_KEY</literal>
            </para>
          </listitem>
        </orderedlist>
      </section>
      <section xml:id="storageclass">
        <title>StorageClass</title>
        <programlisting language="yaml">
          apiVersion: storage.k8s.io/v1
          kind: StorageClass
          metadata:
          name: rook-ceph-bucket
          labels:
          aws-s3/object [1]
          provisioner: rook-ceph.ceph.rook.io/bucket [2]
          parameters: [3]
          objectStoreName: my-store
          objectStoreNamespace: rook-ceph
          region: us-west-1
          bucketName: ceph-bucket [4]
          reclaimPolicy: Delete [5]
        </programlisting>
        <orderedlist numeration="arabic" spacing="compact">
          <listitem>
            <para>
              <literal>label</literal>(optional) here associates this
              <literal>StorageClass</literal> to a specific provisioner.
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>provisioner</literal> responsible for handling
              <literal>OBCs</literal> referencing this
              <literal>StorageClass</literal>.
            </para>
          </listitem>
          <listitem>
            <para>
              <emphasis role="strong">all</emphasis>
              <literal>parameter</literal> required.
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>bucketName</literal> is required for access to
              existing buckets but is omitted when provisioning new
              buckets. Unlike greenfield provisioning, the brownfield
              bucket name appears in the <literal>StorageClass</literal>,
              not the <literal>OBC</literal>.
            </para>
          </listitem>
          <listitem>
            <para>
              rook-ceph provisioner decides how to treat the
              <literal>reclaimPolicy</literal> when an
              <literal>OBC</literal> is deleted for the bucket. See
              explanation as
              <link xlink:href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#retain">specified
                in Kubernetes</link>
            </para>
          </listitem>
        </orderedlist>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              <emphasis>Delete</emphasis> = physically delete the bucket.
            </para>
          </listitem>
          <listitem>
            <para>
              <emphasis>Retain</emphasis> = do not physically delete the
              bucket.
            </para>
          </listitem>
        </itemizedlist>
      </section>
    </section>
  </section>

  <section xml:id="ceph-object-store-user-crd">
    <title>Ceph Object Store User CRD</title>
    <para>
      Rook allows creation and customization of object store users through
      the custom resource definitions (CRDs). The following settings are
      available for Ceph object store users.
    </para>
    <section xml:id="rook-objectstore-user-crd-sample">
      <title>Sample</title>
      <programlisting language="yaml">
        apiVersion: ceph.rook.io/v1
        kind: CephObjectStoreUser
        metadata:
        name: my-user
        namespace: rook-ceph
        spec:
        store: my-store
        displayName: my-display-name
      </programlisting>
    </section>
    <section xml:id="object-store-user-settings">
      <title>Object Store User Settings</title>
      <section xml:id="objectstore-user-settings-metadata">
        <title>Metadata</title>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              <literal>name</literal>: The name of the object store user
              to create, which will be reflected in the secret and other
              resource names.
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>namespace</literal>: The namespace of the Rook
              cluster where the object store user is created.
            </para>
          </listitem>
        </itemizedlist>
      </section>
      <section xml:id="spec">
        <title>Spec</title>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              <literal>store</literal>: The object store in which the user
              will be created. This matches the name of the objectstore
              CRD.
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>displayName</literal>: The display name which will
              be passed to the
              <literal>radosgw-admin user create</literal> command.
            </para>
          </listitem>
        </itemizedlist>
      </section>
    </section>
  </section>

</chapter>
