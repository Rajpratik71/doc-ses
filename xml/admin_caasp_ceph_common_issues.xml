<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
 xml:id="admin-caasp-ceph-common-issues">
<!-- ============================================================== -->
  <title>Common Issues</title>
  <remark>
    Content from: https://rook.io/docs/rook/v1.4/ceph-common-issues.html
    Ignore "Flex storage class versus Ceph CSI storage class" section
  </remark>

  <section xml:id="ceph-common-issues">
    <title>Ceph Common Issues</title>
    <para>
      Many of these problem cases are hard to summarize down to a short
      phrase that adequately describes the problem. Each problem will
      start with a bulleted list of symptoms. Keep in mind that all
      symptoms may not apply depending on the configuration of Rook. If
      the majority of the symptoms are seen there is a fair chance you are
      experiencing that problem.
    </para>
    <para>
      If after trying the suggestions found on this page and the problem
      is not resolved, the Rook team is very happy to help you
      troubleshoot the issues in their Slack channel. Once you have
      <link xlink:href="https://slack.rook.io">registered for the Rook
        Slack</link>, proceed to the <literal>#ceph</literal> channel to ask
      for assistance.
    </para>
    <section xml:id="troubleshooting-techniques">
      <title>Troubleshooting Techniques</title>
      <para>
        There are two main categories of information you will need to
        investigate issues in the cluster:
      </para>
      <orderedlist numeration="arabic" spacing="compact">
        <listitem>
          <para>
            Kubernetes status and logs documented
            <link xlink:href="common-issues.md">here</link>
          </para>
        </listitem>
        <listitem>
          <para>
            Ceph cluster status (see upcoming
            <xref linkend="ceph-tools"/> section)
          </para>
        </listitem>
      </orderedlist>
      <section xml:id="ceph-tools">
        <title>Ceph Tools</title>
        <para>
          After you verify the basic health of the running pods, next you
          will want to run Ceph tools for status of the storage
          components. There are two ways to run the Ceph tools, either in
          the Rook toolbox or inside other Rook pods that are already
          running.
        </para>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              Logs on a specific node to find why a PVC is failing to
              mount:
            </para>
            <itemizedlist spacing="compact">
              <listitem>
                <para>
                  Rook agent errors around the attach/detach:
                  <literal>kubectl logs -n rook-ceph &lt;rook-ceph-agent-pod&gt;</literal>
                </para>
              </listitem>
            </itemizedlist>
          </listitem>
          <listitem>
            <para>
              See the
              <link xlink:href="ceph-advanced-configuration.md#log-collection">log
                collection topic</link> for a script that will help you
              gather the logs
            </para>
          </listitem>
          <listitem>
            <para>
              Other artifacts:
            </para>
            <itemizedlist spacing="compact">
              <listitem>
                <para>
                  The monitors that are expected to be in quorum:
                  <literal>kubectl -n &lt;cluster-namespace&gt; get configmap rook-ceph-mon-endpoints -o yaml | grep data</literal>
                </para>
              </listitem>
            </itemizedlist>
          </listitem>
        </itemizedlist>
        <section xml:id="tools-in-the-rook-toolbox">
          <title>Tools in the Rook Toolbox</title>
          <para>
            The <link xlink:href="./ceph-toolbox.md">rook-ceph-tools
              pod</link> provides a simple environment to run Ceph tools.
            Once the pod is up and running, connect to the pod to execute
            Ceph commands to evaluate that current state of the cluster.
          </para>
          <screen>
            kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l &quot;app=rook-ceph-tools&quot; -o jsonpath='{.items[0].metadata.name}') bash
          </screen>
        </section>
        <section xml:id="ceph-commands">
          <title>Ceph Commands</title>
          <para>
            Here are some common commands to troubleshoot a Ceph cluster:
          </para>
          <itemizedlist spacing="compact">
            <listitem>
              <para>
                <literal>ceph status</literal>
              </para>
            </listitem>
            <listitem>
              <para>
                <literal>ceph osd status</literal>
              </para>
            </listitem>
            <listitem>
              <para>
                <literal>ceph osd df</literal>
              </para>
            </listitem>
            <listitem>
              <para>
                <literal>ceph osd utilization</literal>
              </para>
            </listitem>
            <listitem>
              <para>
                <literal>ceph osd pool stats</literal>
              </para>
            </listitem>
            <listitem>
              <para>
                <literal>ceph osd tree</literal>
              </para>
            </listitem>
            <listitem>
              <para>
                <literal>ceph pg stat</literal>
              </para>
            </listitem>
          </itemizedlist>
          <para>
            The first two status commands provide the overall cluster
            health. The normal state for cluster operations is HEALTH_OK,
            but will still function when the state is in a HEALTH_WARN
            state. If you are in a WARN state, then the cluster is in a
            condition that it may enter the HEALTH_ERROR state at which
            point <emphasis>all</emphasis> disk I/O operations are halted.
            If a HEALTH_WARN state is observed, then one should take
            action to prevent the cluster from halting when it enters the
            HEALTH_ERROR state.
          </para>
          <para>
            There are many Ceph sub-commands to look at and manipulate
            Ceph objects, well beyond the scope this document. See the
            <link xlink:href="https://docs.ceph.com/">Ceph
              documentation</link> for more details of gathering information
            about the health of the cluster. In addition, there are other
            helpful hints and some best practices located in the
            <link xlink:href="advanced-configuration.md">Advanced
              Configuration section</link>. Of particular note, there are
            scripts for collecting logs and gathering OSD information
            there.
          </para>
        </section>
      </section>
    </section>
    <section xml:id="pod-using-ceph-storage-is-not-running">
      <title>Pod Using Ceph Storage Is Not Running</title>
      <note>
        <para>
          This topic is specific to creating PVCs based on Rookâ€™s
          <emphasis role="strong">Flex</emphasis> driver, which is no
          longer the default option. By default, Rook deploys the CSI
          driver for binding the PVCs to the storage.
        </para>
      </note>
      <section xml:id="symptoms">
        <title>Symptoms</title>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              The pod that is configured to use Rook storage is stuck in
              the <literal>ContainerCreating</literal> status
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>kubectl describe pod</literal> for the pod mentions
              one or more of the following:
            </para>
            <itemizedlist spacing="compact">
              <listitem>
                <para>
                  <literal>PersistentVolumeClaim is not bound</literal>
                </para>
              </listitem>
              <listitem>
                <para>
                  <literal>timeout expired waiting for volumes to attach/mount</literal>
                </para>
              </listitem>
            </itemizedlist>
          </listitem>
          <listitem>
            <para>
              <literal>kubectl -n rook-ceph get pod</literal> shows the
              rook-ceph-agent pods in a
              <literal>CrashLoopBackOff</literal> status
            </para>
          </listitem>
        </itemizedlist>
        <para>
          If you see that the PVC remains in
          <emphasis role="strong">pending</emphasis> state, see the topic
          <xref linkend="pvcs-stay-in-pending-state"/>.
        </para>
      </section>
      <section xml:id="possible-solutions-summary">
        <title>Possible Solutions Summary</title>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              <literal>rook-ceph-agent</literal> pod is in a
              <literal>CrashLoopBackOff</literal> status because it cannot
              deploy its driver on a read-only filesystem:
              <link xlink:href="./ceph-prerequisites.md#ceph-flexvolume-configuration">Flexvolume
                configuration pre-reqs</link>
            </para>
          </listitem>
          <listitem>
            <para>
              Persistent Volume and/or Claim are failing to be created and
              bound: <link linkend="volume-creation">Volume
                Creation</link>
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>rook-ceph-agent</literal> pod is failing to mount
              and format the volume: <link linkend="volume-mounting">Rook
                Agent Mounting</link>
            </para>
          </listitem>
        </itemizedlist>
      </section>
      <section xml:id="investigation-details">
        <title>Investigation Details</title>
        <para>
          If you see some of the symptoms above, itâ€™s because the
          requested Rook storage for your pod is not being created and
          mounted successfully. In this walkthrough, we will be looking at
          the wordpress mysql example pod that is failing to start.
        </para>
        <para>
          To first confirm there is an issue, you can run commands similar
          to the following and you should see similar output (note that
          some of it has been omitted for brevity):
        </para>
        <screen>
          &gt; kubectl get pod
          NAME                              READY     STATUS              RESTARTS   AGE
          wordpress-mysql-918363043-50pjr   0/1       ContainerCreating   0          1h

          &gt; kubectl describe pod wordpress-mysql-918363043-50pjr
          ...
          Events:
          FirstSeen LastSeen    Count   From            SubObjectPath   Type        Reason          Message
          --------- --------    -----   ----            -------------   --------    ------          -------
          1h        1h      3   default-scheduler           Warning     FailedScheduling    PersistentVolumeClaim is not bound: &quot;mysql-pv-claim&quot; (repeated 2 times)
          1h        35s     36  kubelet, 172.17.8.101           Warning     FailedMount     Unable to mount volumes for pod &quot;wordpress-mysql-918363043-50pjr_default(08d14e75-bd99-11e7-bc4c-001c428b9fc8)&quot;: timeout expired waiting for volumes to attach/mount for pod &quot;default&quot;/&quot;wordpress-mysql-918363043-50pjr&quot;. list of unattached/unmounted volumes=[mysql-persistent-storage]
          1h        35s     36  kubelet, 172.17.8.101           Warning     FailedSync      Error syncing pod
        </screen>
        <para>
          To troubleshoot this, letâ€™s walk through the volume provisioning
          steps in order to confirm where the failure is happening.
        </para>
        <section xml:id="ceph-agent-deployment">
          <title>Ceph Agent Deployment</title>
          <para>
            The <literal>rook-ceph-agent</literal> pods are responsible
            for mapping and mounting the volume from the cluster onto the
            node that your pod will be running on. If the
            <literal>rook-ceph-agent</literal> pod is not running then it
            cannot perform this function.
          </para>
          <para>
            Below is an example of the <literal>rook-ceph-agent</literal>
            pods failing to get to the <literal>Running</literal> status
            because they are in a <literal>CrashLoopBackOff</literal>
            status:
          </para>
          <screen>
            &gt; kubectl -n rook-ceph get pod
            NAME                                  READY     STATUS             RESTARTS   AGE
            rook-ceph-agent-ct5pj                 0/1       CrashLoopBackOff   16         59m
            rook-ceph-agent-zb6n9                 0/1       CrashLoopBackOff   16         59m
            rook-operator-2203999069-pmhzn        1/1       Running            0          59m
          </screen>
          <para>
            If you see this occurring, you can get more details about why
            the <literal>rook-ceph-agent</literal> pods are continuing to
            crash with the following command and its sample output:
          </para>
          <screen>
            &gt; kubectl -n rook-ceph get pod -l app=rook-ceph-agent -o jsonpath='{range .items[*]}{.metadata.name}{&quot;\t&quot;}{.status.containerStatuses[0].lastState.terminated.message}{&quot;\n&quot;}{end}'
            rook-ceph-agent-ct5pj   mkdir /usr/libexec/kubernetes: read-only filesystem
            rook-ceph-agent-zb6n9   mkdir /usr/libexec/kubernetes: read-only filesystem
          </screen>
          <para>
            From the output above, we can see that the agents were not
            able to bind mount to
            <literal>/usr/libexec/kubernetes</literal> on the host they
            are scheduled to run on. For some environments, this default
            path is read-only and therefore a better path must be provided
            to the agents.
          </para>
          <para>
            First, clean up the agent deployment with:
          </para>
          <screen>
            kubectl -n rook-ceph delete daemonset rook-ceph-agent
          </screen>
          <para>
            Once the <literal>rook-ceph-agent</literal> pods are gone,
            <emphasis role="strong">follow the instructions in the
              <link xlink:href="./ceph-prerequisites.md#ceph-flexvolume-configuration">Flexvolume
                configuration pre-reqs</link></emphasis> to ensure a good
            value for <literal>--volume-plugin-dir</literal> has been
            provided to the Kubelet. After that has been configured, and
            the Kubelet has been restarted, start the agent pods up again
            by restarting <literal>rook-operator</literal>:
          </para>
          <screen>
            kubectl -n rook-ceph delete pod -l app=rook-ceph-operator
          </screen>
        </section>
        <section xml:id="volume-creation">
          <title>Volume Creation</title>
          <para>
            The volume must first be created in the Rook cluster and then
            bound to a volume claim before it can be mounted to a pod.
            Letâ€™s confirm that with the following commands and their
            output:
          </para>
          <screen>
            &gt; kubectl get pv
            NAME                                       CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS     CLAIM                    STORAGECLASS   REASON    AGE
            pvc-9f273fbc-bdbf-11e7-bc4c-001c428b9fc8   20Gi       RWO           Delete          Bound      default/mysql-pv-claim   rook-ceph-block               25m

            &gt; kubectl get pvc
            NAME             STATUS    VOLUME                                     CAPACITY   ACCESSMODES   STORAGECLASS   AGE
            mysql-pv-claim   Bound     pvc-9f273fbc-bdbf-11e7-bc4c-001c428b9fc8   20Gi       RWO           rook-ceph-block     25m
          </screen>
          <para>
            Both your volume and its claim should be in the
            <literal>Bound</literal> status. If one or neither of them is
            not in the <literal>Bound</literal> status, then look for
            details of the issue in the <literal>rook-operator</literal>
            logs:
          </para>
          <screen>
            kubectl -n rook-ceph logs `kubectl -n rook-ceph -l app=rook-ceph-operator get pods -o jsonpath='{.items[*].metadata.name}'`
          </screen>
          <para>
            If the volume is failing to be created, there should be
            details in the <literal>rook-operator</literal> log output,
            especially those tagged with
            <literal>op-provisioner</literal>.
          </para>
          <para>
            One common cause for the <literal>rook-operator</literal>
            failing to create the volume is when the
            <literal>clusterNamespace</literal> field of the
            <literal>StorageClass</literal> doesnâ€™t match the
            <emphasis role="strong">namespace</emphasis> of the Rook
            cluster, as described in
            <link xlink:href="https://github.com/rook/rook/issues/1502">#1502</link>.
            In that scenario, the <literal>rook-operator</literal> log
            would show a failure similar to the following:
          </para>
          <screen>
            2018-03-28 18:58:32.041603 I | op-provisioner: creating volume with configuration {pool:replicapool clusterNamespace:rook-ceph fstype:}
            2018-03-28 18:58:32.041728 I | exec: Running command: rbd create replicapool/pvc-fd8aba49-32b9-11e8-978e-08002762c796 --size 20480 --cluster=rook --conf=/var/lib/rook/rook-ceph/rook.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
            E0328 18:58:32.060893       5 controller.go:801] Failed to provision volume for claim &quot;default/mysql-pv-claim&quot; with StorageClass &quot;rook-ceph-block&quot;: Failed to create rook block image replicapool/pvc-fd8aba49-32b9-11e8-978e-08002762c796: failed to create image pvc-fd8aba49-32b9-11e8-978e-08002762c796 in pool replicapool of size 21474836480: Failed to complete '': exit status 1. global_init: unable to open config file from search list /var/lib/rook/rook-ceph/rook.config
            . output:
          </screen>
          <para>
            The solution is to ensure that the
            <link xlink:href="https://github.com/rook/rook/blob/master/cluster/examples/kubernetes/ceph/flex/storageclass.yaml#L28"><literal>clusterNamespace</literal></link>
            field matches the <emphasis role="strong">namespace</emphasis>
            of the Rook cluster when creating the
            <literal>StorageClass</literal>.
          </para>
        </section>
        <section xml:id="volume-mounting">
          <title>Volume Mounting</title>
          <para>
            The final step in preparing Rook storage for your pod is for
            the <literal>rook-ceph-agent</literal> pod to mount and format
            it. If all the preceding sections have been successful or
            inconclusive, then take a look at the
            <literal>rook-ceph-agent</literal> pod logs for further clues.
            You can determine which <literal>rook-ceph-agent</literal> is
            running on the same node that your pod is scheduled on by
            using the <literal>-o wide</literal> output, then you can get
            the logs for that <literal>rook-ceph-agent</literal> pod
            similar to the example below:
          </para>
          <screen>
            &gt; kubectl -n rook-ceph get pod -o wide
            NAME                                  READY     STATUS    RESTARTS   AGE       IP             NODE
            rook-ceph-agent-h6scx                 1/1       Running   0          9m        172.17.8.102   172.17.8.102
            rook-ceph-agent-mp7tn                 1/1       Running   0          9m        172.17.8.101   172.17.8.101
            rook-operator-2203999069-3tb68        1/1       Running   0          9m        10.32.0.7      172.17.8.101

            &gt; kubectl -n rook-ceph logs rook-ceph-agent-h6scx
            2017-10-30 23:07:06.984108 I | rook: starting Rook v0.5.0-241.g48ce6de.dirty with arguments '/usr/local/bin/rook agent'
            [...]
          </screen>
          <para>
            In the <literal>rook-ceph-agent</literal> pod logs, you may
            see a snippet similar to the following:
          </para>
          <screen>
            Failed to complete rbd: signal: interrupt.
          </screen>
          <para>
            In this case, the agent waited for the <literal>rbd</literal>
            command but it did not finish in a timely manner so the agent
            gave up and stopped it. This can happen for multiple reasons,
            but using <literal>dmesg</literal> will likely give you
            insight into the root cause. If <literal>dmesg</literal> shows
            something similar to below, then it means you have an old
            kernel that canâ€™t talk to the cluster:
          </para>
          <screen>
            libceph: mon2 10.205.92.13:6789 feature set mismatch, my 4a042a42 &lt; server's 2004a042a42, missing 20000000000
          </screen>
          <para>
            If <literal>uname -a</literal> shows that you have a kernel
            version older than <literal>3.15</literal>, youâ€™ll need to
            perform <emphasis role="strong">one</emphasis> of the
            following:
          </para>
          <itemizedlist spacing="compact">
            <listitem>
              <para>
                Disable some Ceph features by starting the
                <link xlink:href="./ceph-toolbox.md">rook toolbox</link>
                and running
                <literal>ceph osd crush tunables bobtail</literal>
              </para>
            </listitem>
            <listitem>
              <para>
                Upgrade your kernel to <literal>3.15</literal> or later.
              </para>
            </listitem>
          </itemizedlist>
        </section>
        <section xml:id="filesystem-mounting">
          <title>Filesystem Mounting</title>
          <para>
            In the <literal>rook-ceph-agent</literal> pod logs, you may
            see a snippet similar to the following:
          </para>
          <screen>
            2017-11-07 00:04:37.808870 I | rook-flexdriver: WARNING: The node kernel version is 4.4.0-87-generic, which do not support multiple ceph filesystems. The kernel version has to be at least 4.7. If you have multiple ceph filesystems, the result could be inconsistent
          </screen>
          <para>
            This will happen in kernels with versions older than 4.7,
            where the option <literal>mds_namespace</literal> is not
            supported. This option is used to specify a filesystem
            namespace.
          </para>
          <para>
            In this case, if there is only one filesystem in the Rook
            cluster, there should be no issues and the mount should
            succeed. If you have more than one filesystem, inconsistent
            results may arise and the filesystem mounted may not be the
            one you specified.
          </para>
          <para>
            If the issue is still not resolved from the steps above,
            please come chat with us on the
            <emphasis role="strong">#general</emphasis> channel of our
            <link xlink:href="https://slack.rook.io">Rook Slack</link>. We
            want to help you get your storage working and learn from those
            lessons to prevent users in the future from seeing the same
            issue.
          </para>
        </section>
      </section>
    </section>
    <section xml:id="cluster-failing-to-service-requests">
      <title>Cluster failing to service requests</title>
      <section xml:id="symptoms-1">
        <title>Symptoms</title>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              Execution of the <literal>ceph</literal> command hangs
            </para>
          </listitem>
          <listitem>
            <para>
              PersistentVolumes are not being created
            </para>
          </listitem>
          <listitem>
            <para>
              Large amount of slow requests are blocking
            </para>
          </listitem>
          <listitem>
            <para>
              Large amount of stuck requests are blocking
            </para>
          </listitem>
          <listitem>
            <para>
              One or more MONs are restarting periodically
            </para>
          </listitem>
        </itemizedlist>
      </section>
      <section xml:id="investigation">
        <title>Investigation</title>
        <para>
          Create a <link xlink:href="ceph-toolbox.md">rook-ceph-tools
            pod</link> to investigate the current state of Ceph. Here is an
          example of what one might see. In this case the
          <literal>ceph status</literal> command would just hang so a
          CTRL-C needed to be sent.
        </para>
        <screen>
          $ kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l &quot;app=rook-ceph-tools&quot; -o jsonpath='{.items[0].metadata.name}') bash
          root@rook-ceph-tools:/# ceph status
          ^CCluster connection interrupted or timed out
        </screen>
        <para>
          Another indication is when one or more of the MON pods restart
          frequently. Note the <quote>mon107</quote> that has only been up
          for 16 minutes in the following output.
        </para>
        <screen>
          $ kubectl -n rook-ceph get all -o wide --show-all
          NAME                                 READY     STATUS    RESTARTS   AGE       IP               NODE
          po/rook-ceph-mgr0-2487684371-gzlbq   1/1       Running   0          17h       192.168.224.46   k8-host-0402
          po/rook-ceph-mon107-p74rj            1/1       Running   0          16m       192.168.224.28   k8-host-0402
          rook-ceph-mon1-56fgm                 1/1       Running   0          2d        192.168.91.135   k8-host-0404
          rook-ceph-mon2-rlxcd                 1/1       Running   0          2d        192.168.123.33   k8-host-0403
          rook-ceph-osd-bg2vj                  1/1       Running   0          2d        192.168.91.177   k8-host-0404
          rook-ceph-osd-mwxdm                  1/1       Running   0          2d        192.168.123.31   k8-host-0403
        </screen>
      </section>
      <section xml:id="solution">
        <title>Solution</title>
        <para>
          What is happening here is that the MON pods are restarting and
          one or more of the Ceph daemons are not getting configured with
          the proper cluster information. This is commonly the result of
          not specifying a value for <literal>dataDirHostPath</literal> in
          your Cluster CRD.
        </para>
        <para>
          The <literal>dataDirHostPath</literal> setting specifies a path
          on the local host for the Ceph daemons to store configuration
          and data. Setting this to a path like
          <literal>/var/lib/rook</literal>, reapplying your Cluster CRD
          and restarting all the Ceph daemons (MON, MGR, OSD, RGW) should
          solve this problem. After the Ceph daemons have been restarted,
          it is advisable to restart the
          <link xlink:href="./toolbox.md">rook-tool pod</link>.
        </para>
      </section>
    </section>
    <section xml:id="monitors-are-the-only-pods-running">
      <title>Monitors are the only pods running</title>
      <section xml:id="symptoms-2">
        <title>Symptoms</title>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              Rook operator is running
            </para>
          </listitem>
          <listitem>
            <para>
              Either a single mon starts or the mons skip letters,
              specifically named <literal>a</literal>,
              <literal>d</literal>, and <literal>f</literal>
            </para>
          </listitem>
          <listitem>
            <para>
              No mgr, osd, or other daemons are created
            </para>
          </listitem>
        </itemizedlist>
      </section>
      <section xml:id="investigation-1">
        <title>Investigation</title>
        <para>
          When the operator is starting a cluster, the operator will start
          one mon at a time and check that they are healthy before
          continuing to bring up all three mons. If the first mon is not
          detected healthy, the operator will continue to check until it
          is healthy. If the first mon fails to start, a second and then a
          third mon may attempt to start. However, they will never form
          quorum and the orchestration will be blocked from proceeding.
        </para>
        <para>
          The likely causes for the mon health not being detected:
        </para>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              The operator pod does not have network connectivity to the
              mon pod
            </para>
          </listitem>
          <listitem>
            <para>
              The mon pod is failing to start
            </para>
          </listitem>
          <listitem>
            <para>
              One or more mon pods are in running state, but are not able
              to form quorum
            </para>
          </listitem>
        </itemizedlist>
        <section xml:id="operator-fails-to-connect-to-the-mon">
          <title>Operator fails to connect to the mon</title>
          <para>
            First look at the logs of the operator to confirm if it is
            able to connect to the mons.
          </para>
          <screen>
            kubectl -n rook-ceph logs -l app=rook-ceph-operator
          </screen>
          <para>
            Likely you will see an error similar to the following that the
            operator is timing out when connecting to the mon. The last
            command is <literal>ceph mon_status</literal>, followed by a
            timeout message five minutes later.
          </para>
          <screen>
            2018-01-21 21:47:32.375833 I | exec: Running command: ceph mon_status --cluster=rook --conf=/var/lib/rook/rook-ceph/rook.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/442263890
            2018-01-21 21:52:35.370533 I | exec: 2018-01-21 21:52:35.071462 7f96a3b82700  0 monclient(hunting): authenticate timed out after 300
            2018-01-21 21:52:35.071462 7f96a3b82700  0 monclient(hunting): authenticate timed out after 300
            2018-01-21 21:52:35.071524 7f96a3b82700  0 librados: client.admin authentication error (110) Connection timed out
            2018-01-21 21:52:35.071524 7f96a3b82700  0 librados: client.admin authentication error (110) Connection timed out
            [errno 110] error connecting to the cluster
          </screen>
          <para>
            The error would appear to be an authentication error, but it
            is misleading. The real issue is a timeout.
          </para>
        </section>
        <section xml:id="solution-1">
          <title>Solution</title>
          <para>
            If you see the timeout in the operator log, verify if the mon
            pod is running (see the next section). If the mon pod is
            running, check the network connectivity between the operator
            pod and the mon pod. A common issue is that the CNI is not
            configured correctly.
          </para>
        </section>
        <section xml:id="failing-mon-pod">
          <title>Failing mon pod</title>
          <para>
            Second we need to verify if the mon pod started successfully.
          </para>
          <screen>
            $ kubectl -n rook-ceph get pod -l app=rook-ceph-mon
            NAME                                READY     STATUS               RESTARTS   AGE
            rook-ceph-mon-a-69fb9c78cd-58szd    1/1       CrashLoopBackOff     2          47s
          </screen>
          <para>
            If the mon pod is failing as in this example, you will need to
            look at the mon pod status or logs to determine the cause. If
            the pod is in a crash loop backoff state, you should see the
            reason by describing the pod.
          </para>
          <screen>
            # The pod shows a termination status that the keyring does not match the existing keyring
            $ kubectl -n rook-ceph describe pod -l mon=rook-ceph-mon0
            ...
            Last State:    Terminated
            Reason:    Error
            Message:    The keyring does not match the existing keyring in /var/lib/rook/rook-ceph-mon0/data/keyring.
            You may need to delete the contents of dataDirHostPath on the host from a previous deployment.
            ...
          </screen>
          <para>
            See the solution in the next section regarding cleaning up the
            <literal>dataDirHostPath</literal> on the nodes.
          </para>
        </section>
        <section xml:id="three-mons-named-a-d-and-f">
          <title>Three mons named a, d, and f</title>
          <para>
            If you see the three mons running with the names
            <literal>a</literal>, <literal>d</literal>, and
            <literal>f</literal>, they likely did not form quorum even
            though they are running.
          </para>
          <screen>
            NAME                               READY   STATUS    RESTARTS   AGE
            rook-ceph-mon-a-7d9fd97d9b-cdq7g   1/1     Running   0          10m
            rook-ceph-mon-d-77df8454bd-r5jwr   1/1     Running   0          9m2s
            rook-ceph-mon-f-58b4f8d9c7-89lgs   1/1     Running   0          7m38s
          </screen>
        </section>
        <section xml:id="solution-2">
          <title>Solution</title>
          <para>
            This is a common problem reinitializing the Rook cluster when
            the local directory used for persistence has
            <emphasis role="strong">not</emphasis> been purged. This
            directory is the <literal>dataDirHostPath</literal> setting in
            the cluster CRD and is typically set to
            <literal>/var/lib/rook</literal>. To fix the issue you will
            need to delete all components of Rook and then delete the
            contents of <literal>/var/lib/rook</literal> (or the directory
            specified by <literal>dataDirHostPath</literal>) on each of
            the hosts in the cluster. Then when the cluster CRD is applied
            to start a new cluster, the rook-operator should start all the
            pods as expected.
          </para>
          <note>
            <para>
              <emphasis role="strong">IMPORTANT: Deleting the
                <literal>dataDirHostPath</literal> folder is destructive to
                the storage. Only delete the folder if you are trying to
                permanently purge the Rook cluster.</emphasis>
            </para>
          </note>
          <para>
            See the <link xlink:href="ceph-teardown.md">Cleanup
              Guide</link> for more details.
          </para>
        </section>
      </section>
    </section>
    <section xml:id="pvcs-stay-in-pending-state">
      <title>PVCs stay in pending state</title>
      <section xml:id="symptoms-3">
        <title>Symptoms</title>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              When you create a PVC based on a rook storage class, it
              stays pending indefinitely
            </para>
          </listitem>
        </itemizedlist>
        <para>
          For the Wordpress example, you might see two PVCs in pending
          state.
        </para>
        <screen>
          $ kubectl get pvc
          NAME             STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS      AGE
          mysql-pv-claim   Pending                                      rook-ceph-block   8s
          wp-pv-claim      Pending                                      rook-ceph-block   16s
        </screen>
      </section>
      <section xml:id="investigation-2">
        <title>Investigation</title>
        <para>
          There are two common causes for the PVCs staying in pending
          state:
        </para>
        <orderedlist numeration="arabic" spacing="compact">
          <listitem>
            <para>
              There are no OSDs in the cluster
            </para>
          </listitem>
          <listitem>
            <para>
              The CSI provisioner pod is not running or is not responding
              to the request to provision the storage
            </para>
          </listitem>
        </orderedlist>
        <para>
          If you are still using the Rook flex driver for the volumes (the
          CSI driver is the default since Rook v1.1), another cause could
          be that the operator is not running or is otherwise not
          responding to the request to provision the storage.
        </para>
        <section xml:id="confirm-if-there-are-osds">
          <title>Confirm if there are OSDs</title>
          <para>
            To confirm if you have OSDs in your cluster, connect to the
            <link xlink:href="ceph-toolbox.md">Rook Toolbox</link> and run
            the <literal>ceph status</literal> command. You should see
            that you have at least one OSD <literal>up</literal> and
            <literal>in</literal>. The minimum number of OSDs required
            depends on the <literal>replicated.size</literal> setting in
            the pool created for the storage class. In a
            <quote>test</quote> cluster, only one OSD is required (see
            <literal>storageclass-test.yaml</literal>). In the production
            storage class example (<literal>storageclass.yaml</literal>),
            three OSDs would be required.
          </para>
          <screen>
            $ ceph status
            cluster:
            id:     a0452c76-30d9-4c1a-a948-5d8405f19a7c
            health: HEALTH_OK

            services:
            mon: 3 daemons, quorum a,b,c (age 11m)
            mgr: a(active, since 10m)
            osd: 1 osds: 1 up (since 46s), 1 in (since 109m)
          </screen>
        </section>
        <section xml:id="osd-prepare-logs">
          <title>OSD Prepare Logs</title>
          <para>
            If you donâ€™t see the expected number of OSDs, letâ€™s
            investigate why they werenâ€™t created. On each node where Rook
            looks for OSDs to configure, you will see an <quote>osd
              prepare</quote> pod.
          </para>
          <screen>
            $ kubectl -n rook-ceph get pod -l app=rook-ceph-osd-prepare
            NAME                                 ...  READY   STATUS      RESTARTS   AGE
            rook-ceph-osd-prepare-minikube-9twvk   0/2     Completed   0          30m
          </screen>
          <para>
            See the section on
            <link linkend="osd-pods-are-not-created-on-my-devices">why
              OSDs are not getting created</link> to investigate the logs.
          </para>
        </section>
        <section xml:id="csi-driver">
          <title>CSI Driver</title>
          <para>
            The CSI driver may not be responding to the requests. Look in
            the logs of the CSI provisioner pod to see if there are any
            errors during the provisioning.
          </para>
          <para>
            There are two provisioner pods:
          </para>
          <screen>
            kubectl -n rook-ceph get pod -l app=csi-rbdplugin-provisioner
          </screen>
          <para>
            Get the logs of each of the pods. One of them should be the
            <quote>leader</quote> and be responding to requests.
          </para>
          <screen>
            kubectl -n rook-ceph logs csi-cephfsplugin-provisioner-d77bb49c6-q9hwq csi-provisioner
          </screen>
        </section>
        <section xml:id="operator-unresponsiveness">
          <title>Operator unresponsiveness</title>
          <para>
            Lastly, if you have OSDs <literal>up</literal> and
            <literal>in</literal>, the next step is to confirm the
            operator is responding to the requests. Look in the Operator
            pod logs around the time when the PVC was created to confirm
            if the request is being raised. If the operator does not show
            requests to provision the block image, the operator may be
            stuck on some other operation. In this case, restart the
            operator pod to get things going again.
          </para>
        </section>
      </section>
      <section xml:id="solution-3">
        <title>Solution</title>
        <para>
          If the <quote>osd prepare</quote> logs didnâ€™t give you enough
          clues about why the OSDs were not being created, please review
          your
          <link xlink:href="eph-cluster-crd.html#storage-selection-settings">cluster.yaml</link>
          configuration. The common misconfigurations include:
        </para>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              If <literal>useAllDevices: true</literal>, Rook expects to
              find local devices attached to the nodes. If no devices are
              found, no OSDs will be created.
            </para>
          </listitem>
          <listitem>
            <para>
              If <literal>useAllDevices: false</literal>, OSDs will only
              be created if <literal>deviceFilter</literal> is specified.
            </para>
          </listitem>
          <listitem>
            <para>
              Only local devices attached to the nodes will be
              configurable by Rook. In other words, the devices must show
              up under <literal>/dev</literal>.
            </para>
            <itemizedlist spacing="compact">
              <listitem>
                <para>
                  The devices must not have any partitions or filesystems
                  on them. Rook will only configure raw devices.
                  Partitions are not yet supported.
                </para>
              </listitem>
            </itemizedlist>
          </listitem>
        </itemizedlist>
      </section>
    </section>
    <section xml:id="osd-pods-are-failing-to-start">
      <title>OSD pods are failing to start</title>
      <section xml:id="symptoms-4">
        <title>Symptoms</title>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              OSD pods are failing to start
            </para>
          </listitem>
          <listitem>
            <para>
              You have started a cluster after tearing down another
              cluster
            </para>
          </listitem>
        </itemizedlist>
      </section>
      <section xml:id="investigation-3">
        <title>Investigation</title>
        <para>
          When an OSD starts, the device or directory will be configured
          for consumption. If there is an error with the configuration,
          the pod will crash and you will see the CrashLoopBackoff status
          for the pod. Look in the osd pod logs for an indication of the
          failure.
        </para>
        <screen>
          $ kubectl -n rook-ceph logs rook-ceph-osd-fl8fs
          ...
        </screen>
        <para>
          One common case for failure is that you have re-deployed a test
          cluster and some state may remain from a previous deployment. If
          your cluster is larger than a few nodes, you may get lucky
          enough that the monitors were able to start and form quorum.
          However, now the OSDs pods may fail to start due to the old
          state. Looking at the OSD pod logs you will see an error about
          the file already existing.
        </para>
        <screen>
          $ kubectl -n rook-ceph logs rook-ceph-osd-fl8fs
          ...
          2017-10-31 20:13:11.187106 I | mkfs-osd0: 2017-10-31 20:13:11.186992 7f0059d62e00 -1 bluestore(/var/lib/rook/osd0) _read_fsid unparsable uuid
          2017-10-31 20:13:11.187208 I | mkfs-osd0: 2017-10-31 20:13:11.187026 7f0059d62e00 -1 bluestore(/var/lib/rook/osd0) _setup_block_symlink_or_file failed to create block symlink to /dev/disk/by-partuuid/651153ba-2dfc-4231-ba06-94759e5ba273: (17) File exists
          2017-10-31 20:13:11.187233 I | mkfs-osd0: 2017-10-31 20:13:11.187038 7f0059d62e00 -1 bluestore(/var/lib/rook/osd0) mkfs failed, (17) File exists
          2017-10-31 20:13:11.187254 I | mkfs-osd0: 2017-10-31 20:13:11.187042 7f0059d62e00 -1 OSD::mkfs: ObjectStore::mkfs failed with error (17) File exists
          2017-10-31 20:13:11.187275 I | mkfs-osd0: 2017-10-31 20:13:11.187121 7f0059d62e00 -1  ** ERROR: error creating empty object store in /var/lib/rook/osd0: (17) File exists
        </screen>
      </section>
      <section xml:id="solution-4">
        <title>Solution</title>
        <para>
          If the error is from the file that already exists, this is a
          common problem reinitializing the Rook cluster when the local
          directory used for persistence has
          <emphasis role="strong">not</emphasis> been purged. This
          directory is the <literal>dataDirHostPath</literal> setting in
          the cluster CRD and is typically set to
          <literal>/var/lib/rook</literal>. To fix the issue you will need
          to delete all components of Rook and then delete the contents of
          <literal>/var/lib/rook</literal> (or the directory specified by
          <literal>dataDirHostPath</literal>) on each of the hosts in the
          cluster. Then when the cluster CRD is applied to start a new
          cluster, the rook-operator should start all the pods as
          expected.
        </para>
      </section>
    </section>
    <section xml:id="osd-pods-are-not-created-on-my-devices">
      <title>OSD pods are not created on my devices</title>
      <section xml:id="symptoms-5">
        <title>Symptoms</title>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              No OSD pods are started in the cluster
            </para>
          </listitem>
          <listitem>
            <para>
              Devices are not configured with OSDs even though specified
              in the Cluster CRD
            </para>
          </listitem>
          <listitem>
            <para>
              One OSD pod is started on each node instead of multiple pods
              for each device
            </para>
          </listitem>
        </itemizedlist>
      </section>
      <section xml:id="investigation-4">
        <title>Investigation</title>
        <para>
          First, ensure that you have specified the devices correctly in
          the CRD. The
          <link xlink:href="ceph-cluster-crd.md#storage-selection-settings">Cluster
            CRD</link> has several ways to specify the devices that are to
          be consumed by the Rook storage:
        </para>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              <literal>useAllDevices: true</literal>: Rook will consume
              all devices it determines to be available
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>deviceFilter</literal>: Consume all devices that
              match this regular expression
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>devices</literal>: Explicit list of device names on
              each node to consume
            </para>
          </listitem>
        </itemizedlist>
        <para>
          Second, if Rook determines that a device is not available (has
          existing partitions or a formatted filesystem), Rook will skip
          consuming the devices. If Rook is not starting OSDs on the
          devices you expect, Rook may have skipped it for this reason. To
          see if a device was skipped, view the OSD preparation log on the
          node where the device was skipped. Note that it is completely
          normal and expected for OSD prepare pod to be in the
          <literal>completed</literal> state. After the job is complete,
          Rook leaves the pod around in case the logs need to be
          investigated.
        </para>
        <screen>
          # Get the prepare pods in the cluster
          $ kubectl -n rook-ceph get pod -l app=rook-ceph-osd-prepare
          NAME                                   READY     STATUS      RESTARTS   AGE
          rook-ceph-osd-prepare-node1-fvmrp      0/1       Completed   0          18m
          rook-ceph-osd-prepare-node2-w9xv9      0/1       Completed   0          22m
          rook-ceph-osd-prepare-node3-7rgnv      0/1       Completed   0          22m

          # view the logs for the node of interest in the &quot;provision&quot; container
          $ kubectl -n rook-ceph logs rook-ceph-osd-prepare-node1-fvmrp provision
          [...]
        </screen>
        <para>
          Here are some key lines to look for in the log:
        </para>
        <screen>
          # A device will be skipped if Rook sees it has partitions or a filesystem
          2019-05-30 19:02:57.353171 W | cephosd: skipping device sda that is in use
          2019-05-30 19:02:57.452168 W | skipping device &quot;sdb5&quot;: [&quot;Used by ceph-disk&quot;]

          # Other messages about a disk being unusable by ceph include:
          Insufficient space (&lt;5GB) on vgs
          Insufficient space (&lt;5GB)
          LVM detected
          Has BlueStore device label
          locked
          read-only

          # A device is going to be configured
          2019-05-30 19:02:57.535598 I | cephosd: device sdc to be configured by ceph-volume

          # For each device configured you will see a report printed to the log
          2019-05-30 19:02:59.844642 I |   Type            Path                                                    LV Size         % of device
          2019-05-30 19:02:59.844651 I | ----------------------------------------------------------------------------------------------------
          2019-05-30 19:02:59.844677 I |   [data]          /dev/sdc                                                7.00 GB         100%
        </screen>
      </section>
      <section xml:id="solution-5">
        <title>Solution</title>
        <para>
          Either update the CR with the correct settings, or clean the
          partitions or filesystem from your devices. To clean devices
          from a previous install see the
          <link xlink:href="ceph-teardown.md#zapping-devices">cleanup
            guide</link>.
        </para>
        <para>
          After the settings are updated or the devices are cleaned,
          trigger the operator to analyze the devices again by restarting
          the operator. Each time the operator starts, it will ensure all
          the desired devices are configured. The operator does
          automatically deploy OSDs in most scenarios, but an operator
          restart will cover any scenarios that the operator doesnâ€™t
          detect automatically.
        </para>
        <screen>
          # Restart the operator to ensure devices are configured. A new pod will automatically be started when the current operator pod is deleted.
          $ kubectl -n rook-ceph delete pod -l app=rook-ceph-operator
          [...]
        </screen>
      </section>
    </section>
    <section xml:id="node-hangs-after-reboot">
      <title>Node hangs after reboot</title>
      <para>
        This issue is fixed in Rook v1.3 or later.
      </para>
      <section xml:id="symptoms-6">
        <title>Symptoms</title>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              After issuing a <literal>reboot</literal> command, node
              never returned online
            </para>
          </listitem>
          <listitem>
            <para>
              Only a power cycle helps
            </para>
          </listitem>
        </itemizedlist>
      </section>
      <section xml:id="investigation-5">
        <title>Investigation</title>
        <para>
          On a node running a pod with a Ceph persistent volume
        </para>
        <screen>
          $ mount | grep rbd

          # _netdev mount option is absent, also occurs for cephfs
          # OS is not aware PV is mounted over network
          /dev/rbdx on ... (rw,relatime, ..., noquota)
        </screen>
        <para>
          When the reboot command is issued, network interfaces are
          terminated before disks are unmounted. This results in the node
          hanging as repeated attempts to unmount Ceph persistent volumes
          fail with the following error:
        </para>
        <screen>
          libceph: connect [monitor-ip]:6789 error -101
        </screen>
      </section>
      <section xml:id="solution-6">
        <title>Solution</title>
        <para>
          The node needs to be
          <link xlink:href="https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/">drained</link>
          before reboot. After the successful drain, the node can be
          rebooted as usual.
        </para>
        <para>
          Because <literal>kubectl drain</literal> command automatically
          marks the node as unschedulable
          (<literal>kubectl cordon</literal> effect), the node needs to be
          uncordoned once itâ€™s back online.
        </para>
        <para>
          Drain the node:
        </para>
        <screen>
          kubectl drain &lt;node-name&gt; --ignore-daemonsets --delete-local-data
        </screen>
        <para>
          Uncordon the node:
        </para>
        <screen>
          kubectl uncordon &lt;node-name&gt;
        </screen>
      </section>
    </section>
    <section xml:id="rook-agent-modprobe-exec-format-error">
      <title>Rook Agent modprobe exec format error</title>
      <section xml:id="symptoms-7">
        <title>Symptoms</title>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              PersistentVolumes from Ceph fail/timeout to mount
            </para>
          </listitem>
          <listitem>
            <para>
              Rook Agent logs contain
              <literal>modinfo: ERROR: could not get modinfo from 'rbd': Exec format error</literal>
              lines
            </para>
          </listitem>
        </itemizedlist>
      </section>
      <section xml:id="solution-7">
        <title>Solution</title>
        <para>
          If it is feasible to upgrade your kernel, you should upgrade to
          <literal>4.x</literal>, even better is &gt;=
          <literal>4.7</literal> due to a feature for CephFS added to the
          kernel.
        </para>
        <para>
          If you are unable to upgrade the kernel, you need to go to each
          host that will consume storage and run:
        </para>
        <screen>
          modprobe rbd
        </screen>
        <para>
          This command inserts the <literal>rbd</literal> module into the
          kernel.
        </para>
        <para>
          To persist this fix, you need to add the <literal>rbd</literal>
          kernel module to either <literal>/etc/modprobe.d/</literal> or
          <literal>/etc/modules-load.d/</literal>. For both paths create a
          file called <literal>rbd.conf</literal> with the following
          content:
        </para>
        <screen>
          rbd
        </screen>
        <para>
          Now when a host is restarted, the module should be loaded
          automatically.
        </para>
      </section>
    </section>
    <section xml:id="rook-agent-rbd-module-missing-error">
      <title>Rook Agent rbd module missing error</title>
      <section xml:id="symptoms-8">
        <title>Symptoms</title>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              Rook Agent in <literal>Error</literal> or
              <literal>CrashLoopBackOff</literal> status when deploying
              the Rook operator with
              <literal>kubectl create -f operator.yaml</literal>:
            </para>
          </listitem>
        </itemizedlist>
        <screen>
          $kubectl -n rook-ceph get pod
          NAME                                 READY     STATUS    RESTARTS   AGE
          rook-ceph-agent-gfrm5                0/1       Error     0          14s
          rook-ceph-operator-5f4866946-vmtff   1/1       Running   0          23s
          rook-discover-qhx6c                  1/1       Running   0          14s
        </screen>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              Rook Agent logs contain below messages:
            </para>
          </listitem>
        </itemizedlist>
        <screen>
          2018-08-10 09:09:09.461798 I | exec: Running command: cat /lib/modules/4.15.2/modules.builtin
          2018-08-10 09:09:09.473858 I | exec: Running command: modinfo -F parm rbd
          2018-08-10 09:09:09.477215 N | ceph-volumeattacher: failed rbd single_major check, assuming it's unsupported: failed to check for rbd module single_major param: Failed to complete 'check kmod param': exit status 1. modinfo: ERROR: Module rbd not found.
          2018-08-10 09:09:09.477239 I | exec: Running command: modprobe rbd
          2018-08-10 09:09:09.480353 I | modprobe rbd: modprobe: FATAL: Module rbd not found.
          2018-08-10 09:09:09.480452 N | ceph-volumeattacher: failed to load kernel module rbd: failed to load kernel module rbd: Failed to complete 'modprobe rbd': exit status 1.
          failed to run rook ceph agent. failed to create volume manager: failed to load kernel module rbd: Failed to complete 'modprobe rbd': exit status 1.
        </screen>
      </section>
      <section xml:id="solution-8">
        <title>Solution</title>
        <para>
          From the log message of Agent, we can see that the
          <literal>rbd</literal> kernel module is not available in the
          current system, neither as a builtin nor a loadable external
          kernel module.
        </para>
        <para>
          In this case, you have to
          <link xlink:href="https://www.linuxjournal.com/article/6568">re-configure
            and build</link> a new kernel to address this issue, thereâ€™re
          two options:
        </para>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              Re-configure your kernel to make sure the
              <literal>CONFIG_BLK_DEV_RBD=y</literal> in the
              <literal>.config</literal> file, then build the kernel.
            </para>
          </listitem>
          <listitem>
            <para>
              Re-configure your kernel to make sure the
              <literal>CONFIG_BLK_DEV_RBD=m</literal> in the
              <literal>.config</literal> file, then build the kernel.
            </para>
          </listitem>
        </itemizedlist>
        <para>
          Rebooting the system to use the new kernel, this issue should be
          fixed: the Agent will be in normal <literal>running</literal>
          status if everything was done correctly.
        </para>
      </section>
    </section>
    <section xml:id="using-multiple-shared-filesystem-cephfs-is-attempted-on-a-kernel-version-older-than-47">
      <title>Using multiple shared filesystem (CephFS) is attempted on a
        kernel version older than 4.7</title>
      <section xml:id="symptoms-9">
        <title>Symptoms</title>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              More than one shared filesystem (CephFS) has been created in
              the cluster
            </para>
          </listitem>
          <listitem>
            <para>
              A pod attempts to mount any other shared filesystem besides
              the <emphasis role="strong">first</emphasis> one that was
              created
            </para>
          </listitem>
          <listitem>
            <para>
              The pod incorrectly gets the first filesystem mounted
              instead of the intended filesystem
            </para>
          </listitem>
        </itemizedlist>
      </section>
      <section xml:id="solution-9">
        <title>Solution</title>
        <para>
          The only solution to this problem is to upgrade your kernel to
          <literal>4.7</literal> or higher. This is due to a mount flag
          added in the kernel version <literal>4.7</literal> which allows
          to chose the filesystem by name.
        </para>
        <para>
          For additional info on the kernel version requirement for
          multiple shared filesystems (CephFS), see
          <link xlink:href="ceph-filesystem.md#kernel-version-requirement">Filesystem
            - Kernel version requirement</link>.
        </para>
      </section>
    </section>
    <section xml:id="activate-log-to-file-for-a-particular-ceph-daemon">
      <title>Activate log to file for a particular Ceph daemon</title>
      <para>
        They are cases where looking at Kubernetes logs is not enough for
        diverse reasons, but just to name a few:
      </para>
      <itemizedlist spacing="compact">
        <listitem>
          <para>
            not everyone is familiar for Kubernetes logging and expects to
            find logs in traditional directories
          </para>
        </listitem>
        <listitem>
          <para>
            logs get eaten (buffer limit from the log engine) and thus not
            requestable from Kubernetes
          </para>
        </listitem>
      </itemizedlist>
      <para>
        So for each daemon, <literal>dataDirHostPath</literal> is used to
        store logs, if logging is activated. Rook will bindmount
        <literal>dataDirHostPath</literal> for every pod. As of Ceph
        Nautilus 14.2.1, it is possible to enable logging for a particular
        daemon on the fly. Letâ€™s say you want to enable logging for
        <literal>mon.a</literal>, but only for this daemon. Using the
        toolbox or from inside the operator run:
      </para>
      <screen>
        ceph config daemon mon.a log_to_file true
      </screen>
      <para>
        This will activate logging on the filesystem, you will be able to
        find logs in <literal>dataDirHostPath/$NAMESPACE/log</literal>, so
        typically this would mean
        <literal>/var/lib/rook/rook-ceph/log</literal>. You donâ€™t need to
        restart the pod, the effect will be immediate.
      </para>
      <para>
        To disable the logging on file, simply set
        <literal>log_to_file</literal> to <literal>false</literal>.
      </para>
      <para>
        For Ceph Luminous/Mimic releases,
        <literal>mon_cluster_log_file</literal> and
        <literal>cluster_log_file</literal> can be set to
        <literal>/var/log/ceph/XXXX</literal> in the config override
        ConfigMap to enable logging. See the (Advanced
        Documentation)[Documentation/advanced-configuration.md#kubernetes]
        for information about how to use the config override ConfigMap.
      </para>
      <para>
        For Ceph Luminous/Mimic releases,
        <literal>mon_cluster_log_file</literal> and
        <literal>cluster_log_file</literal> can be set to
        <literal>/var/log/ceph/XXXX</literal> in the config override
        ConfigMap to enable logging. See the
        <link linkend="custom-cephconf-settings">Advanced
          Documentation</link> for information about how to use the config
        override ConfigMap.
      </para>
    </section>
    <section xml:id="flex-storage-class-versus-ceph-csi-storage-class">
      <title>Flex storage class versus Ceph CSI storage class</title>
      <para>
        Since Rook 1.1, Ceph CSI has become stable and moving forward is
        the ultimate replacement over the Flex driver. However, not all
        Flex storage classes are available through Ceph CSI since itâ€™s
        basically catching up on features. Ceph CSI in its 1.2 version
        (with Rook 1.1) does not support the Erasure coded pools storage
        class.
      </para>
      <para>
        So, if you are looking at using such storage class you should
        enable the Flex driver by setting
        <literal>ROOK_ENABLE_FLEX_DRIVER: true</literal> in your
        <literal>operator.yaml</literal>. Also, if you are in the need of
        specific features and wonder if CSI is capable of handling them,
        you should read
        <link xlink:href="https://github.com/ceph/ceph-csi#support-matrix">the
          ceph-csi support matrix</link>.
      </para>
    </section>
    <section xml:id="a-worker-node-using-rbd-devices-hangs-up">
      <title>A worker node using RBD devices hangs up</title>
      <section xml:id="symptoms-10">
        <title>Symptoms</title>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              There is no progress on I/O from/to one of RBD devices
              (<literal>/dev/rbd*</literal> or
              <literal>/dev/nbd*</literal>).
            </para>
          </listitem>
          <listitem>
            <para>
              After that, the whole worker node hangs up.
            </para>
          </listitem>
        </itemizedlist>
      </section>
      <section xml:id="investigation-6">
        <title>Investigation</title>
        <para>
          This hapens when the following conditions are satisfied.
        </para>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              The problematic RBD device and the corresponding OSDs are
              co-located.
            </para>
          </listitem>
          <listitem>
            <para>
              There is an XFS filesystem on top of this device.
            </para>
          </listitem>
        </itemizedlist>
        <para>
          In addition, when this problem happens, you can see the
          following messages in <literal>dmesg</literal>.
        </para>
        <screen>
          # dmesg
          ...
          [51717.039319] INFO: task kworker/2:1:5938 blocked for more than 120 seconds.
          [51717.039361]       Not tainted 4.15.0-72-generic #81-Ubuntu
          [51717.039388] &quot;echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs&quot; disables this message.
          ...
        </screen>
        <para>
          Itâ€™s so-called <literal>hung_task</literal> problem and means
          that there is a deadlock in the kernel. For more detail, please
          refer to
          <link xlink:href="https://github.com/rook/rook/issues/3132#issuecomment-580508760">the
            corresponding issue comment</link>.
        </para>
      </section>
      <section xml:id="solution-10">
        <title>Solution</title>
        <para>
          This problem will be solve by the following two fixes.
        </para>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              Linux kernel: A minor feature that is introduced by
              <link xlink:href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=8d19f1c8e1937baf74e1962aae9f90fa3aeab463">this
                commit</link>. It will be included in Linux v5.6.
            </para>
          </listitem>
          <listitem>
            <para>
              Ceph: A fix that uses the above-mentioned kernelâ€™s feature.
              The Ceph community will probably discuss this fix after
              releasing Linux v5.6.
            </para>
          </listitem>
        </itemizedlist>
        <para>
          You can bypass this problem by using ext4 or any other
          filesystems rather than XFS. Filesystem type can be specified
          with <literal>csi.storage.k8s.io/fstype</literal> in
          StorageClass resource.
        </para>
      </section>
    </section>
    <section xml:id="too-few-pgs-per-osd-warning-is-shown">
      <title>Too few PGs per OSD warning is shown</title>
      <section xml:id="symptoms-11">
        <title>Symptoms</title>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              <literal>ceph status</literal> shows <quote>too few PGs per
                OSD</quote> warning as follows.
            </para>
          </listitem>
        </itemizedlist>
        <screen>
          # ceph status
          cluster:
          id:     fd06d7c3-5c5c-45ca-bdea-1cf26b783065
          health: HEALTH_WARN
          too few PGs per OSD (16 &lt; min 30)
        </screen>
      </section>
      <section xml:id="solution-11">
        <title>Solution</title>
        <para>
          The meaning of this warning is written in
          <link xlink:href="https://docs.ceph.com/docs/master/rados/operations/health-checks#too-few-pgs">the
            document</link>. However, in many cases it is benign. For more
          information, please see
          <link xlink:href="http://ceph.com/community/new-luminous-pg-overdose-protection/">the
            blog entry</link>. Please refer to
          <link xlink:href="ceph-advanced-configuration.md#configuring-pools">Configuring
            Pools</link> if you want to know the proper
          <literal>pg_num</literal> of pools and change these values.
        </para>
      </section>
    </section>
    <section xml:id="lvm-metadata-can-be-corrupted-with-osd-on-lv-backed-pvc">
      <title>LVM metadata can be corrupted with OSD on LV-backed
        PVC</title>
      <section xml:id="symptoms-12">
        <title>Symptoms</title>
        <para>
          There is a critical flaw in OSD on LV-backed PVC. LVM metadata
          can be corrupted if both the host and OSD container modify it
          simultaneously. For example, the administrator might modify it
          on the host, while the OSD initialization process in a container
          could modify it too. In addition, if <literal>lvmetad</literal>
          is running, the possibility of occurence gets higher. In this
          case, the change of LVM metadata in OSD container is not
          reflected to LVM metadata cache in host for a while.
        </para>
        <para>
          If you still decide to configure an OSD on LVM, please keep the
          following in mind to reduce the probability of this issue.
        </para>
      </section>
      <section xml:id="solution-12">
        <title>Solution</title>
        <itemizedlist spacing="compact">
          <listitem>
            <para>
              Disable <literal>lvmetad.</literal>
            </para>
          </listitem>
          <listitem>
            <para>
              Avoid configuration of LVs from the host. In addition, donâ€™t
              touch the VGs and physical volumes that back these LVs.
            </para>
          </listitem>
          <listitem>
            <para>
              Avoid incrementing the <literal>count</literal> field of
              <literal>storageClassDeviceSets</literal> and create a new
              LV that backs a OSD simultaneously.
            </para>
          </listitem>
        </itemizedlist>
        <para>
          You can know whether the above-mentioned tag exists tag with the
          command: <literal>sudo lvs -o lv_name,lv_tags</literal>. If the
          <literal>lv_tag</literal> field is empty in an LV corresponding
          to the OSD lv_tags, this OSD encountered the problem. In this
          case, please
          <link xlink:href="ceph-osd-mgmt.md#remove-an-osd">retire this
            OSD</link> or replace with other new OSD before restarting.
        </para>
      </section>
    </section>
  </section>

</chapter>
