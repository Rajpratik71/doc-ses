<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
  xml:id="admin-caasp-cephconfig">
  <!-- ============================================================== -->
  <title>Configuration</title>
  <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
  <remark>
    Content from: https://rook.io/docs/rook/v1.4/ceph-configuration.html
  </remark>

  <section xml:id="configuration">
    <title>Configuration</title>
    <para>
      For most any Ceph cluster, the user will want to–and may need
      to–change some Ceph configurations. These changes often may be
      warranted in order to alter performance to meet SLAs or to update
      default data resiliency settings.
    </para>
    <note>
      <para>
        <emphasis role="strong">WARNING</emphasis>: Modify Ceph settings
        carefully, and review the
        <link xlink:href="https://docs.ceph.com/docs/master/rados/configuration/">Ceph
          configuration documentation</link> before making any changes.
        Changing the settings could result in unhealthy daemons or even
        data loss if used incorrectly.
      </para>
    </note>
    <section xml:id="required-configurations">
      <title>Required configurations</title>
      <para>
        Rook and Ceph both strive to make configuration as easy as
        possible, but there are some configuration options which users are
        well advised to consider for any production cluster.
      </para>
      <section xml:id="default-pg-and-pgp-counts">
        <title>Default PG and PGP counts</title>
        <para>
          The number of PGs and PGPs can be configured on a per-pool
          basis, but it is highly advised to set default values that are
          appropriate for your Ceph cluster. Appropriate values depend on
          the number of OSDs the user expects to have backing each pool.
          The Ceph
          <link xlink:href="https://docs.ceph.com/docs/master/rados/operations/placement-groups/#a-preselection-of-pg-num">OSD
            and Pool config docs</link> provide detailed information about
          how to tune these parameters:
          <literal>osd_pool_default_pg_num</literal> and
          <literal>osd_pool_default_pgp_num</literal>.
        </para>
        <para>
          Pools created prior to v1.1 will have a default PG count of 100.
          Pools created after v1.1 will have Ceph’s default PG count.
        </para>
        <para>
          An easier option exists for Rook-Ceph clusters running Ceph
          Nautilus (v14.2.x) or newer. Nautilus
          <link xlink:href="https://ceph.com/rados/new-in-nautilus-pg-merging-and-autotuning/">introduced
            the PG auto-scaler mgr module</link> capable of automatically
          managing PG and PGP values for pools. Please see
          <link xlink:href="https://ceph.io/rados/new-in-nautilus-pg-merging-and-autotuning/">Ceph
            New in Nautilus: PG merging and autotuning</link> for more
          information about this module.
        </para>
        <para>
          In Nautilus, This module is not enabled by default, but can be
          enabled by the following setting in the
          <link xlink:href="ceph-cluster-crd.md#mgr-settings">CephCluster
            CR</link>:
        </para>
        <screen>
          spec:
          mgr:
          modules:
          - name: pg_autoscaler
          enabled: true
        </screen>
        <para>
          In Octopus (v15.2.x), this module is enabled by default without
          the above-mentioned setting.
        </para>
        <para>
          With that setting, the autoscaler will be enabled for all new
          pools. If you do not desire to have the autoscaler enabled for
          all new pools, you will need to use the Rook toolbox to enable
          the module and
          <link xlink:href="https://docs.ceph.com/docs/master/rados/operations/placement-groups/">enable
            the autoscaling</link> on individual pools.
        </para>
        <para>
          The autoscaler is not enabled for the existing pools after
          enabling the module. So if you want to enable the autoscaling
          for these existing pools, they must be configured from the
          toolbox.
        </para>
      </section>
    </section>
    <section xml:id="specifying-configuration-options">
      <title>Specifying configuration options</title>
      <section xml:id="toolbox-ceph-cli">
        <title>Toolbox + Ceph CLI</title>
        <para>
          The most recommended way of configuring Ceph is to set Ceph’s
          configuration directly. The first method for doing so is to use
          Ceph’s CLI from the Rook-Ceph toolbox pod. Using the toolbox pod
          is detailed <link xlink:href="ceph-toolbox.md">here</link>. From
          the toolbox, the user can change Ceph configurations, enable
          manager modules, create users and pools, and much more.
        </para>
      </section>
      <section xml:id="rook-ceph-dashboard">
        <title>Ceph Dashboard</title>
        <para>
          The Ceph Dashboard, examined in more detail
          <link xlink:href="ceph-dashboard.md">here</link>, is another way
          of setting some of Ceph’s configuration directly. Configuration
          by the Ceph dashboard is recommended with the same priority as
          configuration via the Ceph CLI (above).
        </para>
      </section>
      <section xml:id="advanced-configuration-via-ceph-conf-override-configmap">
        <title>Advanced configuration via ceph.conf override
          ConfigMap</title>
        <para>
          Setting configs via Ceph’s CLI requires that at least one mon be
          available for the configs to be set, and setting configs via
          dashboard requires at least one mgr to be available. Ceph may
          also have a small number of very advanced settings that aren’t
          able to be modified easily via CLI or dashboard. The
          <emphasis role="strong">least</emphasis> recommended method for
          configuring Ceph is intended as a last-resort fallback in
          situations like these. This is covered in detail
          <link xlink:href="ceph-advanced-configuration.md#custom-cephconf-settings">here</link>.
        </para>
      </section>
    </section>
  </section>

</chapter>
