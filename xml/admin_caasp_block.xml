<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
  xml:id="admin-caasp-block-storage">
  <!-- ============================================================== -->
  <title>&blockstore;</title>
  <remark>
    Content from: https://rook.io/docs/rook/v1.4/ceph-block.html
    Ignore "Flex Driver" section
  </remark>
    <para>
      Block storage allows a single pod to mount storage. This guide shows
      how to create a simple, multi-tier web application on Kubernetes
      using persistent volumes enabled by Rook.
    </para>
    <section xml:id="rook-block-storage-prerequisites">
      <title>Prerequisites</title>
      <para>
        This guide assumes a Rook cluster as explained in the
        <link xlink:href="ceph-quickstart.md">Quickstart</link>.
      </para>
    </section>
    <section xml:id="rook-block-storage-provision-storage">
      <title>Provision Storage</title>
      <para>
        Before Rook can provision storage, a
        <link xlink:href="https://kubernetes.io/docs/concepts/storage/storage-classes"><literal>StorageClass</literal></link>
        and
        <link xlink:href="ceph-pool-crd.md"><literal>CephBlockPool</literal></link>
        need to be created. This will allow Kubernetes to interoperate
        with Rook when provisioning persistent volumes.
      </para>
      <note>
        <para>
          <emphasis role="strong">NOTE</emphasis>: This sample requires
          <emphasis>at least 1 OSD per node</emphasis>, with each OSD
          located on <emphasis>3 different nodes</emphasis>.
        </para>
      </note>
      <para>
        Each OSD must be located on a different node, because the
        <link xlink:href="ceph-pool-crd.md#spec"><literal>failureDomain</literal></link>
        is set to <literal>host</literal> and the
        <literal>replicated.size</literal> is set to <literal>3</literal>.
      </para>
      <note>
        <para>
          <emphasis role="strong">NOTE</emphasis>: This example uses the
          CSI driver, which is the preferred driver going forward for K8s
          1.13 and newer. Examples are found in the
          <link xlink:href="https://github.com/rook/rook/tree/%7B%7B%20branchName%20%7D%7D/cluster/examples/kubernetes/ceph/csi/rbd">CSI
            RBD</link> directory. For an example of a storage class using
          the flex driver (required for K8s 1.12 or earlier), see the
          <link linkend="rook-block-storage-flex-driver">Flex Driver</link> section below,
          which has examples in the
          <link xlink:href="https://github.com/rook/rook/tree/%7B%7B%20branchName%20%7D%7D/cluster/examples/kubernetes/ceph/flex">flex</link>
          directory.
        </para>
      </note>
      <para>
        Save this <literal>StorageClass</literal> definition as
        <literal>storageclass.yaml</literal>:
      </para>
      <programlisting language="yaml">
        apiVersion: ceph.rook.io/v1
        kind: CephBlockPool
        metadata:
        name: replicapool
        namespace: rook-ceph
        spec:
        failureDomain: host
        replicated:
        size: 3
        ---
        apiVersion: storage.k8s.io/v1
        kind: StorageClass
        metadata:
        name: rook-ceph-block
        # Change &quot;rook-ceph&quot; provisioner prefix to match the operator namespace if needed
        provisioner: rook-ceph.rbd.csi.ceph.com
        parameters:
        # clusterID is the namespace where the rook cluster is running
        clusterID: rook-ceph
        # Ceph pool into which the RBD image shall be created
        pool: replicapool

        # RBD image format. Defaults to &quot;2&quot;.
        imageFormat: &quot;2&quot;

        # RBD image features. Available for imageFormat: &quot;2&quot;. CSI RBD currently supports only `layering` feature.
        imageFeatures: layering

        # The secrets contain Ceph admin credentials.
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

        # Specify the filesystem type of the volume. If not specified, csi-provisioner
        # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
        # in hyperconverged settings where the volume is mounted on the same node as the osds.
        csi.storage.k8s.io/fstype: ext4

        # Delete the rbd volume when a PVC is deleted
        reclaimPolicy: Delete
      </programlisting>
      <para>
        If youâ€™ve deployed the Rook operator in a namespace other than
        <quote>rook-ceph</quote>, change the prefix in the provisioner to
        match the namespace you used. For example, if the Rook operator is
        running in the namespace <quote>my-namespace</quote> the
        provisioner value should be
        <quote>my-namespace.rbd.csi.ceph.com</quote>.
      </para>
      <para>
        Create the storage class.
      </para>
      <programlisting>
        kubectl create -f cluster/examples/kubernetes/ceph/csi/rbd/storageclass.yaml
      </programlisting>
      <note>
        <para>
          <emphasis role="strong">NOTE</emphasis>: As
          <link xlink:href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#retain">specified
            by Kubernetes</link>, when using the <literal>Retain</literal>
          reclaim policy, any Ceph RBD image that is backed by a
          <literal>PersistentVolume</literal> will continue to exist even
          after the <literal>PersistentVolume</literal> has been deleted.
          These Ceph RBD images will need to be cleaned up manually using
          <literal>rbd rm</literal>.
        </para>
      </note>
    </section>
    <section xml:id="consume-the-storage-wordpress-sample">
      <title>Consume the storage: Wordpress sample</title>
      <para>
        We create a sample app to consume the block storage provisioned by
        Rook with the classic wordpress and mysql apps. Both of these apps
        will make use of block volumes provisioned by Rook.
      </para>
      <para>
        Start mysql and wordpress from the
        <literal>cluster/examples/kubernetes</literal> folder:
      </para>
      <programlisting>
        kubectl create -f mysql.yaml
        kubectl create -f wordpress.yaml
      </programlisting>
      <para>
        Both of these apps create a block volume and mount it to their
        respective pod. You can see the Kubernetes volume claims by
        running the following:
      </para>
      <programlisting>
        $ kubectl get pvc
        NAME             STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
        mysql-pv-claim   Bound     pvc-95402dbc-efc0-11e6-bc9a-0cc47a3459ee   20Gi       RWO           1m
        wp-pv-claim      Bound     pvc-39e43169-efc1-11e6-bc9a-0cc47a3459ee   20Gi       RWO           1m
      </programlisting>
      <para>
        Once the wordpress and mysql pods are in the
        <literal>Running</literal> state, get the cluster IP of the
        wordpress app and enter it in your browser:
      </para>
      <programlisting>
        $ kubectl get svc wordpress
        NAME        CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGE
        wordpress   10.3.0.155   &lt;pending&gt;     80:30841/TCP   2m
      </programlisting>
      <para>
        You should see the wordpress app running.
      </para>
      <para>
        If you are using Minikube, the Wordpress URL can be retrieved with
        this one-line command:
      </para>
      <programlisting>
        echo http://$(minikube ip):$(kubectl get service wordpress -o jsonpath='{.spec.ports[0].nodePort}')
      </programlisting>
      <note>
        <para>
          <emphasis role="strong">NOTE</emphasis>: When running in a
          vagrant environment, there will be no external IP address to
          reach wordpress with. You will only be able to reach wordpress
          via the <literal>CLUSTER-IP</literal> from inside the Kubernetes
          cluster.
        </para>
      </note>
    </section>
    <section xml:id="consume-the-storage-toolbox">
      <title>Consume the storage: Toolbox</title>
      <para>
        With the pool that was created above, we can also create a block
        image and mount it directly in a pod. See the
        <link xlink:href="direct-tools.md#block-storage-tools">Direct
          Block Tools</link> topic for more details.
      </para>
    </section>
    <section xml:id="rook-block-storage-teardown">
      <title>Teardown</title>
      <para>
        To clean up all the artifacts created by the block demo:
      </para>
      <programlisting>
        kubectl delete -f wordpress.yaml
        kubectl delete -f mysql.yaml
        kubectl delete -n rook-ceph cephblockpools.ceph.rook.io replicapool
        kubectl delete storageclass rook-ceph-block
      </programlisting>
    </section>
    <section xml:id="rook-block-storage-flex-driver">
      <title>Flex Driver</title>
      <para>
        To create a volume based on the flex driver instead of the CSI
        driver, see the following example of a storage class. Make sure
        the flex driver is enabled over Ceph CSI. For this, you need to
        set <literal>ROOK_ENABLE_FLEX_DRIVER</literal> to
        <literal>true</literal> in your operator deployment in the
        <literal>operator.yaml</literal> file. The pool definition is the
        same as for the CSI driver.
      </para>
      <programlisting language="yaml">
        apiVersion: ceph.rook.io/v1
        kind: CephBlockPool
        metadata:
        name: replicapool
        namespace: rook-ceph
        spec:
        failureDomain: host
        replicated:
        size: 3
        ---
        apiVersion: storage.k8s.io/v1
        kind: StorageClass
        metadata:
        name: rook-ceph-block
        provisioner: ceph.rook.io/block
        parameters:
        blockPool: replicapool
        # The value of &quot;clusterNamespace&quot; MUST be the same as the one in which your rook cluster exist
        clusterNamespace: rook-ceph
        # Specify the filesystem type of the volume. If not specified, it will use `ext4`.
        fstype: ext4
        # Optional, default reclaimPolicy is &quot;Delete&quot;. Other options are: &quot;Retain&quot;, &quot;Recycle&quot; as documented in https://kubernetes.io/docs/concepts/storage/storage-classes/
        reclaimPolicy: Retain
        # Optional, if you want to add dynamic resize for PVC. Works for Kubernetes 1.14+
        # For now only ext3, ext4, xfs resize support provided, like in Kubernetes itself.
        allowVolumeExpansion: true
      </programlisting>
      <para>
        Create the pool and storage class using
        <literal>kubectl</literal>:
      </para>
      <programlisting>
        kubectl create -f cluster/examples/kubernetes/ceph/flex/storageclass.yaml
      </programlisting>
      <para>
        Continue with the example above for the
        <link linkend="consume-the-storage-wordpress-sample">wordpress
          application</link>.
      </para>
    </section>
    <section xml:id="advanced-example-erasure-coded-block-storage">
      <title>Advanced Example: Erasure Coded Block Storage</title>
      <para>
        If you want to use erasure coded pool with RBD, your OSDs must use
        <literal>bluestore</literal> as their
        <literal>storeType</literal>. Additionally the nodes that are
        going to mount the erasure coded RBD block storage must have Linux
        kernel &gt;= <literal>4.11</literal>.
      </para>
      <para>
        <emphasis role="strong">NOTE</emphasis>: This example requires
        <emphasis>at least 3 bluestore OSDs</emphasis>, with each OSD
        located on a <emphasis>different node</emphasis>.
      </para>
      <para>
        The OSDs must be located on different nodes, because the
        <link xlink:href="ceph-pool-crd.md#spec"><literal>failureDomain</literal></link>
        is set to <literal>host</literal> and the
        <literal>erasureCoded</literal> chunk settings require at least 3
        different OSDs (2 <literal>dataChunks</literal> + 1
        <literal>codingChunks</literal>).
      </para>
      <para>
        To be able to use an erasure coded pool you need to create two
        pools (as seen below in the definitions): one erasure coded and
        one replicated. &gt; <emphasis role="strong">NOTE</emphasis>: This
        example requires <emphasis>at least 3 bluestore OSDs</emphasis>,
        with each OSD located on a <emphasis>different node</emphasis>.
      </para>
      <para>
        The OSDs must be located on different nodes, because the
        <link xlink:href="ceph-pool-crd.md#spec"><literal>failureDomain</literal></link>
        is set to <literal>host</literal> and the
        <literal>erasureCoded</literal> chunk settings require at least 3
        different OSDs (2 <literal>dataChunks</literal> + 1
        <literal>codingChunks</literal>).
      </para>
      <section xml:id="erasure-coded-csi-driver">
        <title>Erasure Coded CSI Driver</title>
        <para>
          The erasure coded pool must be set as the
          <literal>dataPool</literal> parameter in
          <link xlink:href="https://github.com/rook/rook/blob/%7B%7B%20branchName%20%7D%7D/cluster/examples/kubernetes/ceph/csi/rbd/storage-class-ec.yaml"><literal>storageclass-ec.yaml</literal></link>
          It is used for the data of the RBD images.
        </para>
      </section>
      <section xml:id="erasure-coded-flex-driver">
        <title>Erasure Coded Flex Driver</title>
        <para>
          The erasure coded pool must be set as the
          <literal>dataBlockPool</literal> parameter in
          <link xlink:href="https://github.com/rook/rook/blob/%7B%7B%20branchName%20%7D%7D/cluster/examples/kubernetes/ceph/flex/storage-class-ec.yaml"><literal>storageclass-ec.yaml</literal></link>.
          It is used for the data of the RBD images.
        </para>
      </section>
    </section>

</chapter>
