<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_ganesha.xml" version="5.0" xml:id="cha-as-ganesha">

 <title>Installation de NFS Ganesha</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>modification</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>oui</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  NFS Ganesha fournit un accès NFS à Object Gateway ou à CephFS. Dans SUSE Enterprise Storage 5, les versions 3 et 4 de NFS sont prises en charge. NFS Ganesha s'exécute dans l'espace utilisateur au lieu de l'espace kernel, et interagit directement avec Object Gateway ou CephFS.
 </para>
 <sect1 xml:id="sec-as-ganesha-preparation">
  <title>Préparation</title>

  <sect2 xml:id="sec-as-ganesha-preparation-general">
   <title>Informations générales</title>
   <para>
    Pour déployer NFS Ganesha, vous devez ajouter un élément <literal>role-ganesha</literal> à votre fichier <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>. Pour plus d'informations, reportez-vous à la <xref linkend="policy-configuration"/>. NFS Ganesha a également besoin d'un <literal>role-rgw</literal> ou d'un <literal>role-mds</literal> présent dans le fichier <filename>policy.cfg</filename>.
   </para>
   <para>
    Bien qu'il soit possible d'installer et d'exécuter le serveur NFS Ganesha sur un noeud Ceph déjà existant, il est recommandé de l'exécuter sur un hôte dédié disposant d'un accès à la grappe Ceph. Les hôtes du client ne font généralement pas partie de la grappe, mais ils doivent disposer d'un accès réseau au serveur NFS Ganesha.
   </para>
   <para>
    Pour activer le serveur NFS Ganesha à tout moment après l'installation initiale, ajoutez le <literal>role-ganesha</literal> au fichier <filename>policy.cfg</filename> et réexécutez au moins les phases 2 et 4 de DeepSea. Pour plus d'informations, reportez-vous à la <xref linkend="ceph-install-stack"/>.
   </para>
   <para>
    NFS Ganesha est configuré via le fichier <filename>/etc/ganesha/ganesha.conf</filename> qui existe sur le noeud NFS Ganesha. Toutefois, ce fichier est écrasé chaque fois que la phase 4 de DeepSea est exécutée. Par conséquent, il est recommandé de modifier le modèle utilisé par Salt, qui est le fichier <filename>/srv/salt/ceph/ganesha/files/ganesha.conf.j2</filename> sur Salt Master. Pour plus d'informations sur le fichier de configuration, reportez-vous au <xref linkend="ceph-nfsganesha-config"/>.
   </para>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-preparation-requirements">
   <title>Résumé des conditions requises</title>
   <para>
    Les conditions suivantes doivent être remplies avant de pouvoir exécuter les phases 2 et 4 de DeepSea pour installer NFS Ganesha :
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Au moins un noeud doit être assigné au <literal>role-ganesha</literal>.
     </para>
    </listitem>
    <listitem>
     <para>
      Vous ne pouvez définir qu'un seul <literal>role-ganesha</literal> par minion.
     </para>
    </listitem>
    <listitem>
     <para>
      NFS Ganesha a besoin d'une instance Object Gateway ou de CephFS pour fonctionner.
     </para>
    </listitem>
    <listitem>
     <para>
      Si NFS Ganesha est supposé utiliser Object Gateway pour s'interfacer avec la grappe, le fichier <filename>/srv/pillar/ceph/rgw.sls</filename> de Salt Master doit être rempli.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-basic-example">
  <title>Exemple d'installation</title>

  <para>
   Cette procédure fournit un exemple d'installation qui utilise à la fois la passerelle Object Gateway et les couches d'abstraction du système de fichiers (FSAL) CephFS de NFS Ganesha.
  </para>

  <procedure>
   <step>
    <para>
     Si vous ne l'avez pas encore fait, exécutez les phases 0 et 1 de DeepSea avant de poursuivre cette procédure.
    </para>
<screen><prompt>root # </prompt><command>salt-run</command> state.orch ceph.stage.0
<prompt>root # </prompt><command>salt-run</command> state.orch ceph.stage.1</screen>
   </step>
   <step>
    <para>
     Après avoir exécuté la phase 1 de DeepSea, modifiez le fichier <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> et ajoutez la ligne
    </para>
<screen>role-ganesha/cluster/<replaceable>NODENAME</replaceable></screen>
    <para>
     Remplacez <replaceable>NODENAME</replaceable> (NOM_NOEUD) par le nom d'un noeud de la grappe.
    </para>
    <para>
     Vérifiez également qu'un <literal>role-mds</literal> et un <literal>role-rgw</literal> sont assignés.
    </para>
   </step>
   <step>
    <para>
     Créez le fichier <filename>/srv/pillar/ceph/rgw.sls</filename> et insérez le contenu suivant :
    </para>
<screen>rgw_configurations:
  rgw:
    users:
      - { uid: "demo", name: "Demo", email: "demo@demo.nil" }
      - { uid: "demo1", name: "Demo1", email: "demo1@demo.nil" }</screen>
    <para>
     Ces utilisateurs sont créés par la suite en tant qu'utilisateurs d'Object Gateway, et les clés d'API sont générées. Sur le noeud Object Gateway, vous pouvez ensuite exécuter <command>radosgw-admin user list</command> pour répertorier tous les utilisateurs créés et <command>radosgw-admin user info --uid=demo</command> pour obtenir des détails sur des utilisateurs spécifiques.
    </para>
    <para>
     DeepSea s'assure qu'Object Gateway et NFS Ganesha reçoivent les informations d'identification de tous les utilisateurs répertoriés dans la section <literal>rgw</literal> du fichier <filename>rgw.sls</filename>.
    </para>
    <para>
     Le NFS exporté utilise ces noms d'utilisateur au premier niveau du système de fichiers ; dans cet exemple, les chemins <filename>/demo</filename> and <filename>/demo1</filename> sont exportés.
    </para>
   </step>
   <step>
    <para>
     Exécutez au moins les phases 2 et 4 de DeepSea. L'exécution de la phase 3 entre les deux est recommandée.
    </para>
<screen><prompt>root # </prompt><command>salt-run</command> state.orch ceph.stage.2
<prompt>root # </prompt><command>salt-run</command> state.orch ceph.stage.3 # optional but recommended
<prompt>root # </prompt><command>salt-run</command> state.orch ceph.stage.4</screen>
   </step>
   <step>
    <para>
     Vérifiez que NFS Ganesha fonctionne en montant le partage NFS à partir d'un noeud client :
    </para>
<screen><prompt>root # </prompt><command>mount</command> -o sync -t nfs <replaceable>GANESHA_NODE</replaceable>:/ /mnt
<prompt>root # </prompt><command>ls</command> /mnt
cephfs  demo  demo1</screen>
    <para>
     <filename>/mnt</filename> doit contenir tous les chemins exportés. Des répertoires pour les utilisateurs de CephFS, comme pour ceux du noeud Object Gateway, doivent exister. Pour chaque compartiment qu'un utilisateur possède, un chemin <filename>/mnt/<replaceable>USERNAME</replaceable>/<replaceable>BUCKETNAME</replaceable></filename> (NOM_UTILISATEUR, NOM_COMPARTIMENT) est exporté.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-ha-ap">
  <title>Configuration haute disponibilité active-passive</title>

  <para>
   Cette section fournit un exemple de configuration active-passive à deux noeuds des serveurs NFS Ganesha. La configuration requiert l'extension SUSE Linux Enterprise High Availability Extension. Les deux noeuds sont appelés <systemitem class="domainname">earth</systemitem> et <systemitem class="domainname">mars</systemitem>.
  </para>

  <para>
   Pour plus d'informations sur l'extension SUSE Linux Enterprise High Availability Extension, reportez-vous à l'adresse <link xlink:href="https://www.suse.com/documentation/sle-ha-12/"/>.
  </para>

  <sect2 xml:id="sec-as-ganesha-ha-ap-basic">
   <title>Installation classique</title>
   <para>
    Dans cette configuration, <systemitem class="domainname">earth</systemitem> a l'adresse IP <systemitem class="ipaddress">192.168.1.1</systemitem> et <systemitem class="domainname">mars</systemitem> <systemitem class="ipaddress">192.168.1.2</systemitem>.
   </para>
   <para>
    Par ailleurs, deux adresses IP virtuelles flottantes sont utilisées, permettant aux clients de se connecter au service indépendamment du noeud physique sur lequel il s'exécute. L'adresse IP <systemitem class="ipaddress">192.168.1.10</systemitem> est utilisée pour l'administration des grappes avec Hawk2 et <systemitem class="ipaddress">192.168.2.1</systemitem> est utilisée exclusivement pour les exportations NFS. Cela facilite l'application des restrictions de sécurité ultérieurement.
   </para>
   <para>
    La procédure suivante décrit l'exemple d'installation. Pour plus d'informations, reportez-vous à l'adresse <link xlink:href="https://www.suse.com/documentation/sle-ha-12/install-quick/data/install-quick.html"/>.
   </para>
   <procedure xml:id="proc-as-ganesha-ha-ap">
    <step>
     <para>
      Préparez les noeuds NFS Ganesha sur Salt Master :
     </para>
     <substeps>
      <step>
       <para>
        Exécutez les phases 0 et 1 de DeepSea sur Salt Master.
       </para>
<screen>
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.0
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.1
</screen>
      </step>
      <step>
       <para>
        Assignez aux noeuds <systemitem class="domainname">earth</systemitem> et <systemitem class="domainname">mars</systemitem> le <literal>role-ganesha</literal> dans le fichier <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> :
       </para>
<screen>role-ganesha/cluster/earth*.sls
role-ganesha/cluster/mars*.sls</screen>
      </step>
      <step>
       <para>
        Exécutez les phases 3 et 4 de DeepSea sur Salt Master.
       </para>
<screen><prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.3
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.4</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Enregistrez l'extension SUSE Linux Enterprise High Availability Extension sur <systemitem class="domainname">earth</systemitem> et <systemitem class="domainname">mars</systemitem>.
     </para>
<screen>
<prompt>root # </prompt><command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable>
</screen>
    </step>
    <step>
     <para>
      Installez <package>ha-cluster-bootstrap</package> sur les deux noeuds :
     </para>
<screen><prompt>root # </prompt><command>zypper</command> in ha-cluster-bootstrap</screen>
    </step>
    <step>
     <substeps>
      <step>
       <para>
        Initialisez la grappe sur <systemitem class="domainname">earth</systemitem> :
       </para>
<screen><prompt>root@earth # </prompt><command>ha-cluster-init</command></screen>
      </step>
      <step>
       <para>
        Permettez à <systemitem class="domainname">mars</systemitem> de rejoindre la grappe :
       </para>
<screen><prompt>root@mars # </prompt><command>ha-cluster-join</command> -c earth</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Vérifiez l'état de la grappe. Vous devriez voir deux noeuds ajoutés à la grappe :
     </para>
<screen><prompt>root@earth # </prompt><command>crm</command> status</screen>
    </step>
    <step>
     <para>
      Sur les deux noeuds, désactivez le lancement automatique du service NFS Ganesha au démarrage :
     </para>
<screen><prompt>root # </prompt><command>systemctl</command> disable nfs-ganesha</screen>
    </step>
    <step>
     <para>
      Lancez le shell <command>crm</command> sur <systemitem class="domainname">earth</systemitem> :
     </para>
<screen><prompt>root@earth # </prompt><command>crm</command> configure</screen>
     <para>
      Les commandes suivantes sont exécutées avec le shell crm.
     </para>
    </step>
    <step>
     <para>
      Sur <systemitem class="domainname">earth</systemitem>, exécutez le shell crm afin d'exécuter les commandes suivantes permettant de configurer la ressource pour les daemons NFS Ganesha en tant que clone du type de ressource systemd :
     </para>
<screen>
<prompt>crm(live)configure# </prompt>primitive nfs-ganesha-server systemd:nfs-ganesha \
op monitor interval=30s
<prompt>crm(live)configure# </prompt>clone nfs-ganesha-clone nfs-ganesha-server meta interleave=true
<prompt>crm(live)configure# </prompt>commit
<prompt>crm(live)configure# </prompt>status
    2 nodes configured
    2 resources configured

    Online: [ earth mars ]

    Full list of resources:
         Clone Set: nfs-ganesha-clone [nfs-ganesha-server]
         Started:  [ earth mars ]</screen>
    </step>
    <step>
     <para>
      Créez une IPAddr2 primitive avec le shell crm :
     </para>
<screen>
<prompt>crm(live)configure# </prompt>primitive ganesha-ip IPaddr2 \
params ip=192.168.2.1 cidr_netmask=24 nic=eth0 \
op monitor interval=10 timeout=20

<prompt>crm(live)# </prompt>status
Online: [ earth mars  ]
Full list of resources:
 Clone Set: nfs-ganesha-clone [nfs-ganesha-server]
     Started: [ earth mars ]
 ganesha-ip    (ocf::heartbeat:IPaddr2):    Started earth</screen>
    </step>
    <step>
     <para>
      Pour configurer une relation entre le serveur NFS Ganesha et l'adresse IP virtuelle flottante, nous utilisons la colocalisation et le classement.
     </para>
<screen>
<prompt>crm(live)configure# </prompt>colocation ganesha-ip-with-nfs-ganesha-server inf: ganesha-ip nfs-ganesha-clone
<prompt>crm(live)configure# </prompt>order ganesha-ip-after-nfs-ganesha-server Mandatory: nfs-ganesha-clone ganesha-ip
</screen>
    </step>
    <step>
     <para>
      Utilisez la commande <command>mount</command> à partir du client pour vous assurer que la configuration de la grappe est terminée :
     </para>
<screen><prompt>root # </prompt><command>mount</command> -t nfs -v -o sync,nfsvers=4 192.168.2.1:/ /mnt</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-ha-ap-cleanup">
   <title>Nettoyage des ressources</title>
   <para>
    En cas d'échec de NFS Ganesha sur l'un des noeuds, par exemple <systemitem class="domainname">earth</systemitem>, résolvez le problème et nettoyez la ressource. Ce n'est que lorsque la ressource est nettoyée qu'elle peut effectuer un basculement vers <systemitem class="domainname">earth</systemitem> en cas d'échec de NFS Ganesha au niveau de <systemitem class="domainname">mars</systemitem>.
   </para>
   <para>
    Pour nettoyer la ressource :
   </para>
<screen><prompt>root@earth # </prompt><command>crm</command> resource cleanup nfs-ganesha-clone earth
<prompt>root@earth # </prompt><command>crm</command> resource cleanup ganesha-ip earth</screen>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-ha-ap-ping-resource">
   <title>Configuration d'une ressource ping</title>
   <para>
    Il peut arriver que le serveur ne puisse pas contacter le client en raison d'un problème réseau. Une ressource ping peut détecter et atténuer ce problème. La configuration de cette ressource est facultative.
   </para>
   <procedure>
    <step>
     <para>
      Définissez la ressource ping :
     </para>
<screen><prompt>crm(live)configure# </prompt>primitive ganesha-ping ocf:pacemaker:ping \
        params name=ping dampen=3s multiplier=100 host_list="<replaceable>CLIENT1</replaceable> <replaceable>CLIENT2</replaceable>" \
        op monitor interval=60 timeout=60 \
        op start interval=0 timeout=60 \
        op stop interval=0 timeout=60</screen>
     <para>
      <literal>host_list</literal> est une liste d'adresses IP séparées par des espaces. Une ressource ping sera régulièrement envoyée aux adresses IP pour vérifier les pannes du réseau. Si un client doit toujours avoir accès au serveur NFS, ajoutez-le à la liste <literal>host_list</literal>.
     </para>
    </step>
    <step>
     <para>
      Créez un clone :
     </para>
<screen><prompt>crm(live)configure# </prompt>clone ganesha-ping-clone ganesha-ping \
        meta interleave=true</screen>
    </step>
    <step>
     <para>
      La commande suivante crée une contrainte pour le service NFS Ganesha. Elle impose au service de se déplacer vers un autre noeud lorsque <literal>host_list</literal> est inaccessible.
     </para>
<screen><prompt>crm(live)configure# </prompt>location nfs-ganesha-server-with-ganesha-ping
        nfs-ganesha-clone \
        rule -inf: not_defined ping or ping lte 0</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ganesha-ha-deepsea">
   <title>NFS Ganesha HA et DeepSea</title>
   <para>
    DeepSea ne prend pas en charge la configuration NFS Ganesha HA. Pour empêcher l'échec de DeepSea après la configuration de NFS Ganesha HA, excluez le démarrage et l'arrêt du service NFS Ganesha à partir de la phase 4 de DeepSea :
   </para>
   <procedure>
    <step>
     <para>
      Copiez <filename>/srv/salt/ceph/ganesha/default.sls</filename> vers <filename>/srv/salt/ceph/ganesha/ha.sls</filename>.
     </para>
    </step>
    <step>
     <para>
      Supprimez l'entrée <literal>.service</literal> du fichier <filename>/srv/salt/ceph/ganesha/ha.sls</filename> afin qu'il se présente comme suit :
     </para>
<screen>include:
- .keyring
- .install
- .configure</screen>
    </step>
    <step>
     <para>
      Ajoutez la ligne suivante à <filename>/srv/pillar/ceph/stack/global.yml</filename> :
     </para>
<screen>ganesha_init: ha</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-info">
  <title>Pour plus d'informations</title>

  <para>
   Pour plus d'informations, reportez-vous au <xref linkend="cha-ceph-nfsganesha"/>.
  </para>
 </sect1>
</chapter>
