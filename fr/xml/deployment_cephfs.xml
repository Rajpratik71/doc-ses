<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_cephfs.xml" version="5.0" xml:id="cha-ceph-as-cephfs">

 <title>Installation de CephFS</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>modification</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>oui</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  Le système de fichiers de Ceph (CephFS) est un système de fichiers compatible POSIX qui utilise une grappe de stockage Ceph pour stocker ses données. CephFS utilise le même système de grappe que les périphériques de bloc Ceph, que le stockage des objets Ceph avec ses API S3 et Swift ou que les liaisons natives (<systemitem>librados</systemitem>).
 </para>
 <para>
  Pour utiliser CephFS, vous devez disposer d'une grappe de stockage Ceph en cours d'exécution et d'au moins un serveur de métadonnées Ceph (<emphasis>Ceph Metadata Server</emphasis>) en cours d'exécution.
 </para>
 <sect1 xml:id="ceph-cephfs-limitations">
  <title>Scénarios CephFS pris en charge et conseils</title>

  <para>
   Avec SUSE Enterprise Storage, SUSE introduit la prise en charge officielle de nombreux scénarios dans lesquels le composant évolutif et distribué CephFS est utilisé. Cette rubrique décrit les limites fixes et fournit des conseils pour les cas d'utilisation suggérés.
  </para>

  <para>
   Un déploiement de CephFS pris en charge doit répondre aux conditions requises suivantes :
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Un minimum d'un serveur de métadonnées (MDS). SUSE recommande de déployer plusieurs noeuds avec le rôle MDS. Un seul sera « actif » et les autres seront « passifs ». N'oubliez pas de mentionner tous les noeuds MDS dans la commande <command>mount</command> lors du montage de CephFS à partir d'un client.
    </para>
   </listitem>
   <listitem>
    <para>
     Les instantanés CephFS sont désactivés (par défaut) et non pris en charge dans cette version.
    </para>
   </listitem>
   <listitem>
    <para>
     Les clients sont basés sur SUSE Linux Enterprise Server 12 SP2 ou SP3, utilisant le pilote du module de kernel <literal>cephfs</literal>. Le module FUSE n'est pas pris en charge.
    </para>
   </listitem>
   <listitem>
    <para>
     Les quotas CephFS ne sont pas pris en charge dans SUSE Enterprise Storage, car la prise en charge des quotas est implémentée dans le client FUSE uniquement.
    </para>
   </listitem>
   <listitem>
    <para>
     CephFS prend en charge les modifications de disposition de fichier comme expliqué sur le site <link xlink:href="http://docs.ceph.com/docs/jewel/cephfs/file-layouts/"/>. Toutefois, alors que le système de fichiers est monté par n'importe quel client, de nouvelles réserves de données ne peuvent pas être ajoutées à un système de fichiers CephFS existant (<literal>ceph mds add_data_pool</literal>). Elles peuvent être ajoutées uniquement lorsque le système de fichiers est démonté.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="ceph-cephfs-mds">
  <title>Serveur de métadonnées Ceph</title>

  <para>
   Le serveur de métadonnées (MDS) Ceph stocke des métadonnées pour CephFS. Les périphériques de bloc Ceph et le stockage des objets Ceph <emphasis>n'utilisent pas</emphasis> le MDS. Les MDS permettent aux utilisateurs du système de fichiers POSIX d'exécuter des commandes de base, telles que <command>ls</command> ou <command>find</command>, sans imposer une charge importante sur la grappe de stockage Ceph.
  </para>

  <sect2 xml:id="ceph-cephfs-mdf-add">
   <title>Ajout d'un serveur de métadonnées</title>
   <para>
    Vous pouvez déployer le MDS pendant le processus de déploiement de la grappe initial comme décrit à la <xref linkend="ceph-install-stack"/>, ou l'ajouter à une grappe déjà déployée comme décrit dans le manuel <xref linkend="salt-adding-nodes"/>.
   </para>
   <para>
    Après avoir déployé votre MDS, autorisez le service <literal>Ceph OSD/MDS</literal> dans les paramètres de pare-feu du serveur sur lequel le MDS est déployé : démarrez <literal>yast</literal>, accédez à <menuchoice> <guimenu>Security and Users</guimenu> <guimenu>Firewall</guimenu> <guimenu>Allowed Services</guimenu> </menuchoice> (Sécurité et utilisateurs &gt; Pare-feu &gt; Services autorisés), puis dans le menu déroulant <guimenu>Service to Allow</guimenu> (Service à autoriser), sélectionnez <guimenu>Ceph OSD/MDS</guimenu>. Si le trafic complet n'est pas autorisé sur le noeud Ceph MDS, le montage d'un système de fichiers échoue, même si d'autres opérations peuvent fonctionner correctement.
   </para>
  </sect2>

  <sect2 xml:id="ceph-cephfs-mds-config">
   <title>Configuration d'un serveur de métadonnées</title>
   <para>
    Vous pouvez affiner le comportement du MDS en insérant les options appropriées dans le fichier de configuration <filename>ceph.conf</filename>.
   </para>
   <variablelist>
    <title>Taille du cache du MDS</title>
    <varlistentry>
     <term><option>mds cache memory limit</option>
     </term>
     <listitem>
      <para>
       Limite de mémoire logique (en octets) que le MDS applique à son cache. Les administrateurs doivent l'utiliser à la place de l'ancien paramètre <option>mds cache size</option>. La valeur par défaut est de 1 Go.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>mds cache reservation</option>
     </term>
     <listitem>
      <para>
       Réservation du cache (mémoire ou inodes) que le cache du MDS gère. Lorsque le MDS commence à atteindre sa réservation, il rappelle l'état du client jusqu'à ce que sa taille de cache diminue pour restaurer la réservation. La valeur par défaut est 0,05.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Pour une liste détaillée des options de configuration liées au MDS, reportez-vous à la section <link xlink:href="http://docs.ceph.com/docs/master/cephfs/mds-config-ref/"/>.
   </para>
   <para>
    Pour une liste détaillée des options de configuration journaler du MDS, reportez-vous à la section <link xlink:href="http://docs.ceph.com/docs/master/cephfs/journaler/"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs">
  <title>CephFS</title>

  <para>
   Lorsque vous disposez d'une grappe de stockage Ceph saine avec au moins un serveur de métadonnées Ceph, vous pouvez créer et monter le système de fichiers Ceph. Assurez-vous que votre client dispose d'une connectivité réseau et d'un porte-clés d'authentification approprié.
  </para>

  <sect2 xml:id="ceph-cephfs-cephfs-create">
   <title>Création de CephFS</title>
   <para>
    CephFS nécessite au moins deux réserves RADOS : une pour les <emphasis>données</emphasis> et l'autre pour les <emphasis>métadonnées</emphasis>. Lorsque vous configurez ces réserves, vous pouvez envisager :
   </para>
   <itemizedlist>
    <listitem>
     <para>
      L'utilisation d'un niveau de réplication supérieur pour la réserve de métadonnées, car toute perte de données dans cette réserve peut rendre l'ensemble du système de fichiers inaccessible.
     </para>
    </listitem>
    <listitem>
     <para>
      L'utilisation d'un stockage à faible latence, tel que des disques SSD pour la réserve de métadonnées, car cela améliore la latence observée des opérations du système de fichiers sur les clients.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Lorsque vous assignez un <literal>role-mds</literal> dans le fichier <filename>policy.cfg</filename>, les réserves requises sont automatiquement créées. Vous pouvez créer manuellement les réserves <literal>cephfs_data</literal> et <literal>cephfs_metadata</literal> pour l'optimisation manuelle des performances avant la configuration du serveur de métadonnées. DeepSea ne crée pas ces réserves si elles existent déjà.
   </para>
   <para>
    Pour plus d'informations sur la gestion des réserves, reportez-vous au <xref linkend="ceph-pools"/>.
   </para>
   <para>
    Pour créer les deux réserves requises (par exemple, cephfs_data et cephfs_metadata) avec les paramètres par défaut à utiliser avec CephFS, exécutez les commandes suivantes :
   </para>
<screen><prompt>root # </prompt>ceph osd pool create cephfs_data <replaceable>pg_num</replaceable>
<prompt>root # </prompt>ceph osd pool create cephfs_metadata <replaceable>pg_num</replaceable></screen>
   <para>
    Il est possible d'utiliser des réserves EC au lieu des réserves répliquées. Il est recommandé de n'utiliser les réserves EC que pour les exigences de performances faibles et les accès aléatoires peu fréquents, tels que le stockage à froid, les sauvegardes et l'archivage. CephFS sur les réserves EC nécessite l'activation de BlueStore et l'option <literal>allow_ec_overwrite</literal> doit être définie pour la réserve. Cette option peut être définie en exécutant <command>ceph osd pool set ec_pool allow_ec_overwrites true</command>.
   </para>
   <para>
    Le codage à effacement ajoute un overhead considérable aux opérations du système de fichiers, notamment aux petites mises à jour. Cet overhead est inhérent à l'utilisation du codage à effacement en tant que mécanisme de tolérance aux pannes. Cette restriction est le compromis permettant de réduire considérablement l'overhead de l'espace de stockage.
   </para>
   <para>
    Lorsque les réserves sont créées, vous pouvez activer le système de fichiers avec la commande <command>ceph fs new</command> :
   </para>
<screen><prompt>root # </prompt>ceph fs new <replaceable>fs_name</replaceable> <replaceable>metadata_pool_name</replaceable> <replaceable>data_pool_name</replaceable></screen>
   <para>
    Par exemple :
   </para>
<screen><prompt>root # </prompt>ceph fs new cephfs cephfs_metadata cephfs_data</screen>
   <para>
    Vous pouvez vérifier que le système de fichiers a été créé en répertoriant tous les CephFS disponibles :
   </para>
<screen><prompt>root # </prompt><command>ceph</command> <option>fs ls</option>
 name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]</screen>
   <para>
    Lorsque le système de fichiers est créé, votre MDS peut basculer à l'état <emphasis>actif</emphasis>. Par exemple, dans un système MDS unique :
   </para>
<screen><prompt>root # </prompt><command>ceph</command> <option>mds stat</option>
e5: 1/1/1 up</screen>
   <tip>
    <title>plus de rubriques</title>
    <para>
     Vous pouvez trouver plus d'informations sur des tâches spécifiques, par exemple le montage, le démontage et la configuration avancée de CephFS dans le <xref linkend="cha-ceph-cephfs"/>.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-cephfs-multimds">
   <title>Taille de la grappe du MDS</title>
   <para>
    Une instance CephFS peut être desservie par plusieurs daemons MDS actifs. Tous les daemons MDS actifs assignés à une instance CephFS se partagent l'arborescence de répertoires du système de fichiers entre eux et répartissent ainsi la charge des clients simultanés. Pour pouvoir ajouter un daemon MDS actif à une instance CephFS, une mise en veille est nécessaire. Lancez un daemon supplémentaire ou utilisez une instance existante de mise en veille.
   </para>
   <para>
    La commande suivante affiche le nombre actuel de daemons MDS actifs et passifs.
   </para>
<screen><prompt>root # </prompt>ceph mds stat</screen>
   <para>
    La commande suivante définit le nombre de MDS actifs sur deux dans une instance de système de fichiers.
   </para>
<screen><prompt>root # </prompt>ceph fs set <replaceable>fs_name</replaceable> max_mds 2</screen>
   <para>
    Afin de réduire la grappe MDS avant une mise à jour, deux étapes sont nécessaires. Tout d'abord, définissez <option>max_mds</option> pour qu'il ne reste qu'une seule instance :
   </para>
<screen><prompt>root # </prompt>ceph fs set <replaceable>fs_name</replaceable> max_mds 1</screen>
   <para>
    ensuite, désactivez explicitement les autres daemons MDS actifs :
   </para>
<screen><prompt>root # </prompt>ceph mds deactivate <replaceable>fs_name</replaceable>:<replaceable>rank</replaceable></screen>
   <para>
    où <replaceable>rank</replaceable> est le numéro d'un daemon MDS actif d'une instance de système de fichiers, allant de 0 à <option>max_mds</option>-1. Pour plus d'informations, reportez-vous à la section <link xlink:href="http://docs.ceph.com/docs/luminous/cephfs/multimds/"/>.
   </para>
  </sect2>

  <sect2 xml:id="ceph-cephfs-multimds-updates">
   <title>Grappe du MDS et mises à jour</title>
   <para>
    Au cours des mises à jour de Ceph, les drapeaux de fonction sur une instance de système de fichiers peuvent changer (généralement en ajoutant de nouvelles fonctions). Les daemons incompatibles (par exemple, les versions antérieures) ne sont pas en mesure de fonctionner avec un jeu de fonctions incompatibles et refusent de démarrer. Cela signifie que la mise à jour et le redémarrage d'un daemon peuvent entraîner l'arrêt de tous les autres daemons non encore mis à jour et leur refus de démarrer. Pour cette raison, il est recommandé de réduire la grappe du MDS actif à une seule instance et d'arrêter tous les daemons en veille avant de mettre à jour Ceph. Les étapes manuelles de cette procédure de mise à jour sont les suivantes :
   </para>
   <procedure>
    <step>
     <para>
      Mettez à jour les paquetages liés à Ceph à l'aide de la commande <command>zypper</command>.
     </para>
    </step>
    <step>
     <para>
      Réduisez la grappe du MDS actif comme décrit ci-dessus à 1 instance et arrêtez tous les daemons MDS en veille en utilisant leurs unités <systemitem class="daemon">systemd</systemitem> sur tous les autres noeuds :
     </para>
<screen><prompt>root # </prompt>systemctl stop ceph-mds\*.service ceph-mds.target</screen>
    </step>
    <step>
     <para>
      Ensuite, redémarrez le seul daemon MDS restant, en le faisant redémarrer à l'aide du fichier binaire mis à jour.
     </para>
<screen><prompt>root # </prompt>systemctl restart ceph-mds\*.service ceph-mds.target</screen>
    </step>
    <step>
     <para>
      Redémarrez tous les autres daemons MDS et redéfinissez le paramètre <option>max_mds</option> souhaité.
     </para>
<screen><prompt>root # </prompt>systemctl start ceph-mds.target</screen>
    </step>
   </procedure>
   <para>
    Si vous utilisez DeepSea, ce dernier suit cette procédure dans les cas où le paquetage
    <package>ceph</package> a été mis à jour au cours des phases 0 et 4. Il est possible d'effectuer cette procédure alors que l'instance CephFS est montée sur des clients et que les E/S sont en cours d'exécution. Notez toutefois qu'il y aura une pause d'E/S très brève pendant le redémarrage du MDS actif. La récupération des clients s'effectue automatiquement.
   </para>
   <para>
    Il est recommandé de réduire la charge d'E/S autant que possible avant de mettre à jour une grappe du MDS. Une grappe du MDS inactive sera soumise à cette procédure de mise à jour plus rapidement. À l'inverse, sur une grappe fortement chargée avec plusieurs daemons MDS, il est essentiel de réduire la charge à l'avance pour éviter qu'un seul daemon MDS soit submergé par les E/S en cours.
   </para>
  </sect2>
 </sect1>
</chapter>
