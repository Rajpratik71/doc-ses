<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_hwrecommend.xml" version="5.0" xml:id="storage-bp-hwreq">
 <title>Requisiti hardware e raccomandazioni</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:translation>sì</dm:translation>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  I requisiti hardware di Ceph dipendono strettamente dal carico di lavoro degli IO. Tenere presente i seguenti requisiti hardware e raccomandazioni come base di partenza per una pianificazione dettagliata.
 </para>
 <para>
  In generale, le raccomandazioni date sono riferite a un processo. Se sullo stesso computer sono ubicati più processi, occorre sommare i requisiti di CPU, RAM, disco e rete.
 </para>
 <sect1 xml:id="deployment-osd-recommendation">
  <title>Nodi storage oggetto</title>

  <sect2 xml:id="sysreq-osd">
   <title>Requisiti minimi</title>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Sono richiesti almeno 4 nodi OSD, con 8 dischi OSD ciascuno.
     </para>
    </listitem>
    <listitem>
     <para>
      Per gli OSD che <emphasis>non</emphasis> utilizzano BlueStore, è richiesto almeno 1 GB di RAM per terabyte di capacità OSD nominale per ogni nodo di storage OSD. Consigliati 1,5 GB di RAM per terabyte di capacità OSD nominale. Durante il ripristino, 2 GB di RAM per terabyte di capacità OSD nominale possono risultare ottimali.
     </para>
     <para>
      Per gli OSD che <emphasis>utilizzano</emphasis> BlueStore, calcolare prima la dimensione della RAM consigliata per gli OSD che non utilizzano BlueStore, quindi calcolare 2 GB oltre la dimensione della cache BlueStore di RAM consigliati per ogni processo OSD e scegliere il valore maggiore di RAM dei due risultati. Tenere presente che la cache BlueStore di default è 1 GB per unità HDD e 3 GB per unità SSD di default. Riepilogando, prendere il maggiore tra:
     </para>
<screen>[1GB * OSD count * OSD size]</screen>
     <para>
      oppure
     </para>
<screen>[(2 + BS cache) * OSD count]</screen>
    </listitem>
    <listitem>
     <para>
      sono richiesti 1,5 GHz di un core CPU logica per OSD almeno per ogni processo del daemon OSD. Consigliati 2 GHz per processo del daemon OSD. Tenere presente che Ceph esegue un processo del daemon OSD per disco di storage; non contare i dischi riservati esclusivamente per l'uso come giornali di registrazione OSD, giornali di registrazione WAL, metadati omap o qualsiasi combinazione di questi tre casi.
     </para>
    </listitem>
    <listitem>
     <para>
      Ethernet 10 Gb (due interfacce di rete vincolate a più switch).
     </para>
    </listitem>
    <listitem>
     <para>
      Dischi OSD in configurazioni JBOD.
     </para>
    </listitem>
    <listitem>
     <para>
      I dischi OSD devono essere utilizzati esclusivamente da SUSE Enterprise Storage.
     </para>
    </listitem>
    <listitem>
     <para>
      SSD/disco dedicato per il sistema operativo, preferibilmente in una configurazione RAID 1.
     </para>
    </listitem>
    <listitem>
     <para>
      Se questo host OSD conterrà parte di un pool di cache utilizzato per tiering della cache, allocare almeno ulteriori 4 GB di RAM.
     </para>
    </listitem>
    <listitem>
     <para>
      I nodi OSD devono essere concreti, non virtualizzati, per motivi di prestazioni dei dischi.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="ses-bp-mindisk">
   <title>Dimensioni minime del disco</title>
   <para>
    Vi sono due tipi di spazi su disco necessari da eseguire su OSD: lo spazio per il giornale di registrazione del disco (per FileStore) o dispositivo WAL/DB (per BlueStore) e lo spazio primario per i dati memorizzati. Il valore minimo (e di default) per il giornale di registrazione/WAL/DB è 6 GB. Lo spazio minimo per i dati è pari a 5 GB, in quanto a partizioni più piccole di 5 GB viene assegnato automaticamente il peso di 0.
   </para>
   <para>
    Perciò, sebbene lo spazio minimo su disco per un OSD sia 11 GB, si sconsigliano dischi inferiori a 20 GB, anche per scopi di test.
   </para>
  </sect2>

  <sect2 xml:id="rec-waldb-size">
   <title>Dimensioni consigliate per dispositivo DB e WAL di BlueStore</title>
   <tip>
    <title>ulteriori informazioni</title>
    <para>
     Per ulteriori informazioni su BlueStore, consultare <xref linkend="about-bluestore"/>.
    </para>
   </tip>
   <para>
    Di seguito sono indicate diverse regole per il dimensionamento del dispositivo WAL/DB. Quando si utilizza DeepSea per distribuire gli OSD con BlueStore, si applicano le regole raccomandate automaticamente con notifica pertinente all'amministratore.
   </para>
   <itemizedlist>
    <listitem>
     <para>
      10 GB del dispositivo DB per ogni Terabyte della capacità OSD (1/100° dell'OSD).
     </para>
    </listitem>
    <listitem>
     <para>
      Tra 500 MB e 2 GB per il dispositivo WAL. La dimensione di WAL dipende dal traffico dati e dal carico di lavoro, non dalla dimensione dell'OSD. Se si sa che un OSD è fisicamente in grado di gestire ridotte scritture e sovrascritture con un throughput molto elevato, sono preferibili più WAL che meno WAL. 1 GB per dispositivo WAL è un buon compromesso che soddisfa molte distribuzioni.
     </para>
    </listitem>
    <listitem>
     <para>
      Se si intende mettere il dispositivo WAL e DB sullo stesso disco, si consiglia di utilizzare una singola partizione per entrambi i dispositivi, invece di avere una partizione separata per ciascuno. Ciò consente a Ceph di utilizzare il dispositivo DB anche per il funzionamento di WAL. La gestione dello spazio del disco è quindi più efficace in quanto Ceph utilizza la partizione DB per WAL solo se serve. Un altro vantaggio è che la probabilità di riempimento della partizione WAL è molto bassa e quando non è completamente utilizzata lo spazio non viene sprecato ma utilizzato per il funzionamento del DB.
     </para>
     <para>
      Per condividere il dispositivo DB con WAL, <emphasis>non</emphasis> specificare il dispositivo WAL e specificare solo il dispositivo DB:
     </para>
<screen>
bluestore_block_db_path = "/path/to/db/device"
bluestore_block_db_size = 10737418240
bluestore_block_wal_path = ""
bluestore_block_wal_size = 0
</screen>
    </listitem>
    <listitem>
     <para>
      In alternativa, è possibile mettere il WAL sul relativo dispositivo separato. In tale caso, per il funzionamento di WAL si consiglia il dispositivo più veloce.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="ses-bp-share-ssd-journal">
   <title>Utilizzo di SSD per giornali di registrazione OSD</title>
   <para>
    Le unità a stato solido (SSD) non hanno parti in movimento. Ciò riduce il tempo di accesso casuale e la latenza di lettura accelerando il throughput dei dati. Poiché il loro prezzo per 1MB è sensibilmente maggiore rispetto al prezzo dei dischi rigidi classici, le SSD sono adatte solo per storage di piccole dimensioni.
   </para>
   <para>
    Gli OSD possono avere un significativo miglioramento delle prestazioni memorizzando il loro giornale su una SSD e i dati oggetto su un disco rigido separato.
   </para>
   <tip>
    <title>condivisione di una SSD per più giornali di registrazione</title>
    <para>
     Poiché i dati del giornale di registrazione occupano relativamente poco spazio, è possibile montare diverse directory di giornale su un singolo disco SSD. Ricordare che con ogni giornale di registrazione condiviso, le prestazioni del disco SSD degradano. Si consiglia di non condividere più di sei giornali di registrazione sullo stesso disco SSD e 12 sui dischi NVMe.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="maximum-count-of-disks-osd">
   <title>Numero massimo di dischi consigliato</title>
   <para>
    Il server può contenere tutti i dischi consentiti. Quando si pianifica il numero di dischi per server, vi sono alcuni elementi da tenere presente:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <emphasis>Ampiezza della banda di rete.</emphasis> Più dischi sono presenti in un server, più dati occorre trasferire tramite scheda di rete per le operazioni di scrittura sul disco.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Memoria.</emphasis> Per le prestazioni ottimali, riservare almeno 2 GB di RAM per terabyte di spazio su disco installato.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Tolleranza di errore.</emphasis> In caso di guasto del server completo, più dischi sono presenti, più OSD vengono persi temporaneamente dal cluster. Inoltre, per mantenere in esecuzione le regole di replicazione, occorre copiare tutti i dati dal server guasto negli altri nodi nel cluster.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sysreq-mon">
  <title>Nodi monitor</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Sono richiesti almeno tre nodi Monitor Ceph. Il numero di monitor deve sempre essere dispari (1+2n).
    </para>
   </listitem>
   <listitem>
    <para>
     4 GB di RAM.
    </para>
   </listitem>
   <listitem>
    <para>
     Processore con quattro core logici.
    </para>
   </listitem>
   <listitem>
    <para>
     Si consiglia un'unità SSD o altro tipo di storage sufficientemente veloce per i monitor, in particolare per il percorso <filename>/var/lib/ceph</filename> su ogni nodo monitor, in quanto il quorum potrebbe essere instabile con alte latenze del disco. Si consigliano due dischi in configurazione RAID 1 per ridondanza. Si consiglia di utilizzare dischi separati o almeno partizioni del disco separate per i processi monitor per proteggere lo spazio su disco disponibile del monitor da elementi come file di registro che diventano troppo grandi.
    </para>
   </listitem>
   <listitem>
    <para>
     Deve essere presente solo un processo monitor per nodo.
    </para>
   </listitem>
   <listitem>
    <para>
     Mischiare nodi OSD, monitor o Object Gateway è possibile solo se sono disponibili sufficienti risorse hardware. Ciò significa che si devono sommare i requisiti per tutti i servizi.
    </para>
   </listitem>
   <listitem>
    <para>
     Due interfacce di rete associate a più switch.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sysreq-rgw">
  <title>Nodi Object Gateway</title>

  <para>
   I nodi Object Gateway devono avere CPU con sei - otto core e 32 GB di RAM (consigliati 64 GB). Se nello stesso computer sono co-ubicati altri processi, occorre sommare i loro requisiti.
  </para>
 </sect1>
 <sect1 xml:id="sysreq-mds">
  <title>Nodi Metadata Server</title>

  <para>
   Il corretto dimensionamento dei nodi Metadata Server dipende dal caso d'uso specifico. In genere, il numero di file aperti che deve gestire il Metadata Server è proporzionale alla quantità di CPU e RAM richiesta. Di seguito sono indicati i requisiti minimi:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     3 G di RAM per un daemon del Metadata Server.
    </para>
   </listitem>
   <listitem>
    <para>
     Interfaccia di rete vincolata.
    </para>
   </listitem>
   <listitem>
    <para>
     CPU da 2,5 GHz con almeno 2 core.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sysreq-smaster">
  <title>Salt Master</title>

  <para>
   Richiesti almeno 4 GB di RAM e una CPU quad-core. Ciò comprende l'esecuzione di openATTIC sul Salt master. Per i grandi cluster con centinaia di nodi, consigliati 6 GB di RAM.
  </para>
 </sect1>
 <sect1 xml:id="sysreq-iscsi">
  <title>Nodi iSCSI</title>

  <para>
   I nodi iSCSI devono avere CPU con sei/otto core e 16 GB di RAM.
  </para>
 </sect1>
 <sect1 xml:id="ceph-install-ceph-deploy-network">
  <title>Raccomandazioni di rete</title>

  <para>
   L'ambiente di rete in cui si intende eseguire Ceph deve essere idealmente un insieme vincolato di almeno due interfacce di rete suddiviso logicamente in una parte pubblica e una parte interna affidabile che utilizza VLAN. Si consiglia la modalità di vincolo 802.3ad se possibile per fornire una resilienza e un'ampiezza di banda massima.
  </para>

  <para>
   La VLAN pubblica fornisce il servizio ai clienti, mentre la parte interna fornisce la comunicazione di rete Ceph autenticata. Il motivo principale è che sebbene Ceph fornisca automazione e protezione dagli attacchi dopo aver istituito le chiavi segrete, i messaggi utilizzati per configurare tali chiavi possono essere trasferiti solo in modo aperto e vulnerabile.
  </para>

  <tip>
   <title>nodi configurati tramite DHCP</title>
   <para>
    Se i nodi di storage sono configurati tramite DHCP, i timeout default possono non essere sufficienti per configurare correttamente la rete prima dell'avvio dei vari daemon Ceph. In questo caso, gli OSD e MON Ceph non si avviano correttamente (l'esecuzione di <command>systemctl status ceph\*</command> determina errori del tipo "unable to bind"). Per evitare questo problema, si consiglia di aumentare il timeout del client DHCP ad almeno 30 secondi su ogni nodo nel cluster di storage. È possibile fare questo modificando le impostazioni seguenti su ogni nodo:
   </para>
   <para>
    In <filename>/etc/sysconfig/network/dhcp</filename>, impostare
   </para>
<screen>DHCLIENT_WAIT_AT_BOOT="30"</screen>
   <para>
    In <filename>/etc/sysconfig/network/config</filename>, impostare
   </para>
<screen>WAIT_FOR_INTERFACES="60"</screen>
  </tip>

  <sect2 xml:id="storage-bp-net-private">
   <title>Aggiunta di una rete privata a un cluster in esecuzione</title>
   <para>
    Se non si specifica una rete di cluster durante la distribuzione, Ceph presume un ambiente di rete pubblica singola. Mentre Ceph funziona bene con una rete pubblica, le sue prestazioni e sicurezza aumentano quando si imposta una seconda rete di cluster privata. Per supportare due reti, ogni nodo Ceph deve avere almeno due schede di rete.
   </para>
   <para>
    Occorre applicare le seguenti modifiche a ogni nodo Ceph. È relativamente rapido farlo con un piccolo cluster, ma può essere necessario molto tempo in caso di cluster contenente centinaia o migliaia di nodi.
   </para>
   <procedure>
    <step>
     <para>
      Arrestare i servizi correlati a Ceph su ogni nodo del cluster.
     </para>
     <para>
      Aggiungere una riga a <filename>/etc/ceph/ceph.conf</filename> per definire la rete di cluster, ad esempio:
     </para>
<screen>cluster network = 10.0.0.0/24</screen>
     <para>
      Se occorre specificamente assegnare indirizzi IP statici o ignorare le impostazioni della <option>rete cluster</option>, è possibile farlo con <option>cluster addr</option> opzionale.
     </para>
    </step>
    <step>
     <para>
      Verificare che la rete di cluster privata funzioni come previsto al livello del SO.
     </para>
    </step>
    <step>
     <para>
      Avviare i servizi correlati a Ceph su ogni nodo del cluster.
     </para>
<screen>sudo systemctl start ceph.target</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="storage-bp-net-subnets">
   <title>Nodi monitor su diverse sottoreti</title>
   <para>
    Se i nodi monitor sono su diverse sottoreti, ad esempio si trovano in stanze diverse e serviti da switch differenti, occorre regolare di conseguenza il file <filename>ceph.conf</filename>. Ad esempio, se i nodi hanno gli indirizzi IP 192.168.123.12, 1.2.3.4 242.12.33.12, aggiungere le righe seguenti alla sezione globale:
   </para>
<screen>[global]
[...]
mon host = 192.168.123.12, 1.2.3.4, 242.12.33.12
mon initial members = MON1, MON2, MON3
[...]</screen>
   <para>
    Inoltre, se occorre specificare una rete o un indirizzo pubblico per monitor, occorre aggiungere una sezione <literal>[mon.<replaceable>X</replaceable>]</literal> per ogni monitor:
   </para>
<screen>[mon.MON1]
public network = 192.168.123.0/24

[mon.MON2]
public network = 1.2.3.0/24

[mon.MON3]
public network = 242.12.33.12/0</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sysreq-naming">
  <title>Limitazioni dei nomi</title>

  <para>
   Ceph non supporta in genere i caratteri non ASCII nei file di configurazione, nei nomi di pool, nomi utente e così via. Quando si configura un cluster Ceph, si consiglia di utilizzare solo caratteri alfanumerici semplici (A-Z, a-z, 0-9) e punteggiatura minima (".", "", "_") in tutti i nomi di configurazione/oggetto Ceph.
  </para>
 </sect1>
 <sect1 xml:id="ses-bp-diskshare">
  <title>Condivisione di un server da parte di OSD e Monitor</title>

  <para>
   Sebbene sia tecnicamente possibile eseguire Monitor e OSD Ceph sullo stesso server in ambienti di test, si consiglia la presenza di un server separato per ogni nodo monitor in produzione. Il motivo principale sono le prestazioni: più OSD sono nel cluster, più operazioni di I/O devono essere eseguite dai nodi monitor. Quando un server è condiviso tra un nodo monitor e OSD, le operazioni di I/O OSD sono un fattore limitante per il nodo monitor.
  </para>

  <para>
   Un'altra considerazione è relativa alla condivisione dei dischi tra un OSD, un nodo monitor e il sistema operativo sul server. La risposta è semplice: se possibile, dedicare un disco separato all'OSD e un server separato a un nodo monitor.
  </para>

  <para>
   Sebbene Ceph supporti OSD basati su directory, un OSD deve sempre avere un disco dedicato diverso da quello per il sistema operativo.
  </para>

  <tip>
   <para>
    Se è <emphasis>davvero</emphasis> necessario eseguire il nodo monitor e OSD sullo stesso server, eseguire il monitor su un disco separato montando il disco sulla directory <filename>/var/lib/ceph/mon</filename> per prestazioni leggermente migliori.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="ses-bp-minimum-cluster">
  <title>Configurazione minima del cluster</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Quattro nodi Storage oggetto
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Ethernet 10 Gb (due reti vincolate a più switch).
      </para>
     </listitem>
     <listitem>
      <para>
       32 OSD per cluster di storage
      </para>
     </listitem>
     <listitem>
      <para>
       Il giornale di registrazione OSD può risiedere sul disco OSD
      </para>
     </listitem>
     <listitem>
      <para>
       Disco del sistema operativo dedicato per ogni nodo storage oggetto
      </para>
     </listitem>
     <listitem>
      <para>
       1 GB di RAM per TB di capacità OSD nominale per ogni nodo storage oggetto
      </para>
     </listitem>
     <listitem>
      <para>
       1,5 GHz per OSD per ogni nodo storage oggetto
      </para>
     </listitem>
     <listitem>
      <para>
       Monitor Ceph, gateway e Metadata Server possono risiedere sui nodi storage oggetto
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Tre nodi Monitor Ceph (richiede SSD per unità SO dedicata)
        </para>
       </listitem>
       <listitem>
        <para>
         I nodi Monitor Ceph, Object Gateway e Metadata Server richiedono distribuzione ridondante
        </para>
       </listitem>
       <listitem>
        <para>
         iSCSI Gateway, Object Gateway e Metadata Server richiedono ulteriori 4 GB RAM e quattro core
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     Nodo di gestione separato con 4 GB RAM, quattro core, capacità 1 TB
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="ses-bp-production-cluster">
  <title>Configurazione cluster di produzione consigliata</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Sette nodi Storage oggetto
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Nessun singolo nodo eccede ~15% dello storage totale
      </para>
     </listitem>
     <listitem>
      <para>
       Ethernet 10 Gb (quattro reti fisiche vincolate a più switch).
      </para>
     </listitem>
     <listitem>
      <para>
       56+ OSD per cluster di storage
      </para>
     </listitem>
     <listitem>
      <para>
       Dischi SO RAID 1 per ogni nodo di storage OSD
      </para>
     </listitem>
     <listitem>
      <para>
       SSD per giornale di registrazione con rapporto 6:1 tra giornale SSD e OSD
      </para>
     </listitem>
     <listitem>
      <para>
       1,5 GB di RAM per TB di capacità OSD nominale per ogni nodo storage oggetto
      </para>
     </listitem>
     <listitem>
      <para>
       2 GHz per OSD per ogni nodo storage oggetto
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     Nodi infrastruttura fisica dedicata
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Tre nodi Ceph Monitor: 4 GB RAM, processore a 4 core, SSD RAID 1 per disco
      </para>
     </listitem>
     <listitem>
      <para>
       Un nodo gestione SES: 4 GB RAM, processore 4 core, SSD RAID 1 per disco
      </para>
     </listitem>
     <listitem>
      <para>
       Distribuzione fisica ridondante di nodi gateway o Metadata Server:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Nodi Object Gateway: 32 GB RAM, processore 8 core, SSD RAID 1 per disco
        </para>
       </listitem>
       <listitem>
        <para>
         Nodi iSCSI Gateway: 16 GB RAM, processore 4 core, SSD RAID 1 per disco
        </para>
       </listitem>
       <listitem>
        <para>
         Nodi Metadata Server (uno attivo/uno hot standby): 32 GB RAM, processore 8 core, SSD RAID 1 per disco
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </itemizedlist>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="req-ses-other">
  <title>SUSE Enterprise Storage e altri prodotti SUSE</title>

  <para>
   Questa sezione contiene informazioni importanti sull'integrazione di SUSE Enterprise Storage con altri prodotti SUSE.
  </para>

  <sect2 xml:id="req-ses-suma">
   <title>SUSE Manager</title>
   <para>
    SUSE Manager e SUSE Enterprise Storage non sono integrati, perciò SUSE Manager non è in grado attualmente di gestire un cluster SUSE Enterprise Storage.
   </para>
  </sect2>
 </sect1>
</chapter>
