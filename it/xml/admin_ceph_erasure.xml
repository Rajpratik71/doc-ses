<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_erasure.xml" version="5.0" xml:id="cha-ceph-erasure">
 <title>Pool con codice di cancellazione</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>modifica</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>sì</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  Ceph fornisce un'alternativa alla replica normale dei dati nei pool denominata pool <emphasis>di cancellazione</emphasis> o <emphasis>con codice di cancellazione</emphasis>. I pool di cancellazione non forniscono tutte le funzionalità dei pool <emphasis>replicati</emphasis>, ma richiedono uno spazio di memorizzazione dei dati non elaborati inferiore. Un pool di cancellazione di default in grado di memorizzare 1 TB di dati richiede uno spazio di memorizzazione di dati non elaborati pari a 1,5 TB. Si tratta di un confronto positivo rispetto al pool replicato che richiede 2 TB di spazio di memorizzazione effettivo per la stessa quantità di dati.
 </para>
 <para>
  Per ulteriori informazioni sul codice di cancellazione, vedere <link xlink:href="https://en.wikipedia.org/wiki/Erasure_code"/>.
 </para>
 <note>
  <para>
   Quando si utilizza FileStore, non è possibile accedere ai pool con codice di cancellazione tramite l'interfaccia RBD a meno che non sia stato configurato un livello di cache. Per ulteriori dettagli vedere <xref linkend="ceph-tier-erasure"/> o utilizzare BlueStore.
  </para>
 </note>
 <note>
  <para>
   Assicurarsi che le regole CRUSH per i <emphasis>pool di cancellazione</emphasis> utilizzino <literal>indep</literal> per <literal>step</literal>. Per informazioni, vedere <xref linkend="datamgm-rules-step-mode"/>.
  </para>
 </note>
 <sect1 xml:id="cha-ceph-erasure-default-profile">
  <title>Creazione di un pool con codice di cancellazione di esempio</title>

  <para>
   Il pool con codice di cancellazione più semplice equivale a RAID5 e richiede almeno tre host. In questa procedura è illustrato come creare un pool ai fini del test.
  </para>
  <procedure>
   <step>
    <para>
     Il comando <command>ceph osd pool create</command> viene utilizzato per creare un pool di tipo <emphasis>cancellazione</emphasis>. <literal>12</literal> sta per il numero di gruppi di posizionamento. Con i parametri di default, il pool è in grado di gestire gli errori di un OSD.
    </para>
<screen><prompt>root # </prompt>ceph osd pool create ecpool 12 12 erasure
pool 'ecpool' created</screen>
   </step>
   <step>
    <para>
     La stringa <literal>ABCDEFGHI</literal> viene scritta in un oggetto denominato <literal>NYAN</literal>.
    </para>
<screen><prompt>cephadm &gt; </prompt>echo ABCDEFGHI | rados --pool ecpool put NYAN -</screen>
   </step>
   <step>
    <para>
     Ai fini del test, adesso è possibile disabilitare gli OSD, ad esempio disconnettendoli dalla rete.
    </para>
   </step>
   <step>
    <para>
     Per verificare se il pool è in grado di gestire gli errori dei dispositivi, è possibile accedere al contenuto del file mediante il comando <command>rados</command>.
    </para>
<screen><prompt>root # </prompt>rados --pool ecpool get NYAN -
ABCDEFGHI</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="cha-ceph-erasure-erasure-profiles">
  <title>Profili dei codici di cancellazione</title>
  <para>Quando si richiama il comando <command>ceph osd pool create</command> per creare un <emphasis>pool di cancellazione</emphasis>, viene utilizzato il profilo di default a meno che non se ne specifichi un altro. I profili definiscono la ridondanza dei dati. A tal fine impostare due parametri, denominati arbitrariamente <literal>k</literal> ed <literal>m</literal>. k ed m definiscono il numero di <literal>porzioni</literal> in cui vengono suddivisi i dati e quante porzioni di codifica vengono create. Le porzioni ridondanti vengono quindi memorizzate in OSD diversi.
  </para>
  <para>
   Definizioni necessarie per i profili dei pool di cancellazione:
  </para>

  <variablelist>
   <varlistentry>
    <term>chunk</term>
    <listitem>
     <para>
      quando si richiama la funzione di codifica, vengono restituite porzioni della stessa dimensione: le porzioni di dati che è possibile concatenare per ricostruire l'oggetto originale e le porzioni di codifica che è possibile utilizzare per ricompilare una porzione persa.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>k</term>
    <listitem>
     <para>
      il numero di porzioni di dati, ovvero il numero di porzioni in cui è suddiviso l'oggetto originale. Ad esempio, se <literal>k = 2</literal> un oggetto da 10 KB verrà suddiviso in <literal>k</literal> oggetti da 5 KB ciascuno.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>m</term>
    <listitem>
     <para>
      il numero di porzioni di codifica, ovvero il numero di porzioni aggiuntive calcolato dalle funzioni di codifica. Esistono 2 porzioni di codifica, vale a dire che 2 OSD possono essere fuori senza perdere dati.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>crush-failure-domain</term>
    <listitem>
     <para>
      definisce a quali dispositivi vengono distribuite le porzioni. È necessario impostare come valore un tipo di compartimento. Per tutti i tipi di compartimenti, vedere <xref linkend="datamgm-buckets"/>. Se il dominio dell'errore è <literal>rack</literal>, le porzioni saranno memorizzate in rack diversi al fine di aumentare la resilienza in caso di errore dei rack.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   Con il profilo del codice di cancellazione utilizzato nella <xref linkend="cha-ceph-erasure-default-profile"/>, i dati del cluster non andranno persi in caso di errore di un singolo OSD. Pertanto, per memorizzare 1 TB di dati è necessario uno spazio di memorizzazione effettivo di altri 0,5 TB. Vale a dire che per 1 TB di dati è necessario uno spazio di memorizzazione effettivo di 1,5 TB. Ciò equivale a una normale configurazione RAID 5. Confronto: un pool replicato necessita di 2 TB di spazio di memorizzazione dei dati non elaborati per memorizzare 1 TB di dati.
  </para>
  <para>È possibile visualizzare le impostazioni del profilo di default con:
  </para>

<screen><prompt>root # </prompt>ceph osd erasure-code-profile get default
directory=.libs
k=2
m=1
plugin=jerasure
crush-failure-domain=host
technique=reed_sol_van</screen>

  <para>
   È importante scegliere il profilo giusto perché non è possibile modificarlo dopo la creazione del pool. È necessario creare un nuovo pool con un profilo diverso e spostare tutti gli oggetti del pool precedente in quello nuovo.
  </para>

  <para>
   I parametri più importanti del profilo sono <literal>k</literal>, <literal>m</literal> e <literal>crush-failure-domain</literal> in quanto definiscono l'overhead di memorizzazione e la durata dei dati. Ad esempio, se l'architettura desiderata deve sostenere la perdita di due rack con un overhead di memorizzazione del 66%, è possibile definire il profilo seguente:
  </para>

<screen><prompt>root # </prompt>ceph osd erasure-code-profile set <replaceable>myprofile</replaceable> \
   k=3 \
   m=2 \
   crush-failure-domain=rack</screen>

  <para>
   È possibile ripetere l'esempio della <xref linkend="cha-ceph-erasure-default-profile"/> con questo nuovo profilo:
  </para>

<screen><prompt>root # </prompt>ceph osd pool create ecpool 12 12 erasure <replaceable>myprofile</replaceable>
<prompt>cephadm &gt; </prompt>echo ABCDEFGHI | rados --pool ecpool put NYAN -
<prompt>root # </prompt>rados --pool ecpool get NYAN -
ABCDEFGHI</screen>

  <para>
   L'oggetto NYAN verrà diviso in tre (<literal>k=3</literal>) e verranno create due porzioni aggiuntive (<literal>m=2</literal>). Il valore di <literal>m</literal> definisce quanti OSD è possibile perdere simultaneamente senza perdere alcun dato. Con il comando <literal>crush-failure-domain=rack</literal> verrà creato un set di regole CRUSH che garantisce che le due porzioni non vengano memorizzate nello stesso rack.
  </para>

  <informalfigure>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ceph_erasure_obj.png" width="80%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ceph_erasure_obj.png" width="60%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </informalfigure>

  <para>
   Per ulteriori informazioni sui profili dei codici di cancellazione, vedere <link xlink:href="http://docs.ceph.com/docs/master/rados/operations/erasure-code-profile"/> (in lingua inglese).
  </para>
 </sect1>
 <sect1 xml:id="ceph-tier-erasure">
  <title>Pool con codice di cancellazione e suddivisione in livelli di cache</title>

  <para>
   I pool con codice di cancellazione richiedono più risorse rispetto ai pool replicati e presentano meno funzionalità, come la scrittura parziale. Per superare tali limiti, si consiglia di impostare un livello di cache prima del pool con codice di cancellazione.
  </para>

  <para>
   Ad esempio, se il pool <quote>hot-storage</quote> è costituito da uno spazio di memorizzazione veloce, è possibile velocizzare <quote>ecpool</quote> creato nella <xref linkend="cha-ceph-erasure-erasure-profiles"/> con:
  </para>

<screen><prompt>root # </prompt>ceph osd tier add ecpool hot-storage
<prompt>root # </prompt>ceph osd tier cache-mode hot-storage writeback
<prompt>root # </prompt>ceph osd tier set-overlay ecpool hot-storage</screen>

  <para>
   In tal modo il pool <quote>hot-storage</quote> verrà posizionato come livello di ecpool in modalità Write-back, in modo che per ogni scrittura e lettura in ecpool venga utilizzato effettivamente lo spazio di memorizzazione ad accesso frequente, a favore della flessibilità e della velocità.
  </para>

  <para>
   Quando si utilizza FileStore, non è possibile creare un'immagine RBD o un pool con codice di cancellazione perché sono richieste le scritture parziali. È tuttavia possibile creare un'immagine RBD in un pool con codice di cancellazione quando un livello di pool replicato è impostato come livello di cache:
  </para>

<screen><prompt>root # </prompt>rbd --pool ecpool create --size 10 myvolume</screen>

  <para>
   Per ulteriori informazioni sulla suddivisione in livelli di cache, vedere <xref linkend="cha-ceph-tiered"/>.
  </para>
 </sect1>
 <sect1 xml:id="ec-rbd">
  <title>Pool con codice di cancellazione con RADOS Block Device (dispositivo di blocco RADOS)</title>
  <para>
   Per contrassegnare un pool EC come pool RBD, applicare il rispettivo tag:
  </para>
<screen>
<prompt>root # </prompt>ceph osd pool application enable rbd <replaceable>ec_pool_name</replaceable>
</screen>
  <para>
  RBD può memorizzare <emphasis>data</emphasis> immagine nei pool EC. Tuttavia, l'intestazione dell'immagine e i metadati devono essere comunque memorizzati in un pool replicato. A tal fine, presupporre di disporre di un pool denominato "rbd":
  </para>
<screen>
<prompt>root # </prompt>rbd create rbd/<replaceable>image_name</replaceable> --size 1T --data-pool <replaceable>ec_pool_name</replaceable>
</screen>
  <para>
   È possibile utilizzare normalmente l'immagine come qualsiasi altra, con la differenza che tutti i dati saranno memorizzati nel pool <replaceable>ec_pool_name</replaceable> al posto del pool "rbd".
  </para>
 </sect1>
</chapter>
