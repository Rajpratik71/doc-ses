<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_ganesha.xml" version="5.0" xml:id="cha-as-ganesha">

 <title>Installazione di NFS Ganesha</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>modifica</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>sì</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  NFS Ganesha fornisce accesso NFS a Object Gateway o a CephFS. In SUSE Enterprise Storage 5, sono supportate le versioni NFS 3 e 4. NFS Ganesha viene eseguito nello spazio utente invece che nello spazio kernel e interagisce direttamente con l'Object Gateway o CephFS.
 </para>
 <sect1 xml:id="sec-as-ganesha-preparation">
  <title>Preparazione</title>

  <sect2 xml:id="sec-as-ganesha-preparation-general">
   <title>Informazioni generali</title>
   <para>
    Per installare correttamente NFS Ganesha, occorre aggiungere un <literal>role-ganesha</literal> a <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>. Per informazioni, vedere <xref linkend="policy-configuration"/>. NFS Ganesha richiede inoltre un <literal>role-rgw</literal> o un <literal>role-mds</literal> presente in <filename>policy.cfg</filename>.
   </para>
   <para>
    Sebbene sia possibile installare ed eseguire il server NFS Ganesha su un nodo Ceph già esistente, si consiglia di eseguirlo su un host dedicato con accesso al cluster Ceph. Gli host client non fanno in genere parte del cluster, ma devono avere accesso di rete al server NFS Ganesha.
   </para>
   <para>
    Per abilitare il server NFS Ganesha in qualsiasi punto dopo l'installazione iniziale, aggiungere <literal>role-ganesha</literal> a <filename>policy.cfg</filename> e ripetere almeno le fasi 2 e 4 di DeepSea. Per informazioni, vedere <xref linkend="ceph-install-stack"/>.
   </para>
   <para>
    NFS Ganesha è configurato tramite il file <filename>/etc/ganesha/ganesha.conf</filename> esistente sul nodo NFS Ganesha. Tuttavia, tale file viene sovrascritto ogni volta che si esegue la fase 4 di DeepSea. Perciò si consiglia di modificare il modello utilizzato da Salt, ossia il file <filename>/srv/salt/ceph/ganesha/files/ganesha.conf.j2</filename> sul Salt master. Per informazioni sul file di configurazione, vedere <xref linkend="ceph-nfsganesha-config"/>.
   </para>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-preparation-requirements">
   <title>Riepilogo dei requisiti</title>
   <para>
    Prima di poter eseguire le fasi 2 e 4 di DeepSea per installare NFS Ganesha, occorre soddisfare i seguenti requisiti:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Almeno un nodo deve essere assegnato a <literal>role-ganesha</literal>.
     </para>
    </listitem>
    <listitem>
     <para>
      È possibile definire solo un <literal>role-ganesha</literal> per minion.
     </para>
    </listitem>
    <listitem>
     <para>
      Per il funzionamento, NFS Ganesha richiede un Object Gateway o CephFS.
     </para>
    </listitem>
    <listitem>
     <para>
      Se NFS Ganesha deve utilizzare l'Object Gateway per interfacciarsi con il cluster, specificare <filename>/srv/pillar/ceph/rgw.sls</filename> sul Salt master.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-basic-example">
  <title>Installazione di esempio</title>

  <para>
   Questa procedura fornisce un'installazione di esempio che utilizza l'Object Gateway e CephFS File System Abstraction Layers (FSAL) di NFS Ganesha.
  </para>

  <procedure>
   <step>
    <para>
     Se non è già stato fatto, eseguire le fasi 0 e 1 di DeepSea prima di continuare con questa procedura.
    </para>
<screen><prompt>root # </prompt><command>salt-run</command> state.orch ceph.stage.0
<prompt>root # </prompt><command>salt-run</command> state.orch ceph.stage.1</screen>
   </step>
   <step>
    <para>
     Dopo aver eseguito la fase 1 di DeepSea, modificare <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> e aggiungere la riga
    </para>
<screen>role-ganesha/cluster/<replaceable>NODENAME</replaceable></screen>
    <para>
     Sostituire <replaceable>NODENAME</replaceable> con il nome di un nodo nel cluster.
    </para>
    <para>
     Accertare inoltre che siano assegnati un <literal>role-mds</literal> e un <literal>role-rgw</literal>.
    </para>
   </step>
   <step>
    <para>
     Creare il file <filename>/srv/pillar/ceph/rgw.sls</filename> e inserire il contenuto seguente:
    </para>
<screen>rgw_configurations:
  rgw:
    users:
      - { uid: "demo", name: "Demo", email: "demo@demo.nil" }
      - { uid: "demo1", name: "Demo1", email: "demo1@demo.nil" }</screen>
    <para>
     Questi utenti vengono creati in seguito come utenti di Object Gateway e vengono generate le chiavi API. Sul nodo Object Gateway, è possibile eseguire in seguito <command>radosgw-admin user list</command> per elencare tutti gli utenti creati e <command>radosgw-admin user info --uid=demo</command> per ottenere dettagli sui singoli utenti.
    </para>
    <para>
     DeepSea accerta che l'Object Gateway e NFS Ganesha ricevano entrambi le credenziali di tutti gli utenti elencati nella sezione <literal>rgw</literal> di <filename>rgw.sls</filename>.
    </para>
    <para>
     Il NFS esportato utilizza tali nomi utente sul primo livello del file system, in questo esempio vengono esportati i percorsi <filename>/demo</filename> e <filename>/demo1</filename>.
    </para>
   </step>
   <step>
    <para>
     Eseguire almeno le fasi 2 e 4 di DeepSea. Si consiglia l'esecuzione della fase 3 tra le altre due fasi.
    </para>
<screen><prompt>root # </prompt><command>salt-run</command> state.orch ceph.stage.2
<prompt>root # </prompt><command>salt-run</command> state.orch ceph.stage.3 # optional but recommended
<prompt>root # </prompt><command>salt-run</command> state.orch ceph.stage.4</screen>
   </step>
   <step>
    <para>
     Verificare che NFS Ganesha funzioni montando la condivisione NFS da un nodo client:
    </para>
<screen><prompt>root # </prompt><command>mount</command> -o sync -t nfs <replaceable>GANESHA_NODE</replaceable>:/ /mnt
<prompt>root # </prompt><command>ls</command> /mnt
cephfs  demo  demo1</screen>
    <para>
     <filename>/mnt</filename> deve contenere tutti i percorsi esportati. Le directory degli utenti di CephFS e Object Gateway devono essere esistenti. Per ogni bucket posseduto da un utente, viene esportato un percorso <filename>/mnt/<replaceable>USERNAME</replaceable>/<replaceable>BUCKETNAME</replaceable></filename>.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-ha-ap">
  <title>Configurazione attiva-passiva ad alta disponibilità</title>

  <para>
   Questa sezione fornisce un esempio di come impostare una configurazione attiva-passiva a due nodi dei server NFS Ganesha. La configurazione richiede SUSE Linux Enterprise High Availability Extension. I due nodi sono denominati <systemitem class="domainname">earth</systemitem> e <systemitem class="domainname">mars</systemitem>.
  </para>

  <para>
   Per informazioni su SUSE Linux Enterprise High Availability Extension, vedere <link xlink:href="https://www.suse.com/documentation/sle-ha-12/"/>.
  </para>

  <sect2 xml:id="sec-as-ganesha-ha-ap-basic">
   <title>Installazione di base</title>
   <para>
    In questa configurazione <systemitem class="domainname">earth</systemitem> ha l'indirizzo IP <systemitem class="ipaddress">192.168.1.1</systemitem> e <systemitem class="domainname">mars</systemitem> l'indirizzo <systemitem class="ipaddress">192.168.1.2</systemitem>.
   </para>
   <para>
    Inoltre, vengono utilizzati due indirizzi IP virtuali mobili che consentono ai client di connettersi al servizio indipendentemente dal nodo fisico sul quale è in esecuzione. <systemitem class="ipaddress">192.168.1.10</systemitem> è utilizzato per amministrazione del cluster con Hawk2 e <systemitem class="ipaddress">192.168.2.1</systemitem> esclusivamente per esportazioni NFS. Ciò semplifica la successiva applicazione delle limitazioni di sicurezza.
   </para>
   <para>
    La procedura seguente descrive l'installazione di esempio. Ulteriori informazioni sono disponibili all'indirizzo <link xlink:href="https://www.suse.com/documentation/sle-ha-12/install-quick/data/install-quick.html"/>.
   </para>
   <procedure xml:id="proc-as-ganesha-ha-ap">
    <step>
     <para>
      Preparare i nodi NFS Ganesha sul Salt master:
     </para>
     <substeps>
      <step>
       <para>
        Eseguire le fasi 0 e 1 di DeepSea sul Salt master.
       </para>
<screen>
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.0
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.1
</screen>
      </step>
      <step>
       <para>
        Assegnare ai nodi <systemitem class="domainname">earth</systemitem> e <systemitem class="domainname">mars</systemitem> il <literal>role-ganesha</literal> in <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>:
       </para>
<screen>role-ganesha/cluster/earth*.sls
role-ganesha/cluster/mars*.sls</screen>
      </step>
      <step>
       <para>
        Eseguire le fasi 3 e 4 di DeepSea sul Salt master.
       </para>
<screen><prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.3
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.4</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Registrare SUSE Linux Enterprise High Availability Extension su <systemitem class="domainname">earth</systemitem> e <systemitem class="domainname">mars</systemitem>.
     </para>
<screen>
<prompt>root # </prompt><command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable>
</screen>
    </step>
    <step>
     <para>
      Installare <package>ha-cluster-bootstrap</package> su entrambi i nodi:
     </para>
<screen><prompt>root # </prompt><command>zypper</command> in ha-cluster-bootstrap</screen>
    </step>
    <step>
     <substeps>
      <step>
       <para>
        Inizializzare il cluster su <systemitem class="domainname">earth</systemitem>:
       </para>
<screen><prompt>root@earth # </prompt><command>ha-cluster-init</command></screen>
      </step>
      <step>
       <para>
        Lasciare che <systemitem class="domainname">mars</systemitem> si unisca al cluster:
       </para>
<screen><prompt>root@mars # </prompt><command>ha-cluster-join</command> -c earth</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Verificare lo stato del cluster. Si dovrebbero vedere due nodi aggiunti al cluster:
     </para>
<screen><prompt>root@earth # </prompt><command>crm</command> status</screen>
    </step>
    <step>
     <para>
      Su entrambi i nodi, disabilitare l'avvio automatico del servizio NFS Ganesha durante il boot:
     </para>
<screen><prompt>root # </prompt><command>systemctl</command> disable nfs-ganesha</screen>
    </step>
    <step>
     <para>
      Avviare la shell <command>crm</command> su <systemitem class="domainname">earth</systemitem>:
     </para>
<screen><prompt>root@earth # </prompt><command>crm</command> configure</screen>
     <para>
      I comandi successivi vengono eseguiti nella shell crm.
     </para>
    </step>
    <step>
     <para>
      Su <systemitem class="domainname">earth</systemitem>, avviare la shell crm per eseguire i comandi indicati per configurare la risorsa per i daemon NFS Ganesha come cloni del tipo di risorsa systemd:
     </para>
<screen>
<prompt>crm(live)configure# </prompt>primitive nfs-ganesha-server systemd:nfs-ganesha \
op monitor interval=30s
<prompt>crm(live)configure# </prompt>clone nfs-ganesha-clone nfs-ganesha-server meta interleave=true
<prompt>crm(live)configure# </prompt>commit
<prompt>crm(live)configure# </prompt>status
    2 nodes configured
    2 resources configured

    Online: [ earth mars ]

    Full list of resources:
         Clone Set: nfs-ganesha-clone [nfs-ganesha-server]
         Started:  [ earth mars ]</screen>
    </step>
    <step>
     <para>
      Creare un IPAddr2 primitivo con la shell crm:
     </para>
<screen>
<prompt>crm(live)configure# </prompt>primitive ganesha-ip IPaddr2 \
params ip=192.168.2.1 cidr_netmask=24 nic=eth0 \
op monitor interval=10 timeout=20

<prompt>crm(live)# </prompt>status
Online: [ earth mars  ]
Full list of resources:
 Clone Set: nfs-ganesha-clone [nfs-ganesha-server]
     Started: [ earth mars ]
 ganesha-ip    (ocf::heartbeat:IPaddr2):    Started earth</screen>
    </step>
    <step>
     <para>
      Per configurare una relazione tra il server NFS Ganesha e l'IP virtuale mobile, si utilizza collocazione e ordinamento.
     </para>
<screen>
<prompt>crm(live)configure# </prompt>colocation ganesha-ip-with-nfs-ganesha-server inf: ganesha-ip nfs-ganesha-clone
<prompt>crm(live)configure# </prompt>order ganesha-ip-after-nfs-ganesha-server Mandatory: nfs-ganesha-clone ganesha-ip
</screen>
    </step>
    <step>
     <para>
      Utilizzare il comando <command>mount</command> dal client per assicurare che la configurazione del cluster sia completa:
     </para>
<screen><prompt>root # </prompt><command>mount</command> -t nfs -v -o sync,nfsvers=4 192.168.2.1:/ /mnt</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-ha-ap-cleanup">
   <title>Effettuare la pulizia delle risorse</title>
   <para>
    Nel caso di errore di NFS Ganesha in uno dei nodi, ad esempio <systemitem class="domainname">earth</systemitem>, risolvere il problema ed effettuare la pulizia della risorsa. Solo dopo aver effettuato la pulizia, la risorsa può riposizionarsi in sicurezza su <systemitem class="domainname">earth</systemitem> in caso di errore di NFS Ganesha su <systemitem class="domainname">mars</systemitem>.
   </para>
   <para>
    Per effettuare la pulizia della risorsa:
   </para>
<screen><prompt>root@earth # </prompt><command>crm</command> resource cleanup nfs-ganesha-clone earth
<prompt>root@earth # </prompt><command>crm</command> resource cleanup ganesha-ip earth</screen>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-ha-ap-ping-resource">
   <title>Impostazione della risorsa di ping</title>
   <para>
    In alcune situazioni, il server potrebbe non essere in grado di raggiungere il client a causa di un problema di rete. Una risorsa di ping può rilevare e mitigare questo problema. La configurazione della risorsa è facoltativa.
   </para>
   <procedure>
    <step>
     <para>
      Definire la risorsa di ping:
     </para>
<screen><prompt>crm(live)configure# </prompt>primitive ganesha-ping ocf:pacemaker:ping \
        params name=ping dampen=3s multiplier=100 host_list="<replaceable>CLIENT1</replaceable> <replaceable>CLIENT2</replaceable>" \
        op monitor interval=60 timeout=60 \
        op start interval=0 timeout=60 \
        op stop interval=0 timeout=60</screen>
     <para>
      <literal>host_list</literal> è un elenco di indirizzi IP separati da spazi. Viene eseguito regolarmente il ping sugli indirizzi IP per controllare eventuali indisponibilità di rete. Se un client deve sempre avere accesso al server NFS, aggiungerlo a <literal>host_list</literal>.
     </para>
    </step>
    <step>
     <para>
      Creare un clone:
     </para>
<screen><prompt>crm(live)configure# </prompt>clone ganesha-ping-clone ganesha-ping \
        meta interleave=true</screen>
    </step>
    <step>
     <para>
      Il comando seguente consente di creare un vincolo per il servizio NFS Ganesha. Forza il servizio a spostarsi su un altro nodo quando <literal>host_list</literal> non è raggiungibile.
     </para>
<screen><prompt>crm(live)configure# </prompt>location nfs-ganesha-server-with-ganesha-ping
        nfs-ganesha-clone \
        rule -inf: not_defined ping or ping lte 0</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ganesha-ha-deepsea">
   <title>DeepSea e HA di NFS Ganesha</title>
   <para>
    DeepSea non supporta la configurazione HA (alta disponibilità) di NFS Ganesha. Per impedire a DeepSea di andare in errore dopo aver configurato HA di NFS Ganesha, escludere avvio e arresto del servizio NFS Ganesha dalla Fase 4 di DeepSea:
   </para>
   <procedure>
    <step>
     <para>
      Copiare <filename>/srv/salt/ceph/ganesha/default.sls</filename> in <filename>/srv/salt/ceph/ganesha/ha.sls</filename>.
     </para>
    </step>
    <step>
     <para>
      Rimuovere la voce <literal>.service</literal> da <filename>/srv/salt/ceph/ganesha/ha.sls</filename> in modo che abbia l'aspetto seguente:
     </para>
<screen>include:
- .keyring
- .install
- .configure</screen>
    </step>
    <step>
     <para>
      Aggiungere la riga seguente a <filename>/srv/pillar/ceph/stack/global.yml</filename>:
     </para>
<screen>ganesha_init: ha</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-info">
  <title>ulteriori informazioni</title>

  <para>
   Ulteriori informazioni sono disponibili all'indirizzo <xref linkend="cha-ceph-nfsganesha"/>.
  </para>
 </sect1>
</chapter>
