<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_rbd.xml" version="5.0" xml:id="ceph-rbd">
 <title>RADOS Block Device (dispositivo di blocco RADOS)</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:translation>sì</dm:translation>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  Un blocco è una sequenza di byte, ad esempio un blocco di dati da 512 byte. Le interfacce di memorizzazione basate su blocchi rappresentano il modo più comune per memorizzare i dati con supporti rotanti, come dischi rigidi, CD, dischi floppy. L'ubiquità delle interfacce dei dispositivi di blocco rende un dispositivo di blocco virtuale un candidato ideale per interagire con un sistema di memorizzazione di massa come Ceph.
 </para>
 <para>
  I dispositivi di blocco Ceph consentono la condivisione di risorse fisiche e sono ridimensionabili. Questi dispositivi memorizzano i dati suddivisi in più OSD in un cluster Ceph. I dispositivi di blocco Ceph sfruttano le funzionalità RADOS come la creazione di snapshot, replica e coerenza. I RADOS Block Device (RBD, dispositivi di blocco RADOS) di Ceph interagiscono con gli OSD utilizzando i moduli del kernel o la libreria <systemitem>librbd</systemitem>.
 </para>
 <figure>
  <title>Protocollo RADOS</title>
  <mediaobject>
   <imageobject role="fo">
    <imagedata fileref="ceph_rbd_schema.png" width="70%" format="PNG"/>
   </imageobject>
   <imageobject role="html">
    <imagedata fileref="ceph_rbd_schema.png" width="70%" format="PNG"/>
   </imageobject>
  </mediaobject>
 </figure>
 <para>
  I dispositivi di blocco Ceph offrono prestazioni elevate con scalabilità infinita ai moduli del kernel. Supportano soluzioni di virtualizzazione come QEMU o sistemi di calcolo basati su cloud come OpenStack che fanno affidamento su <systemitem class="library">libvirt</systemitem>. È possibile utilizzare lo stesso cluster per attivare Object Gateway, CephFS e RADOS Block Device (dispositivi di blocco RADOS) simultaneamente.
 </para>
 <sect1 xml:id="ceph-rbd-commands">
  <title>Comandi dei dispositivi di blocco</title>

  <para>
   Il comando <command>rbd</command> consente di creare, elencare, analizzare e rimuovere immagini dei dispositivi di blocco. È inoltre possibile utilizzarlo per clonare immagini, creare snapshot, eseguire il rollback di un'immagine in uno snapshot o visualizzare uno snapshot.
  </para>

  <tip>
   <title>accesso a un cluster</title>
   <para>
    Per utilizzare i comandi dei RADOS Block Device (dispositivi di blocco RADOS), è necessario disporre dell'accesso a un cluster Ceph in esecuzione.
   </para>
  </tip>

  <sect2 xml:id="ceph-rbd-cmds-create">
   <title>Creazione di un'immagine del dispositivo di blocco</title>
   <para>
    Prima di poter aggiungere un dispositivo di blocco a un nodo, è necessario creare un'immagine correlata nel cluster. Per creare un'immagine del dispostivi di blocco, eseguire quanto riportato di seguito: 
   </para>
<screen><prompt>root # </prompt>rbd create --size <replaceable>megabytes</replaceable> <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
   <para>
    Ad esempio, per creare un'immagine da 1 GB denominata "bar" le cui informazioni vengono memorizzate in un pool denominato "swimmingpool", eseguire quanto riportato di seguito:
   </para>
<screen><prompt>root # </prompt>rbd create --size 1024 swimmingpool/bar</screen>
   <tip>
    <title>pool di default</title>
    <para>
     Se non si specifica un pool quando si crea un'immagine, questa verrà memorizzata nel pool "rbd" di default.
    </para>
   </tip>
   <note>
    <title>creare prima un pool</title>
    <para>
     È necessario creare un pool prima di poterlo specificare come origine. Per ulteriori dettagli, vedere <xref linkend="ceph-pools"/>.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-create-ec">
   <title>Creazione di un'immagine del dispositivo di blocco in un pool con codice di cancellazione</title>
   <para>
    A partire da SUSE Enterprise Storage 5, è possibile memorizzare i dati dell'immagine di un dispositivo di blocco in pool con codice di cancellazione. In un pool con codice di cancellazione è possibile memorizzare solo la parte riguardate i dati di un'immagine RBD. Inoltre, è necessario che il flag "overwrite" di sovrascrittura del pool con codice di cancellazione deve essere impostato su <emphasis>true</emphasis> e ciò è possibile farlo solo se tutti gli <emphasis/> OSD utilizzano BlueStore.
   </para>
   <para>
    I metadati dell'immagine non possono risiedere in un pool con codice di cancellazione. I metadati possono risiedere nel pool "rbd" o nel pool specificato esplicitamente dall'utente con il parametro <parameter>--pool=</parameter> nel comando <command>rbd create</command>.
   </para>
   <note>
    <title>BlueStore obbligatorio</title>
    <para>
     Per utilizzare i pool con codice di cancellazione per le immagini dei dispositivi di blocco è necessario BlueStore in tutti i nodi.
    </para>
   </note>
   <para>
    Seguire i passaggi riportati di seguito per creare un'immagine RBD in un pool con codice di cancellazione:
   </para>
<screen><prompt>root # </prompt><command>ceph</command> osd pool create <replaceable>POOL_NAME</replaceable> 12 12 erasure
<prompt>root # </prompt><command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> allow_ec_overwrites true

# Metadata will reside in pool "rbd", and data in pool "<replaceable>POOL_NAME</replaceable>"
<prompt>root # </prompt><command>rbd</command> create <replaceable>IMAGE_NAME</replaceable> --size=1G --data-pool <replaceable>POOL_NAME</replaceable>

#Metadata will reside in pool "<replaceable>OTHER_POOL</replaceable>", and data in pool "<replaceable>POOL_NAME</replaceable>"
<prompt>root # </prompt><command>rbd</command> create <replaceable>IMAGE_NAME</replaceable> --size=1G --data-pool <replaceable>POOL_NAME</replaceable> --pool=<replaceable>OTHER_POOL</replaceable></screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-list">
   <title>Elenco delle immagini dei dispositivi di blocco</title>
   <para>
    Per elencare i dispositivi di blocco nel pool "rbd", eseguire quanto riportato di seguito ("rbd" è il nome pool di default):
   </para>
<screen><prompt>root # </prompt>rbd ls</screen>
   <para>
    Per elencare i dispositivi di blocco in un pool denominato "swimmingpool", eseguire quanto riportato di seguito:
   </para>
<screen><prompt>root # </prompt>rbd ls swimmingpool</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-info">
   <title>Recupero delle informazioni sull'immagine</title>
   <para>
    Per recuperare informazioni da un'immagine "bar" in un pool denominato "swimmingpool", eseguire quanto riportato di seguito:
   </para>
<screen><prompt>root # </prompt>rbd info swimmingpool/bar</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-resize">
   <title>Ridimensionamento di un'immagine del dispositivo di blocco</title>
   <para>
    Le immagini del RADOS Block Device (dispositivo di blocco RADOS) sono sottoposte a thin provisioning, non utilizzano effettivamente alcuno spazio di memorizzazione fisico fino a quando non si inizia a salvare i dati in esse. Dispongono tuttavia di una capacità massima che è possibile impostare con l'opzione <option>--size</option>. Se si desidera aumentare (o diminuire) le dimensioni massime di un'immagine, eseguire quanto riportato di seguito:
   </para>
<screen><prompt>root # </prompt>rbd resize --size 2048 foo # to increase
rbd resize --size 2048 foo --allow-shrink # to decrease</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-rm">
   <title>Rimozione di un'immagine del dispositivo di blocco</title>
   <para>
    Per rimuovere un dispositivo di blocco che corrisponde a un'immagine "bar" in un pool denominato "swimmingpool", eseguire quanto riportato di seguito:
   </para>
<screen><prompt>root # </prompt>rbd rm swimmingpool/bar</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="storage-bp-integration-mount-rbd">
  <title>Montaggio e smontaggio di immagini RBD</title>

  <para>
   Dopo aver creato un RADOS Block Device (dispositivo di blocco RADOS), è possibile formattarlo, montarlo in modo che sia in grado di scambiare file e smontarlo al termine di tale operazione.
  </para>

  <procedure>
   <step>
    <para>
     Accertarsi che nel cluster Ceph sia incluso un pool con l'immagine disco che si desidera montare. Presupporre che il pool sia denominato <literal>mypool</literal> e l'immagine <literal>myimage</literal>.
    </para>
<screen>rbd list mypool</screen>
   </step>
   <step>
    <para>
     Mappare l'immagine nel nuovo dispositivo di blocco.
    </para>
<screen><prompt>root # </prompt>rbd map --pool mypool myimage</screen>
    <tip>
     <title>nome e autenticazione utente</title>
     <para>
      Per specificare un nome utente, utilizzare <option>--id <replaceable>user-name</replaceable></option>. Inoltre, se si utilizza l'autenticazione <systemitem>cephx</systemitem>, è necessario specificare anche un segreto. Quest'ultimo potrebbe essere ricavato da un portachiavi o da un file contenente il segreto:
     </para>
<screen><prompt>root # </prompt>rbd map --pool rbd myimage --id admin --keyring /path/to/keyring</screen>
     <para>
      oppure
     </para>
<screen><prompt>root # </prompt>rbd map --pool rbd myimage --id admin --keyfile /path/to/file</screen>
    </tip>
   </step>
   <step>
    <para>
     Elencare tutti i dispositivi mappati:
    </para>
<screen><prompt>root # </prompt>rbd showmapped
 id pool   image   snap device
 0  mypool myimage -    /dev/rbd0</screen>
    <para>
     Il dispositivo che si desidera utilizzare è <filename>/dev/rbd0</filename>.
    </para>
   </step>
   <step>
    <para>
     Creare un file system XFS sul dispositivo <filename>/dev/rbd0</filename>.
    </para>
<screen><prompt>root # </prompt>mkfs.xfs /dev/rbd0
 log stripe unit (4194304 bytes) is too large (maximum is 256KiB)
 log stripe unit adjusted to 32KiB
 meta-data=/dev/rbd0              isize=256    agcount=9, agsize=261120 blks
          =                       sectsz=512   attr=2, projid32bit=1
          =                       crc=0        finobt=0
 data     =                       bsize=4096   blocks=2097152, imaxpct=25
          =                       sunit=1024   swidth=1024 blks
 naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
 log      =internal log           bsize=4096   blocks=2560, version=2
          =                       sectsz=512   sunit=8 blks, lazy-count=1
 realtime =none                   extsz=4096   blocks=0, rtextents=0</screen>
   </step>
   <step>
    <para>
     Montare il dispositivo e verificare che sia montato correttamente. Sostituire <filename>/mnt</filename> con il proprio punto di montaggio.
    </para>
<screen><prompt>root # </prompt>mount /dev/rbd0 /mnt
<prompt>root # </prompt>mount | grep rbd0
/dev/rbd0 on /mnt type xfs (rw,relatime,attr2,inode64,sunit=8192,...</screen>
    <para>
     Adesso è possibile spostare i dati nel e dal dispositivo come se questo fosse una directory locale.
    </para>
    <tip>
     <title>aumento delle dimensioni del dispositivo RBD</title>
     <para>
      Se le dimensioni del dispositivo RBD non sono più sufficienti, è possibile aumentarle.
     </para>
     <orderedlist spacing="normal">
      <listitem>
       <para>
        Aumentare le dimensioni dell'immagine RBD, ad esempio fino a 10 GB.
       </para>
<screen><prompt>root # </prompt>rbd resize --size 10000 mypool/myimage
 Resizing image: 100% complete...done.</screen>
      </listitem>
      <listitem>
       <para>
        Accrescere il file system in modo da riempire le nuove dimensioni del dispositivo.
       </para>
<screen><prompt>root # </prompt>xfs_growfs /mnt
 [...]
 data blocks changed from 2097152 to 2560000</screen>
      </listitem>
     </orderedlist>
    </tip>
   </step>
   <step>
    <para>
     Una volta terminato l'accesso al dispositivo, non è possibile smontarlo.
    </para>
<screen><prompt>root # </prompt>unmount /mnt</screen>
   </step>
  </procedure>

  <tip>
   <title>montaggio/smontaggio manuale</title>
   <para>
    Poiché mappare e montare manualmente le immagini RBD dopo l'avvio e annullare tali operazioni prima dello spegnimento può essere tedioso, vengono forniti uno script <command>rbdmap</command> e un'unità <systemitem class="daemon">systemd</systemitem>. Vedere <xref linkend="ceph-rbd-rbdmap"/>.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="cha-ceph-snapshots-rbd">
  <title>Snapshot dei dispositivi di blocco</title>

  <para>
   Uno snapshot RBD è uno snapshot di un'immagine del RADOS Block Device (dispositivo di blocco RADOS). Gli snapshot consentono di conservare la cronologia dello stato dell'immagine. Ceph supporta anche il layering di snapshot, che consente di clonare rapidamente e facilmente immagini VM. Ceph supporta gli snapshot dei dispositivi di blocco mediante l'uso del comando <command>rbd</command> e molte interfacce di livello superiore, tra cui QEMU, <systemitem>libvirt</systemitem>, OpenStack e CloudStack.
  </para>

  <note>
   <para>
    Interrompere le operazioni di input e output prima di eseguire lo snapshot di un'immagine. Se l'immagine contiene un file system, questo deve presentare uno stato coerente <emphasis>prima</emphasis> dello snapshot.
   </para>
  </note>

  <sect2>
   <title>Note su Cephx</title>
   <para>
    Quando <systemitem>cephx</systemitem> è abilitato (vedere <link xlink:href="http://ceph.com/docs/master/rados/configuration/auth-config-ref/"/> per ulteriori informazioni), è necessario specificare un nome o ID utente e un percorso del portachiavi contenente la chiave corrispondente dell'utente. Per ulteriori dettagli, vedere <link xlink:href="http://ceph.com/docs/master/rados/operations/user-management/">User Management</link> (in lingua inglese). È inoltre possibile aggiungere la variabile di ambiente <systemitem>CEPH_ARGS</systemitem> per evitare di immettere di nuovo i seguenti parametri.
   </para>
<screen><prompt>root # </prompt>rbd --id <replaceable>user-ID</replaceable> --keyring=/path/to/secret <replaceable>commands</replaceable>
<prompt>root # </prompt>rbd --name <replaceable>username</replaceable> --keyring=/path/to/secret <replaceable>commands</replaceable></screen>
   <para>
    Ad esempio:
   </para>
<screen><prompt>root # </prompt>rbd --id admin --keyring=/etc/ceph/ceph.keyring <replaceable>commands</replaceable>
<prompt>root # </prompt>rbd --name client.admin --keyring=/etc/ceph/ceph.keyring <replaceable>commands</replaceable></screen>
   <tip>
    <para>
     Aggiungere l'utente e il segreto nella variabile di ambiente <systemitem>CEPH_ARGS</systemitem> in modo che non sia necessario immetterli ogni volta.
    </para>
   </tip>
  </sect2>

  <sect2>
   <title>Nozioni di base sugli snapshot</title>
   <para>
    Nelle procedure seguenti è dimostrato come creare, elencare e rimuovere snapshot mediante l'uso del comando <command>rbd</command> sulla riga di comando.
   </para>
   <sect3>
    <title>Creazione di snapshot</title>
    <para>
     Per creare uno snapshot con <command>rbd</command>, specificare l'opzione <option>snap create</option>, il nome pool e il nome immagine.
    </para>
<screen><prompt>root # </prompt>rbd --pool <replaceable>pool-name</replaceable> snap create --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
<prompt>root # </prompt>rbd snap create <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     Ad esempio:
    </para>
<screen><prompt>root # </prompt>rbd --pool rbd snap create --snap snapshot1 image1
<prompt>root # </prompt>rbd snap create rbd/image1@snapshot1</screen>
   </sect3>
   <sect3>
    <title>Elenchi di snapshot</title>
    <para>
     Per elencare gli snapshot di un'immagine, specificare il nome pool e il nome immagine.
    </para>
<screen><prompt>root # </prompt>rbd --pool <replaceable>pool-name</replaceable> snap ls <replaceable>image-name</replaceable>
<prompt>root # </prompt>rbd snap ls <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     Ad esempio:
    </para>
<screen><prompt>root # </prompt>rbd --pool rbd snap ls image1
<prompt>root # </prompt>rbd snap ls rbd/image1</screen>
   </sect3>
   <sect3>
    <title>Rollback di snapshot</title>
    <para>
     Per eseguire il rollback a uno snapshot con <command>rbd</command>, specificare l'opzione <option>snap rollback</option>, il nome pool, il nome immagine e il nome snapshot.
    </para>
<screen><prompt>root # </prompt>rbd --pool <replaceable>pool-name</replaceable> snap rollback --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
<prompt>root # </prompt>rbd snap rollback <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     Ad esempio:
    </para>
<screen><prompt>root # </prompt>rbd --pool pool1 snap rollback --snap snapshot1 image1
<prompt>root # </prompt>rbd snap rollback pool1/image1@snapshot1</screen>
    <note>
     <para>
      Eseguire il rollback di un'immagine a uno snapshot significa sovrascrivere la versione attuale dell'immagine con i dati provenienti da uno snapshot. La durata di esecuzione di un rollback aumenta proporzionalmente alle dimensioni dell'immagine. È <emphasis>più rapido eseguire la clonazione</emphasis> da uno snapshot <emphasis>piuttosto che eseguire il rollback</emphasis> di un'immagine a uno snapshot; questo è inoltre il metodo preferito per tornare a uno stato preesistente.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Eliminazione di uno snapshot</title>
    <para>
     Per eliminare uno snapshot con <command>rbd</command>, specificare l'opzione <option>snap rm</option>, il nome pool, il nome immagine e il nome utente.
    </para>
<screen><prompt>root # </prompt>rbd --pool <replaceable>pool-name</replaceable> snap rm --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
<prompt>root # </prompt>rbd snap rm <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     Ad esempio:
    </para>
<screen><prompt>root # </prompt>rbd --pool pool1 snap rm --snap snapshot1 image1
<prompt>root # </prompt>rbd snap rm pool1/image1@snapshot1</screen>
    <note>
     <para>
      Nei Ceph OSD i dati vengono eliminati in modo asincrono, quindi con l'eliminazione di uno snapshot non si libera immediatamente spazio su disco.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Eliminazione definitiva di snapshot</title>
    <para>
     Per eliminare tutti gli snapshot di un'immagine con <command>rbd</command>, specificare l'opzione <option>snap purge</option> e il nome immagine.
    </para>
<screen><prompt>root # </prompt>rbd --pool <replaceable>pool-name</replaceable> snap purge <replaceable>image-name</replaceable>
<prompt>root # </prompt>rbd snap purge <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     Ad esempio:
    </para>
<screen><prompt>root # </prompt>rbd --pool pool1 snap purge image1
<prompt>root # </prompt>rbd snap purge pool1/image1</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph-snapshoti-layering">
   <title>Layering</title>
   <para>
    Ceph supporta la creazione di molti cloni copia su scrittura (copy-on-write, COW) di uno snapshot del dispositivo di blocco. Il layering degli snapshot consente ai client dei dispositivi di blocco Ceph di creare immagini molto rapidamente. Ad esempio, si può creare un'immagine del dispositivo di blocco con una Linux VM scritta al suo interno, quindi eseguire lo snapshot dell'immagine, proteggere lo snapshot e creare tutti i cloni copia su scrittura desiderati. Poiché gli snapshot sono di sola lettura, la clonazione di uno di essi semplifica la semantica e consente quindi di creare i cloni rapidamente.
   </para>
   <note>
    <para>
     I termini "parent" e "child" menzionati negli esempi di riga di comando riportati sotto significano uno snapshot del dispositivo di blocco Ceph (parent) e l'immagine corrispondente clonata dallo snapshot (child).
    </para>
   </note>
   <para>
    In ciascuna immagine clonata (child) è memorizzato il rifermento alla rispettiva immagine superiore, che consente all'immagine clonata di aprire e leggere lo snapshot superiore.
   </para>
   <para>
    Un clone COW di uno snapshot si comporta esattamente come qualsiasi altra immagine del dispositivo di blocco Ceph. Nelle immagini clonate è possibile eseguire operazioni di lettura e scrittura ed è possibile clonarle e ridimensionarle. Con le immagini clonate non esistono restrizioni speciali. Il clone copia su scrittura di uno snapshot si riferisce tuttavia allo snapshot, quindi è <emphasis>necessario</emphasis> proteggere quest'ultimo prima di clonarlo.
   </para>
   <note>
    <para>
     Ceph supporta la clonazione solo per le immagini <emphasis>format 2</emphasis> (creata con <command>rbd create --image-format 2</command>).
    </para>
   </note>
   <sect3>
    <title>Introduzione al layering</title>
    <para>
     Il layering dei dispositivi di blocco Ceph è un processo semplice. È necessario disporre di un'immagine, creare uno snapshot dell'immagine, proteggere lo snapshot. Dopo aver eseguito questi passaggi, è possibile iniziare la clonazione dello snapshot.
    </para>
    <para>
     L'immagine clonata fa riferimento allo snapshot superiore e include l'ID pool, l'ID immagine e l'ID snapshot. L'inclusione dell'ID pool significa che è possibile clonare snapshot da un pool nelle immagini in un altro pool.
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       <emphasis>Modello di immagine</emphasis>: un caso comune di layering dei dispositivi di blocco consiste nel creare un'immagine master e uno snapshot che funge da modello per i cloni. Ad esempio, un utente può creare un'immagine per una distribuzione Linux (ad esempio, SUSE Linux Enterprise Server) e creare uno shapshot corrispondente. Periodicamente, l'utente può aggiornare l'immagine e creare un nuovo snapshot (ad esempio, <command>zypper ref &amp;&amp; zypper patch</command> seguito da <command>rbd snap create</command>). Ma mano che l'immagine matura, l'utente può clonare qualsiasi snapshot.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Modello esteso</emphasis>: un caso più avanzato consiste nell'estensione di un'immagine modello che fornisce ulteriori informazioni rispetto all'immagine di base. Ad esempio, un utente può clonare un'immagine (un modello VM) e installare un software diverso (ad esempio un database, un sistema di gestione di contenuti o un sistema di analisi) ed eseguire quindi lo snapshot dell'immagine estesa, che a sua volta è possibile aggiornare allo stesso modo dell'immagine di base.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Pool di modelli</emphasis>: un metodo per utilizzare il layering dei dispositivi di blocco consiste nel creare un pool contenente immagini master che fungono da modelli e snapshot di tali modelli. È quindi possibile estendere i privilegi di sola lettura agli utenti in modo che possano clonare gli snapshot senza doverli scrivere o eseguire nel pool.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Migrazione/recupero dell'immagine</emphasis>: un metodo per utilizzare il layering dei dispositivi di blocco consiste nell'eseguire la migrazione o il recupero dei dati da un pool in un altro.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3>
    <title>Protezione di uno snapshot</title>
    <para>
     I cloni accedono agli shapshot superiori. Tutti i cloni verrebbero interrotti se un utente eliminasse inavvertitamente lo snapshot superiore. Per impedire la perdita di dati, è necessario proteggere lo snapshot prima di poterlo clonare.
    </para>
<screen><prompt>root # </prompt>rbd --pool <replaceable>pool-name</replaceable> snap protect \
 --image <replaceable>image-name</replaceable> --snap <replaceable>snapshot-name</replaceable>
<prompt>root # </prompt>rbd snap protect <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     Ad esempio:
    </para>
<screen><prompt>root # </prompt>rbd --pool pool1 snap protect --image image1 --snap snapshot1
<prompt>root # </prompt>rbd snap protect pool1/image1@snapshot1</screen>
    <note>
     <para>
      Non è possibile eliminare uno snapshot protetto.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Clonazione di uno snapshot</title>
    <para>
     Per clonare uno snapshot, è necessario specificare il pool superiore, l'immagine e lo snapshot, il pool secondario e il nome immagine. È necessario proteggere lo snapshot prima di poterlo clonare.
    </para>
<screen><prompt>root # </prompt>rbd --pool <replaceable>pool-name</replaceable> --image <replaceable>parent-image</replaceable> \
 --snap <replaceable>snap-name</replaceable> --dest-pool <replaceable>pool-name</replaceable> \
 --dest <replaceable>child-image</replaceable>
<prompt>root # </prompt>rbd clone <replaceable>pool-name</replaceable>/<replaceable>parent-image</replaceable>@<replaceable>snap-name</replaceable> \
<replaceable>pool-name</replaceable>/<replaceable>child-image-name</replaceable></screen>
    <para>
     Ad esempio:
    </para>
<screen><prompt>root # </prompt>rbd clone pool1/image1@snapshot1 pool1/image2</screen>
    <note>
     <para>
      Si può clonare uno snapshot da un pool in un'immagine in un altro pool. Ad esempio, si possono mantenere immagini e snapshot di sola lettura come modelli in un pool e cloni scrivibili in un altro pool.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Annullamento della protezione di uno snapshot</title>
    <para>
     Prima di poter eliminare uno snapshot, è necessario annullarne la protezione. Inoltre, <emphasis>non</emphasis> è possibile eliminare snapshot con riferimenti dai cloni. È necessario appiattire ciascun clone di uno snapshot prima di poter eliminare quest'ultimo.
    </para>
<screen><prompt>root # </prompt>rbd --pool <replaceable>pool-name</replaceable> snap unprotect --image <replaceable>image-name</replaceable> \
 --snap <replaceable>snapshot-name</replaceable>
<prompt>root # </prompt>rbd snap unprotect <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     Ad esempio:
    </para>
<screen><prompt>root # </prompt>rbd --pool pool1 snap unprotect --image image1 --snap snapshot1
<prompt>root # </prompt>rbd snap unprotect pool1/image1@snapshot1</screen>
   </sect3>
   <sect3>
    <title>Elenco degli elementi secondari di uno snapshot</title>
    <para>
     Per elencare gli elementi secondari di uno snapshot, eseguire quanto riportato di seguito:
    </para>
<screen><prompt>root # </prompt>rbd --pool <replaceable>pool-name</replaceable> children --image <replaceable>image-name</replaceable> --snap <replaceable>snap-name</replaceable>
<prompt>root # </prompt>rbd children <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     Ad esempio:
    </para>
<screen><prompt>root # </prompt>rbd --pool pool1 children --image image1 --snap snapshot1
<prompt>root # </prompt>rbd children pool1/image1@snapshot1</screen>
   </sect3>
   <sect3>
    <title>Appiattimento di un'immagine clonata</title>
    <para>
     Le immagini clonate mantengono un riferimento allo snapshot superiore. Quando si rimuove il riferimento dal clone secondario nello parent superiore, di fatto si "appiattisce" l'immagine copiando le informazioni dallo snapshot al clone. La durata di appiattimento di un clone aumenta proporzionalmente alle dimensioni dello snapshot. Per eliminare uno snapshot, prima è necessario appiattire le immagini secondarie.
    </para>
<screen><prompt>root # </prompt>rbd --pool <replaceable>pool-name</replaceable> flatten --image <replaceable>image-name</replaceable>
<prompt>root # </prompt>rbd flatten <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     Ad esempio:
    </para>
<screen><prompt>root # </prompt>rbd --pool pool1 flatten --image image1
<prompt>root # </prompt>rbd flatten pool1/image1</screen>
    <note>
     <para>
      Poiché un'immagine appiattita contiene tutte le informazioni provenienti dallo snapshot, questa occuperà uno spazio di memorizzazione maggiore rispetto a un clone su più strati.
     </para>
    </note>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-rbd-rbdmap">
  <title>rbdmap: mappatura dei dispositivi RBD all'avvio</title>

  <para>
   <command>rbdmap</command> è uno script della shell che consente di automatizzare le operazioni <command>rbd map</command> e <command>rbd unmap</command> su una o più immagini del RADOS Block Device (dispositivo di blocco RADOS). Sebbene sia possibile eseguire manualmente lo script in qualsiasi momento, i principali casi di utilizzo sono la mappatura automatica e il montaggio di immagini RBD all'avvio (e lo smontaggio e l'annullamento della mappatura allo spegnimento), attivati dal sistema Init. A tal fine è incluso un file di unità <systemitem class="daemon">systemd</systemitem>, <filename>rbdmap.service</filename> con il pacchetto <systemitem>ceph-common</systemitem>.
  </para>

  <para>
   Lo script impiega un singolo argomento, che può essere <option>map</option> o <option>unmap</option>. In entrambi i casi lo script analizza sintatticamente un file di configurazione. Il valore di default è <filename>/etc/ceph/rbdmap</filename>, ma è possibile ignorarlo tramite una variabile di ambiente <literal>RBDMAPFILE</literal>. Ciascuna riga del file di configurazione corrisponde a un'immagine RBD che deve anche essere mappata o non mappata.
  </para>

  <para>
   Il file di configurazione presenta il seguente formato:
  </para>

<screen>image_specification rbd_options</screen>

  <variablelist>
   <varlistentry>
    <term>image_specification</term>
    <listitem>
     <para>
      Percorso di un'immagine in un pool. Specificare come <replaceable>pool_name</replaceable>/<replaceable>image_name</replaceable>. Se si omette <replaceable>pool_name</replaceable>, si presuppone che il default sia "rbd".
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>rbd_options</term>
    <listitem>
     <para>
      Elenco di parametri opzionale da passare al comando sottostante <command>rbd map</command>. Questi parametri e i rispettivi valori devono essere specificati come stringa separata da virgola, ad esempio:
     </para>
<screen>PARAM1=VAL1,PARAM2=VAL2,...</screen>
     <para>
      Nell'esempio con lo script <command>rbdmap</command> viene eseguito il seguente comando:
     </para>
<screen>rbd map <replaceable>pool_name</replaceable>/<replaceable>image_name</replaceable> --PARAM1 VAL1 --PARAM2 VAL2</screen>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   Quando viene eseguito come <command>rbdmap map</command>, lo script analizza sintatticamente il file di configurazione e, per ogni immagine RBD specificata, prima tenta di mappare l'immagine (con il comando <command>the rbd map</command>), quindi ne esegue il montaggio.
  </para>

  <para>
   Quando eseguito come <command>rbdmap unmap</command>, le immagini elencate nel file di configurazione il montaggio e la mappatura verranno annullati.
  </para>

  <para>
   <command>rbdmap unmap-all</command> tenta di smontare e successivamente di annullare la mappatura di tutte le immagini RBD attualmente mappate, indipendentemente dalla loro presenza nell'elenco del file di configurazione.
  </para>

  <para>
   Se ha esito positivo, l'operazione rbd map mappa l'immagine a un dispositivo /dev/rbdX device, quindi viene attivata una regola udev per creare un collegamento simbolico del nome del dispositivo <filename>/dev/rbd/<replaceable>pool_name</replaceable>/<replaceable>image_name</replaceable></filename> che punta al dispositivo realmente mappato.
  </para>

  <para>
   Affinché il montaggio e lo smontaggio abbiano esito positivo, il nome del dispositivo "intuitivo" deve avere una voce corrispondente in <filename>/etc/fstab</filename>. Quando si scrivono voci <filename>/etc/fstab</filename> per le immagini RBD, specificare l'opzione di montaggio "noauto" (o "nofail"). In tal modo si impedisce al sistema Init di tentare di montare il dispositivo troppo presto, perfino prima dell'esistenza del dispositivo in questione, poiché di norma <filename>rbdmap.service</filename> viene attivato piuttosto tardi nella sequenza di avvio.
  </para>

  <para>
   Per un elenco completo di opzioni <command>rbd</command>, vedere la documentazione relativa a <command>rbd</command> (<command>man 8 rbd</command>).
  </para>

  <para>
   Per alcuni esempi di utilizzo di <command>rbdmap</command>, vedere la documentazione relativa a <command>rbdmap</command> (<command>man 8 rbdmap</command>).
  </para>
 </sect1>
 <sect1 xml:id="ceph-rbd-mirror">
  <title>Copia speculare di RADOS Block Device (dispositivo di blocco RADOS)</title>

  <para>
   È possibile eseguire la copia speculare delle immagini RBD in modo asincrono tra due cluster Ceph. Questa funzionalità utilizza la funzione journaling dell'immagine RBD per assicurare la replica con coerenza per arresto anomalo tra cluster. La copia speculare è configurata per ogni singolo pool nei cluster peer e può essere configurata in modo che venga eseguita automaticamente la copia speculare di tutte le immagini in un pool o solo di un sottoinsieme specifico di immagini. Per la configurazione della copia speculare si utilizza il comando <command>rbd</command>. Il daemon <systemitem>rbd-mirror</systemitem> è responsabile del pull degli aggiornamenti delle immagini dal cluster peer remoto e della loro applicazione all'immagine nel cluster locale.
  </para>

  <important>
   <title>daemon rbd-mirror</title>
   <para>
    Per utilizzare la copia speculare RBD, è necessario disporre di due cluster Ceph, su ciascuno dei quali è in esecuzione il daemon <systemitem>rbd-mirror</systemitem>.
   </para>
  </important>

  <sect2 xml:id="rbd-mirror-daemon">
   <title>daemon rbd-mirror</title>
   <para>
    I due daemon <systemitem>rbd-mirror</systemitem> sono responsabili dell'osservazione dei journal dell'immagine sul cluster peer remoto e della riproduzione degli eventi del journal a fronte del cluster locale. La funzione di journaling dell'immagine RBD consente di registrare tutte le modifiche dell'immagine nell'ordine in cui vengono apportate. In tal modo si garantisce la disponibilità a livello locale di una copia speculare con coerenza per arresto anomalo dell'immagine remota.
   </para>
   <para>
    Il daemon <systemitem>rbd-mirror</systemitem> è disponibile nel pacchetto <systemitem>rbd-mirror</systemitem>. Installarlo, abilitarlo e avviarlo su uno dei nodi del cluster:
   </para>
<screen><prompt>root@minion &gt; </prompt>zypper install rbd-mirror
<prompt>root@minion &gt; </prompt>systemctl enable ceph-rbd-mirror@<replaceable>server_name</replaceable>.service
<prompt>root@minion &gt; </prompt>systemctl start ceph-rbd-mirror@<replaceable>server_name</replaceable>.service</screen>
   <important>
    <para>
     Per ciascun daemon <systemitem>rbd-mirror</systemitem> è necessario connettersi a entrambi i cluster simultaneamente.
    </para>
   </important>
  </sect2>

  <sect2 xml:id="ceph-rbd-mirror-poolconfig">
   <title>Configurazione del pool</title>
   <para>
    Nelle procedure seguenti è illustrato come eseguire task amministrativi di base per configurare la copia speculare tramite il comando <command>rbd</command>. La copia speculare è configurata per ogni singolo pool nei cluster Ceph.
   </para>
   <para>
    È necessario eseguire i passaggi della configurazione del pool su entrambi i cluster peer. Per maggior chiarezza, in queste procedure si presuppone che due cluster, denominati "local" e "remote", siano accessibili da un singolo host.
   </para>
   <para>
    Vedere la documentazione relativa a <command>rbd</command> (<command>man 8 rbd</command>) per ulteriori dettagli su come connettersi a cluster Ceph diversi.
   </para>
   <tip>
    <title>cluster multipli</title>
    <para>
     Il nome cluster negli esempi seguenti corrisponde a un file di configurazione Ceph omonimo <filename>/etc/ceph/remote.conf</filename>. Per la configurazione di più cluster, vedere la documentazione relativa a <link xlink:href="http://docs.ceph.com/docs/master/rados/configuration/ceph-conf/#running-multiple-clusters">ceph-conf</link> (in lingua inglese).
    </para>
   </tip>
   <sect3>
    <title>Abilitazione della copia speculare</title>
    <para>
     Per abilitare la copia speculare su un pool, specificare il sottocomando <command>mirror pool enable</command>, il nome pool e la modalità di esecuzione di copia speculare. La modalità di esecuzione di copia speculare può essere "pool" oppure "image":
    </para>
    <variablelist>
     <varlistentry>
      <term>pool</term>
      <listitem>
       <para>
        Tutte le immagini nel pool in cui è abilitata la funzione di journaling vengono sottoposte a copia speculare.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>image</term>
      <listitem>
       <para>
        La copia speculare deve essere abilitata esplicitamente su ciascuna immagine. Per ulteriori informazioni, vedere <xref linkend="rbd-mirror-enable-image-mirroring"/>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     Ad esempio:
    </para>
<screen><prompt>root # </prompt>rbd --cluster local mirror pool enable image-pool pool
<prompt>root # </prompt>rbd --cluster remote mirror pool enable image-pool pool</screen>
   </sect3>
   <sect3>
    <title>Disabilitazione della copia speculare</title>
    <para>
     Per disabilitare la copia speculare su un pool, specificare il sottocomando <command>mirror pool disable</command> e il nome pool. Quando si disabilita la copia speculare su un pool in questo modo, questa verrà disabilitata anche su qualsiasi immagine (nel pool) per la quale è stata abilitata esplicitamente la copia speculare.
    </para>
<screen><prompt>root # </prompt>rbd --cluster local mirror pool disable image-pool
<prompt>root # </prompt>rbd --cluster remote mirror pool disable image-pool</screen>
   </sect3>
   <sect3>
    <title>Aggiunta di un peer del cluster</title>
    <para>
     Affinché il daemon <systemitem>rbd-mirror</systemitem> rilevi il rispettivo cluster peer, è necessario registrare il peer nel pool. Per aggiungere un cluster peer in copia speculare, specificare il sottocomando <command>mirror pool peer add</command>, il nome pool e una specifica del cluster:
    </para>
<screen><prompt>root # </prompt>rbd --cluster local mirror pool peer add image-pool client.remote@remote
<prompt>root # </prompt>rbd --cluster remote mirror pool peer add image-pool client.local@local</screen>
   </sect3>
   <sect3>
    <title>Rimozione di un peer del cluster</title>
    <para>
     Per rimuovere un cluster peer in copia speculare, specificare il sottocomando <command>mirror pool peer remove</command>, il nome pool e l'UUID peer (reso disponibile dal comando <command>rbd mirror pool info</command>):
    </para>
<screen><prompt>root # </prompt>rbd --cluster local mirror pool peer remove image-pool \
 55672766-c02b-4729-8567-f13a66893445
<prompt>root # </prompt>rbd --cluster remote mirror pool peer remove image-pool \
 60c0e299-b38f-4234-91f6-eed0a367be08</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="rbd-mirror-imageconfig">
   <title>Configurazione dell'immagine</title>
   <para>
    Diversamente dalla configurazione del pool, la configurazione dell'immagine deve essere eseguita solo a fronte di un singolo cluster Ceph peer in copia speculare.
   </para>
   <para>
    Le immagini RBD sottoposte a copia speculare sono designate come <emphasis>primarie</emphasis> o <emphasis>non primarie</emphasis>. Questa è una proprietà dell'immagine e non del pool. Non è possibile modificare le immagini designate come non primarie.
   </para>
   <para>
    Le immagini vengono promosse automaticamente a primarie quando la copia speculare viene prima abilitata su un'immagine (implicitamente, se la modalità di copia speculare del pool è "pool" e la funzione di journaling dell'immagine è abilitata, oppure esplicitamente (vedere <xref linkend="rbd-mirror-enable-image-mirroring"/>) mediante il comando <command>rbd</command>).
   </para>
   <sect3>
    <title>Abilitazione del supporto per il journaling dell'immagine</title>
    <para>
     Nella copia speculare RBD viene utilizzata la funzione di journaling RBD per assicurare che l'immagine replicata mantenga sempre con coerenza per arresto anomalo. Prima di poter eseguire la copia speculare di un'immagine a un cluster peer, è necessario abilitare la funzione di journaling. È possibile abilitare la funzione al momento della creazione dell'immagine fornendo l'opzione <option>--image-feature exclusive-lock,journaling</option> al comando <command>rbd</command>.
    </para>
    <para>
     In alternativa, è possibile abilitare dinamicamente la funzione di journaling sulle immagini RBD preesistenti. Per abilitare il journaling, specificare il sottocomando <command>feature enable</command>, il nome del pool e dell'immagine e il nome della funzione:
    </para>
<screen><prompt>root # </prompt>rbd --cluster local feature enable image-pool/image-1 journaling</screen>
    <note>
     <title>dipendenza dell'opzione</title>
     <para>
      La funzione <option>journaling</option> dipende dalla funzione <option>exclusive-lock</option>. Se la funzione <option>exclusive-lock</option> non è già stata abilitata, è necessario farlo prima di abilitare la funzione <option>journaling</option>.
     </para>
    </note>
    <tip>
     <title>journaling su tutte le immagini nuove</title>
     <para>
      È possibile abilitare per default il journaling su tutte le immagini nuove aggiungendo la seguente riga al file di configurazione Ceph:
     </para>
<screen>rbd default features = 125</screen>
    </tip>
   </sect3>
   <sect3 xml:id="rbd-mirror-enable-image-mirroring">
    <title>Abilitazione della copia speculare dell'immagine</title>
    <para>
     Se la copia speculare è configurata in modalità "image" per il pool dell'immagine, è necessario abilitare esplicitamente la copia speculare per ciascuna immagine nel pool. Per abilitare la copia speculare per un'immagine specifica, specificare il sottocomando <command>mirror image enable</command> insieme al nome del pool e dell'immagine:
    </para>
<screen><prompt>root # </prompt>rbd --cluster local mirror image enable image-pool/image-1</screen>
   </sect3>
   <sect3>
    <title>Disabilitazione della copia speculare dell'immagine</title>
    <para>
     Per disabilitare la copia speculare per un'immagine specifica, specificare il sottocomando <command>mirror image disable</command> insieme al nome del pool e dell'immagine:
    </para>
<screen><prompt>root # </prompt>rbd --cluster local mirror image disable image-pool/image-1</screen>
   </sect3>
   <sect3>
    <title>Promozione e abbassamento di livello dell'immagine</title>
    <para>
     In uno scenario di failover in cui è necessario spostare la designazione primaria all'immagine nel cluster peer, è necessario interrompere l'accesso all'immagine primaria, abbassare di livello l'attuale immagine primaria, promuovere quella nuova e riprendere l'accesso all'immagine sul cluster alternativo.
    </para>
    <para>
     Per abbassare di livello un'immagine specifica a non primaria, specificare il sottocomando <command>mirror image demote</command> insieme al nome del pool e dell'immagine:
    </para>
<screen><prompt>root # </prompt>rbd --cluster local mirror image demote image-pool/image-1</screen>
    <para>
     Per abbassare di livello tutte le immagini primarie in un pool a non primarie, specificare il sottocomando <command>mirror pool demote</command> insieme al nome del pool:
    </para>
<screen><prompt>root # </prompt>rbd --cluster local mirror pool demote image-pool</screen>
    <para>
     Per promuovere un'immagine specifica a primaria, specificare il sottocomando <command>mirror image promote</command> insieme al nome del pool e dell'immagine:
    </para>
<screen><prompt>root # </prompt>rbd --cluster remote mirror image promote image-pool/image-1</screen>
    <para>
     Per promuovere tutte le immagini primarie in un pool a primarie, specificare il sottocomando <command>mirror pool promote</command> insieme al nome del pool:
    </para>
<screen><prompt>root # </prompt>rbd --cluster local mirror pool promote image-pool</screen>
    <tip>
     <title>suddivisione del carico I/O</title>
     <para>
      Poiché lo stato di primaria o non primaria si riferisce a un'immagine singola, è possibile fare in modo che il carico IO e il failover o il failback della fase venga suddiviso tra due cluster.
     </para>
    </tip>
    <note>
     <title>promozione forzata</title>
     <para>
      È possibile forzare la promozione utilizzando l'opzione <option>--force</option>. La promozione forzata è necessaria quando è impossibile propagare l'abbassamento di livello al cluster peer (ad esempio, in caso di errore del cluster o di interruzione della comunicazione). Ne risulterà uno scenario split brain tra i due peer e l'immagine non viene più sincronizzata fino all'emissione di un sottocomando <command>resync</command>.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Risincronizzazione forzata dell'immagine</title>
    <para>
     Se viene rilevato un evento split brain dal daemon <systemitem>rbd-mirror</systemitem>, non verrà effettuato alcun tentativo di copia speculare dell'immagine interessata finché non viene corretto. Per riprendere la copia speculare di un'immagine, prima abbassare di livello l'immagine definita obsoleta, quindi richiedere una risincronizzazione all'immagine primaria. Per richiedere una risincronizzazione dell'immagine, specificare il sottocomando <command>mirror image resync</command> insieme al nome del pool e dell'immagine:
    </para>
<screen><prompt>root # </prompt>rbd mirror image resync image-pool/image-1</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="rbd-mirror-status">
   <title>Stato della copia speculare</title>
   <para>
    Lo stato di replica del cluster peer viene memorizzato per ciascuna immagine primaria in copia speculare. È possibile recuperare tale stato mediante i sottocomandi <command>mirror image status</command> e <command>mirror pool status</command>:
   </para>
   <para>
    Per richiedere lo stato dell'immagine speculare, specificare il sottocomando <command>mirror image status</command> insieme al nome del pool e dell'immagine:
   </para>
<screen><prompt>root # </prompt>rbd mirror image status image-pool/image-1</screen>
   <para>
    Per richiedere lo stato di riepilogo del pool speculare, specificare il sottocomando <command>mirror pool status</command> insieme al nome del pool:
   </para>
<screen><prompt>root # </prompt>rbd mirror pool status image-pool</screen>
   <tip>
    <title/>
    <para>
     Con l'aggiunta dell'opzione <option>--verbose</option> al sottocomando <command>mirror pool status</command> verranno generati ulteriori dettagli sullo stato di ciascuna immagine in copia speculare nel pool.
    </para>
   </tip>
  </sect2>
 </sect1>
</chapter>
