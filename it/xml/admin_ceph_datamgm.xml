<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_datamgm.xml" version="5.0" xml:id="cha-storage-datamgm">
 <title>Gestione dei dati memorizzati</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>modifica</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>sì</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  L'algoritmo CRUSH determina la modalità di storage e recupero dei dati mediante il calcolo delle ubicazioni di memorizzazione dati. CRUSH consente ai client Ceph di comunicare direttamente con gli OSD piuttosto che tramite un server centralizzato o un broker. Grazie ad un sistema per lo storage e il recupero dei dati basato su algoritimi, Ceph consente di evitare single point of failure, colli di bottiglia delle prestazioni e limiti fisici alla rispettiva scalabilità.
 </para>
 <para>
  Per CRUSH è richiesta una mappa del cluster e viene utilizzata la mappa CRUSH per memorizzare e recuperare dati negli OSD in modo pressoché casuale, con una distribuzione uniforme dei dati nel cluster.
 </para>
 <para>
  Le mappe CRUSH contengono: un elenco di OSD, un elenco di "compartimenti" per aggregare i dispositivi in ubicazioni fisiche e un elenco di regole che indicano al sistema come replicare i dati nei pool di un cluster Ceph. Riflettendo l'organizzazione fisica dell'installazione sottostante, CRUSH è in grado di modellare, e quindi di risolvere, potenziali origini di errori dei dispositivi correlati. Tra le origini tipiche sono incluse la prossimità fisica, un'alimentazione condivisa e una rete condivisa. Mediante la codifica di queste informazioni nella mappa del cluster, le policy di posizionamento di CRUSH possono separare le repliche di oggetti in vari domini di errore, continuando a mantenere la distribuzione desiderata. Ad esempio, per risolvere la possibilità di errori simultanei, può essere necessario assicurare che le repliche dei dati siano su dispositivi che utilizzano scaffali, rack, alimentatori, controller e/o ubicazioni fisiche.
 </para>
 <para>
  Dopo aver distribuito un cluster Ceph cluster, viene generata una mappa CRUSH di default. Questa è adatta per l'ambiente sandbox Ceph. Quando tuttavia si distribuisce un cluster di dati su larga scala, considerare attentamente lo sviluppo di una mappa CRUSH personalizzata da utilizzare per gestire il cluster, migliorare le prestazioni e garantire la sicurezza dei dati.
 </para>
 <para>
  Se ad esempio un OSD si interrompe, può essere utile una mappa CRUSH per individuare il data center fisico, la stanza, la fila e il rack dell'host con l'OSD non riuscito nel caso in cui sia necessario utilizzare un supporto on site o sostituire l'hardware.
 </para>
 <para>
  In modo analogo, CRUSH può consentire di identificare gli errori più rapidamente. Ad esempio, se tutti gli OSD in un determinato rack si interrompono simultaneamente, è possibile che l'errore risieda in un interruttore di rete o nell'alimentazione del rack oppure nell'interruttore di rete piuttosto che negli OSD stessi.
 </para>
 <para>
  Una mappa CRUSH può consentire inoltre di identificare le ubicazioni fisiche in cui vengono memorizzate da Ceph le copie ridondanti dei dati quando i gruppi di posizionamento associati a un host non riuscito sono danneggiati.
 </para>
 <para>
  In una mappa CRUSH sono presenti tre sezioni principali.
 </para>
 <itemizedlist mark="bullet" spacing="normal">
  <listitem>
   <para>
    <xref linkend="datamgm-devices" xrefstyle="select: title"/>: qualsiasi dispositivo di memorizzazione degli oggetti, vale a dire il disco rigido che corrisponde a un daemon <systemitem>ceph-osd</systemitem>.
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="datamgm-buckets" xrefstyle="select: title"/>: aggregazione gerarchica delle ubicazioni di memorizzazione (ad esempio file, rack, host e così via) e i rispettivi pesi assegnati.
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="datamgm-rules" xrefstyle="select: title"/>: modo in cui vengono selezionati i compartimenti.
   </para>
  </listitem>
 </itemizedlist>
 <sect1 xml:id="datamgm-devices">
  <title>Dispositivi</title>

  <para>
   Per mappare i gruppi di posizionamento negli OSD, per la mappa CRUSH è richiesto un elenco di dispositivi OSD (il nome del daemon OSD). L'elenco dei dispositivi viene visualizzato prima nella mappa CRUSH.
  </para>

<screen>#devices
device <replaceable>num</replaceable> <replaceable>osd.name</replaceable></screen>

  <para>
   Ad esempio:
  </para>

<screen>#devices
device 0 osd.0
device 1 osd.1
device 2 osd.2
device 3 osd.3</screen>

  <para>
   Come regola generale, un daemon OSD viene mappato in un disco singolo.
  </para>
 </sect1>
 <sect1 xml:id="datamgm-buckets">
  <title>Compartimenti</title>

  <para>
   Le mappe CRUSH contengono un elenco di OSD, che è possibile organizzare in "compartimenti" in modo da aggregare i dispositivi in ubicazioni fisiche.
  </para>

  <informaltable frame="none">
   <tgroup cols="3">
    <colspec colwidth="10*"/>
    <colspec colwidth="30*"/>
    <colspec colwidth="70*"/>
    <tbody>
     <row>
      <entry>
       <para>
        0
       </para>
      </entry>
      <entry>
       <para>
        OSD
       </para>
      </entry>
      <entry>
       <para>
        Un daemon OSD (osd.1, osd.2 e così via).
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        1
       </para>
      </entry>
      <entry>
       <para>
        Host
       </para>
      </entry>
      <entry>
       <para>
        Un nome host contenente uno o più OSD.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        2
       </para>
      </entry>
      <entry>
       <para>
        Telaio
       </para>
      </entry>
      <entry>
       <para>
        Telaio di cui è composto il rack.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        3
       </para>
      </entry>
      <entry>
       <para>
        Rack
       </para>
      </entry>
      <entry>
       <para>
        Un rack per computer. L'impostazione di default è <literal>unknownrack</literal>.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        4
       </para>
      </entry>
      <entry>
       <para>
        Fila
       </para>
      </entry>
      <entry>
       <para>
        Una fila in una serie di rack.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        5
       </para>
      </entry>
      <entry>
       <para>
        Pdu
       </para>
      </entry>
      <entry>
       <para>
        Unità di distribuzione dell'alimentazione
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        6
       </para>
      </entry>
      <entry>
       <para>
        Pod
       </para>
      </entry>
      <entry>
       <para/>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        7
       </para>
      </entry>
      <entry>
       <para>
        Stanza
       </para>
      </entry>
      <entry>
       <para>
        Una stanza contenente i rack e le file di host.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        8
       </para>
      </entry>
      <entry>
       <para>
        Data center
       </para>
      </entry>
      <entry>
       <para>
        Data center fisico contenente le stanze.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        9
       </para>
      </entry>
      <entry>
       <para>
        Regione
       </para>
      </entry>
      <entry>
       <para/>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        10
       </para>
      </entry>
      <entry>
       <para>
        Radice
       </para>
      </entry>
      <entry>
       <para/>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </informaltable>

  <tip>
   <para>
    È possibile rimuovere questi tipi e creare tipi di compartimenti propri.
   </para>
  </tip>

  <para>
   Gli strumenti di distribuzione Ceph generano una mappa CRUSH che contiene un compartimento per ciascun ho e un pool denominato "default", utile per il pool <literal>rbd</literal> di default. I tipi di compartimenti rimanenti sono un mezzo per memorizzare le informazioni sull'ubicazione fisica di nodi/compartimenti, che rende l'amministrazione del cluster molto più semplice in caso di malfunzionamento degli OSD, degli host o dell'hardware di rete e l'amministratore deve accedere all'hardware fisico.
  </para>

  <para>
   Un compartimento è dotato di un tipo, un nome univoco (stringa), un ID univoco espresso come numero intero negativo, un peso relativo alla capacità/funzionalità totale degli elementi, algoritmo del compartimento (<literal>straw</literal> per default) e hash (<literal>0</literal> per default, che riflette l'hash CRUSH <literal>rjenkins1</literal>). Un compartimento può contenere uno o più elementi. Gli elementi possono essere costituiti da altri compartimenti o OSD. Gli elementi possono avere un peso che riflette quello relativo dell'elemento.
  </para>

<screen>[bucket-type] [bucket-name] {
  id [a unique negative numeric ID]
  weight [the relative capacity/capability of the item(s)]
  alg [the bucket type: uniform | list | tree | straw ]
  hash [the hash type: 0 by default]
  item [item-name] weight [weight]
}</screen>

  <para>
   Nell'esempio seguenti è illustrato come utilizzare i compartimenti per aggregare un pool e ubicazioni fisiche come un data center, una stanza, un rack e una fila.
  </para>

<screen>host ceph-osd-server-1 {
        id -17
        alg straw
        hash 0
        item osd.0 weight 1.00
        item osd.1 weight 1.00
}

row rack-1-row-1 {
        id -16
        alg straw
        hash 0
        item ceph-osd-server-1 weight 2.00
}

rack rack-3 {
        id -15
        alg straw
        hash 0
        item rack-3-row-1 weight 2.00
        item rack-3-row-2 weight 2.00
        item rack-3-row-3 weight 2.00
        item rack-3-row-4 weight 2.00
        item rack-3-row-5 weight 2.00
}

rack rack-2 {
        id -14
        alg straw
        hash 0
        item rack-2-row-1 weight 2.00
        item rack-2-row-2 weight 2.00
        item rack-2-row-3 weight 2.00
        item rack-2-row-4 weight 2.00
        item rack-2-row-5 weight 2.00
}

rack rack-1 {
        id -13
        alg straw
        hash 0
        item rack-1-row-1 weight 2.00
        item rack-1-row-2 weight 2.00
        item rack-1-row-3 weight 2.00
        item rack-1-row-4 weight 2.00
        item rack-1-row-5 weight 2.00
}

room server-room-1 {
        id -12
        alg straw
        hash 0
        item rack-1 weight 10.00
        item rack-2 weight 10.00
        item rack-3 weight 10.00
}

datacenter dc-1 {
        id -11
        alg straw
        hash 0
        item server-room-1 weight 30.00
        item server-room-2 weight 30.00
}

pool data {
        id -10
        alg straw
        hash 0
        item dc-1 weight 60.00
        item dc-2 weight 60.00
}</screen>
 </sect1>
 <sect1 xml:id="datamgm-rules">
  <title>Set di regole</title>

  <para>
   Le mappe CRUSH supportano la nozione delle "regole CRUSH", che sono regole che determinano il posizionamento dei dati per un pool. Per i cluster di grandi dimensioni è probabile che vengano creati numerosi pool, ciascuno dei quali presenta set di regole e regole CRUSH propri. La mappa CRUSH di default ha una regola per ogni pool e un set di regole assegnato a ciascuno dei pool di default.
  </para>

  <note>
   <para>
    Nella maggior parte dei casi non sarà necessario modificare le regole di default. Quando si crea un nuovo pool, il rispettivo set di regole di default è 0.
   </para>
  </note>

  <para>
   Una regola è strutturata nel modo seguente:
  </para>

<screen>rule <replaceable>rulename</replaceable> {

        ruleset <replaceable>ruleset</replaceable>
        type <replaceable>type</replaceable>
        min_size <replaceable>min-size</replaceable>
        max_size <replaceable>max-size</replaceable>
        step <replaceable>step</replaceable>

}</screen>

  <variablelist>
   <varlistentry>
    <term>ruleset</term>
    <listitem>
     <para>
      Un numero intero. Classifica una regola come appartenente a un set di regole. Attivato mediante l'impostazione del set di regole in un pool. Questa opzione è obbligatoria. Il valore di default è <literal>0</literal>.
     </para>
     <important>
      <para>
       È necessario aumentare continuamente il numero del set di regole rispetto al valore di default 0, altrimenti il monitoraggio correlato potrebbe bloccarsi.
      </para>
     </important>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>type</term>
    <listitem>
     <para>
      Stringa. Descrive una regola per un disco rigido (replicato) o un RAID. Questa opzione è obbligatoria. L'impostazione di default è <literal>replicated</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>min_size</term>
    <listitem>
     <para>
      Un numero intero. Se un gruppo di posizionamento effettua meno repliche rispetto al numero specificato, questa regola NON verrà selezionata da CRUSH. Questa opzione è obbligatoria. Il valore di default è <literal>2</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>max_size</term>
    <listitem>
     <para>
      Un numero intero. Se un gruppo di posizionamento effettua più repliche rispetto al numero specificato, questa regola NON verrà selezionata da CRUSH. Questa opzione è obbligatoria. Il valore di default è <literal>10</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>step take <replaceable>bucket</replaceable>
    </term>
    <listitem>
     <para>
      Assume un nome di compartimento e inizia a eseguire l'iterazione lungo l'albero. Questa opzione è obbligatoria. Per una spiegazione sull'iterazione nell'albero, vedere <xref linkend="datamgm-rules-step-iterate"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>step <replaceable>target</replaceable><replaceable>mode</replaceable><replaceable>num</replaceable> type <replaceable>bucket-type</replaceable>
    </term>
    <listitem>
     <para>
      <replaceable>target</replaceable> può essere sia <literal>choose</literal> sia <literal>chooseleaf</literal>. Quando impostata su <literal>choose</literal>, viene selezionato un numero di compartimenti. <literal>chooseleaf</literal> seleziona direttamente gli OSD (nodi foglia) dal sottoalbero di ciascun compartimento nel set di compartimenti.
     </para>
     <para>
      <replaceable>mode</replaceable> può essere sia <literal>firstn</literal> sia <literal>indep</literal>. Vedere <xref linkend="datamgm-rules-step-mode"/>.
     </para>
     <para>
      Seleziona il numero di compartimenti di un determinato tipo. Dove N è il numero delle opzioni disponibili, se <replaceable>num</replaceable> &gt; 0 &amp;&amp; &lt; N, scegliere tale numero di compartimenti; se <replaceable>num</replaceable> &lt; 0, significa N - <replaceable>num</replaceable>; e se <replaceable>num</replaceable> == 0, scegliere N compartimenti (tutti disponibili). Segue <literal>step take</literal> o <literal>step choose</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>step emit</term>
    <listitem>
     <para>
      Genera il valore corrente e svuota lo stack. Di norma utilizzata alla fine di una regola, ma può essere impiegata per formare alberi diversi nell'ambito della stessa regola. Segue <literal>step choose</literal>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <important>
   <para>
    Per attivare una o più regole con un numero di set di regole comune in un pool, impostare il numero del set di regole nel pool.
   </para>
  </important>

  <sect2 xml:id="datamgm-rules-step-iterate">
   <title>Iterazione nell'albero di nodi</title>
   <para>
    È possibile visualizzare la struttura definita nei compartimenti sotto forma di albero di nodi. I compartimenti sono nodi e gli OSD sono foglie nell'albero.
   </para>
   <para>
    Le regole nella mappa CRUSH definiscono il modo in cui gli OSD vengono selezionati dall'albero. Una regola inizia con un nodo, quindi effettua un'iterazione lungo l'albero per restituire un set di OSD. Non è possibile definire quale diramazione selezionare. L'algoritmo CRUSH assicura invece che il set di OSD soddisfi i requisiti di replica e distribuisca uniformemente i dati.
   </para>
   <para>
    Con <literal>step take</literal> <replaceable>bucket</replaceable> l'iterazione nell'albero di noti inizia in un determinato compartimento (non in un tipo di compartimento). Se gli OSD provenienti da tutte le diramazioni nell'albero vengono restituiti, il compartimento deve essere il compartimento radice. Altrimenti per gli "step" seguenti l'iterazione ha luogo tramite un sottoalbero.
   </para>
   <para>
    Dopo <literal>step take</literal> nella definizione della regola seguono una o più voci <literal>step choose</literal>. Ogni <literal>step choose</literal> sceglie un numero definito di nodi (o diramazioni) dal nodo superiore selezionato precedentemente.
   </para>
   <para>
    Alla fine gli OSD selezionati vengono restituiti con <literal>step emit</literal>.
   </para>
   <para>
    <literal>step chooseleaf</literal> è una pratica funzione che consente di selezionare direttamente gli OSD dalle diramazioni del compartimento specificato.
   </para>
   <para>
    La <xref linkend="datamgm-rules-step-iterate-figure"/> è un esempio di come viene utilizzato <literal>step</literal> per l'iterazione in un albero. Le frecce e i numeri arancioni corrispondono a <literal>example1a</literal> e <literal>example1b</literal>, mentre quelli blu corrispondono a <literal>example2</literal> nelle seguenti definizioni delle regole.
   </para>
   <figure xml:id="datamgm-rules-step-iterate-figure">
    <title>Albero di esempio</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="crush-step.svg" width="100%" format="SVG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="crush-step.png" width="100%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
<screen># orange arrows
rule example1a {
        ruleset 0
        type replicated
        min_size 2
        max_size 10
        # orange (1)
        step take rack1
        # orange (2)
        step choose firstn 0 host
        # orange (3)
        step choose firstn 1 osd
        step emit
}

rule example1b {
        ruleset 0
        type replicated
        min_size 2
        max_size 10
        # orange (1)
        step take rack1
        # orange (2) + (3)
        step chooseleaf firstn 0 host
        step emit
}

# blue arrows
rule example2 {
        ruleset 0
        type replicated
        min_size 2
        max_size 10
        # blue (1)
        step take room1
        # blue (2)
        step chooseleaf firstn 0 rack
        step emit
}</screen>
  </sect2>

  <sect2 xml:id="datamgm-rules-step-mode">
   <title>firstn e indep</title>
   <para>
    Una regola CRUSH definisce le sostituzioni per i nodi o gli OSD non riusciti (vedere <xref linkend="datamgm-rules"/>). Per lo <literal>step</literal> della parola chiave è richiesto <literal>firstn</literal> o <literal>indep</literal> come parametro. Nella figura <xref linkend="datamgm-rules-step-mode-indep-figure"/> è riportato un esempio.
   </para>
   <para>
    <literal>firstn</literal> aggiunge i nodi di sostituzione alla fine dell'elenco dei nodi attivi. Nel caso di un nodo non riuscito, i seguenti nodi integri vengono spostati a sinistra per colmare il vuoto lasciato dal nodo non riuscito. Questo è il metodo di default e desiderato per i <emphasis>pool replicati</emphasis>, perché un nodo secondario contiene già tutti i dati e può essere impiegato immediatamente per svolgere i compiti del nodo primario.
   </para>
   <para>
    <literal>indep</literal> seleziona i nodi di sostituzione per ciascun nodo attivo. La sostituzione di un nodo non riuscito non cambia l'ordine dei nodi rimanenti. Ciò è adatto per i <emphasis>pool con codice di cancellazione</emphasis>. Nei pool con codice di cancellazione, i dati memorizzati in un nodo dipendono dalla rispettiva posizione nella selezione del nodo. Quando l'ordine dei nodi cambia, tutti i dati nei nodi interessati devono essere trasferiti.
   </para>
   <note>
    <title>pool di cancellazione</title>
    <para>
     Assicurarsi che sia impostata una regola che utilizzi <literal>indep</literal> per ciascun <emphasis>pool con codice di cancellazione</emphasis>.
    </para>
   </note>
   <figure xml:id="datamgm-rules-step-mode-indep-figure">
    <title>Metodi di sostituzione dei nodi</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="crush-firstn-indep.svg" width="100%" format="SVG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="crush-firstn-indep.png" width="100%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>
 </sect1>
 <sect1 xml:id="op-crush">
  <title>Modifica della mappa CRUSH</title>

  <para>
   In questa sezione vengono presentati i modi con cui apportare modifiche di base alla mappa CRUSH, come la modifica di una mappa CRUSH, dei parametri della mappa CRUSH e l'aggiunta, lo spostamento o la rimozione di un OSD.
  </para>

  <sect2>
   <title>Modifica di una mappa CRUSH</title>
   <para>
    Per modificare una mappa CRUSH esistente, procedere come indicato di seguito:
   </para>
   <procedure>
    <step>
     <para>
      Ottenere una mappa CRUSH. Per ottenere la mappa CRUSH per il cluster, eseguire quanto riportato di seguito:
     </para>
<screen><prompt>root # </prompt>ceph osd getcrushmap -o <replaceable>compiled-crushmap-filename</replaceable></screen>
     <para>
      L'output Ceph (<option>-o</option>) sarà una mappa CRUSH compilata nel nome file specificato. Poiché la mappa CRUSH è compilata, è necessario decompilarla prima di poterla modificare.
     </para>
    </step>
    <step>
     <para>
      Decompilare una mappa CRUSH. Per decompilare una mappa CRUSH, eseguire quanto riportato di seguito:
     </para>
<screen><prompt>cephadm &gt; </prompt>crushtool -d <replaceable>compiled-crushmap-filename</replaceable> \
 -o <replaceable>decompiled-crushmap-filename</replaceable></screen>
     <para>
      Ceph decompilerà (<option>-d</option>) la mappa CRUSH compilata e la genererà (<option>-o</option>) nel nome file specificato.
     </para>
    </step>
    <step>
     <para>
      Modificare almeno uno dei parametri di dispositivi, compartimenti e regole.
     </para>
    </step>
    <step>
     <para>
      Compilare una mappa CRUSH. Per compilare una mappa CRUSH, eseguire quanto riportato di seguito:
     </para>
<screen><prompt>cephadm &gt; </prompt>crushtool -c <replaceable>decompiled-crush-map-filename</replaceable> \
 -o <replaceable>compiled-crush-map-filename</replaceable></screen>
     <para>
      Ceph memorizzerà la mappa CRUSH compilata nel nome file specificato.
     </para>
    </step>
    <step>
     <para>
      Impostare una mappa CRUSH. Per impostare la mappa CRUSH per il cluster, eseguire quanto riportato di seguito:
     </para>
<screen><prompt>root # </prompt>ceph osd setcrushmap -i <replaceable>compiled-crushmap-filename</replaceable></screen>
     <para>
      Ceph immetterà la mappa CRUSH compilata del nome file specificato come mappa CRUSH per il cluster.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="op-crush-addosd">
   <title>Aggiunta/Spostamento di un OSD</title>
   <para>
    Per aggiungere o spostare un OSD nella mappa CRUSH di un cluster in esecuzione, eseguire quanto riportato di seguito:
   </para>
<screen><prompt>root # </prompt>ceph osd crush set <replaceable>id_or_name</replaceable> <replaceable>weight</replaceable> root=<replaceable>pool-name</replaceable>
<replaceable>bucket-type</replaceable>=<replaceable>bucket-name</replaceable> ...</screen>
   <variablelist>
    <varlistentry>
     <term>id</term>
     <listitem>
      <para>
       Un numero intero. L'ID numerico dell'OSD. Questa opzione è obbligatoria. 
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>name</term>
     <listitem>
      <para>
       Stringa. Indica il nome completo dell'OSD. Questa opzione è obbligatoria. 
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>weight</term>
     <listitem>
      <para>
       Valore doppio. Il peso CRUSH per l'OSD. Questa opzione è obbligatoria. 
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pool</term>
     <listitem>
      <para>
       Una coppia di chiavi/valori. Per default, la gerarchia CRUSH contiene il default del pool come rispettiva radice. Questa opzione è obbligatoria. 
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bucket-type</term>
     <listitem>
      <para>
       Coppia di chiavi/valori. È possibile specificare l'ubicazione degli OSD nella gerarchia CRUSH.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Nell'esempio seguente viene aggiunto <literal>osd.0</literal> alla gerarchia o viene spostato l'OSD da un'ubicazione precedente.
   </para>
<screen><prompt>root # </prompt>ceph osd crush set osd.0 1.0 root=data datacenter=dc1 room=room1 \
row=foo rack=bar host=foo-bar-1</screen>
  </sect2>

  <sect2 xml:id="op-crush-osdweight">
   <title>Regolazione del peso CRUSH di un OSD</title>
   <para>
    Per regolare il peso CRUSH di un OSD nella mappa CRUSH di un cluster in esecuzione, eseguire quanto riportato di seguito:
   </para>
<screen><prompt>root # </prompt>ceph osd crush reweight <replaceable>name</replaceable> <replaceable>weight</replaceable></screen>
   <variablelist>
    <varlistentry>
     <term>name</term>
     <listitem>
      <para>
       Stringa. Indica il nome completo dell'OSD. Questa opzione è obbligatoria. 
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>weight</term>
     <listitem>
      <para>
       Valore doppio. Il peso CRUSH per l'OSD. Questa opzione è obbligatoria. 
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="op-crush-osdremove">
   <title>Rimozione di un OSD </title>
   <para>
    Per rimuovere un OSD dalla mappa CRUSH di un cluster in esecuzione, eseguire quanto riportato di seguito:
   </para>
<screen><prompt>root # </prompt>ceph osd crush remove <replaceable>name</replaceable></screen>
   <variablelist>
    <varlistentry>
     <term>name</term>
     <listitem>
      <para>
       Stringa. Indica il nome completo dell'OSD. Questa opzione è obbligatoria. 
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="op-crush-movebucket">
   <title>Spostamento di un compartimento</title>
   <para>
    Per spostare un compartimento in un'ubicazione o posizione diversa nella gerarchia di mappe CRUSH, eseguire quanto riportato di seguito:
   </para>
<screen><prompt>root # </prompt>ceph osd crush move <replaceable>bucket-name</replaceable> <replaceable>bucket-type</replaceable>=<replaceable>bucket-name</replaceable>, ...</screen>
   <variablelist>
    <varlistentry>
     <term>bucket-name</term>
     <listitem>
      <para>
       Stringa. Il nome del compartimento da spostare/riposizionare. Questa opzione è obbligatoria. 
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bucket-type</term>
     <listitem>
      <para>
       Coppia di chiavi/valori. È possibile specificare l'ubicazione del compartimento nella gerarchia CRUSH.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="scrubbing">
  <title>Pulitura</title>

  <para>
   Oltre a creare più copie di oggetti, Ceph assicura l'integrità dei dati mediante la <emphasis>pulitura</emphasis> di gruppi di posizionamento. La pulitura Ceph è analoga all'esecuzione di <command>fsck</command> nel strato di memorizzazione degli oggetti. Per ciascun gruppo di posizionamento, Ceph genera un catalogo di tutti gli oggetti e confronta ciascun oggetto e le rispettive repliche per assicurare che non vi siano oggetti mancanti o non corrispondenti. Con la pulitura durante il giorno, vengono verificati le dimensioni e gli attributi degli oggetti, mentre con una pulitura settimanale più approfondita si effettua la lettura dei dati e mediante l'uso di checksum ne garantisce l'integrità.
  </para>

  <para>
   La pulitura è importante per mantenere l'integrità dei dati, ma può ridurre le prestazioni. È possibile regolare le impostazioni seguenti per aumentare o diminuire le operazioni di pulitura:
  </para>

  <variablelist>
   <varlistentry>
    <term><option>osd max scrubs</option>
    </term>
    <listitem>
     <para>
      Numero massimo di operazioni di pulitura simultanee per Ceph OSD. Il valore di default è 1.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub begin hour</option>, <option>osd scrub end hour</option>
    </term>
    <listitem>
     <para>
      Le ore del giorno (da 0 a 24) che definiscono la finestra temporale in cui può essere eseguita la pulitura. Per default inizia alle ore 0 e termina alle 24.
     </para>
     <important>
      <para>
       Se l'intervallo di pulitura del gruppo di posizionamento supera il valore dell'impostazione <option>osd scrub max interval</option>, la pulitura verrà eseguita indipendentemente dalla finestra temporale definita per la stessa.
      </para>
     </important>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub during recovery</option>
    </term>
    <listitem>
     <para>
      Consente le operazioni di pulitura durante il recupero. Se si imposta su 'false' la pianificazione di nuove operazioni di pulitura verrà disabilitata durante un recupero attivo. Le operazioni di pulitura già in esecuzione continueranno normalmente. Questa opzione è utile per ridurre il carico di cluster trafficati. L'impostazione di default è "true".
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub thread timeout</option>
    </term>
    <listitem>
     <para>
      Numero massimo di secondi prima del timeout della pulitura. Il valore di default è 60.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub finalize thread timeout</option>
    </term>
    <listitem>
     <para>
      Numero massimo di secondi prima del timeout del thread di completamento della pulitura. Il valore di default è 60*10.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub load threshold</option>
    </term>
    <listitem>
     <para>
      Carico massimo normalizzato. Ceph non eseguirà la pulitura quando il carico del sistema (come definito dal rapporto <literal>getloadavg()</literal>/numero di <literal>online cpus</literal>) supera tale numero. Il valore di default è 0,5.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub min interval</option>
    </term>
    <listitem>
     <para>
      Intervallo minimo espresso in secondi per la pulitura di Ceph OSD quando il carico del cluster Ceph è basso. Il valore di default è 60*60*24 (una volta al giorno).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub max interval</option>
    </term>
    <listitem>
     <para>
      Intervallo massimo espresso in secondi per la pulitura di Ceph OSD indipendentemente dal carico del cluster. 7*60*60*24 (una volta alla settimana).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub chunk min</option>
    </term>
    <listitem>
     <para>
      Numero minimo di porzioni dell'archivio dati da ripulire durante una singola operazione. I blocchi Ceph eseguono la scrittura in un singola porzione durante la pulitura. Il valore di default è 5.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub chunk max</option>
    </term>
    <listitem>
     <para>
      Numero massimo di porzioni dell'archivio dati da ripulire durante una singola operazione. Il valore di default è 25.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub sleep</option>
    </term>
    <listitem>
     <para>
      Tempo di inattività prima della pulitura del gruppo di porzioni successivo. Se si aumenta questo valore si rallenta l'intera operazione di pulitura con un impatto minore sulle operazioni del client. Il valore di default è 0.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd deep scrub interval</option>
    </term>
    <listitem>
     <para>
      Intervallo per la pulitura "approfondita" (lettura completa di tutti i dati). L'opzione <option>osd scrub load threshold</option> non influisce su questa impostazione. Il valore di default è 60*60*24*7 (una volta alla settimana).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub interval randomize ratio</option>
    </term>
    <listitem>
     <para>
      Consente di aggiungere un ritardo casuale al valore <option>osd scrub min interval</option> quando si pianifica il lavoro di pulitura successivo per un gruppo di posizionamento. Il ritardo è un valore casuale più piccolo rispetto al risultato di <option>osd scrub min interval</option> * <option>osd scrub interval randomized ratio</option>. Pertanto, l'impostazione di default distribuisce le puliture praticamente in modo casuale nella finestra temporale consentita di [1, 1,5] * <option>osd scrub min interval</option>. Il valore di default è 0,5
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd deep scrub stride</option>
    </term>
    <listitem>
     <para>
      Consente di leggere le dimensioni quando si effettua una pulitura approfondita. Il valore di default è 524288 (512 kB).
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="op-mixed-ssd-hdd">
  <title>Combinazione di SSD e HDD sullo stesso nodo</title>

  <para>
   Si potrebbe desiderare di configurare un cluster Ceph in modo tale che su ciascun nodo sia presente una combinazione di SSD e HDD, con un pool di memorizzazione sugli SSD veloci e uno sugli HDD più lenti. A tal fine, è necessario modificare la mappa CRUSH.
  </para>

  <para>
   La mappa CRUSH di default presenterà una gerarchia semplice, in cui la radice di default contiene gli host e questi a loro volta contengono gli OSD, ad esempio:
  </para>

<screen><prompt>root # </prompt>ceph osd tree
 ID CLASS WEIGHT   TYPE NAME      STATUS REWEIGHT PRI-AFF
 -1       83.17899 root default
 -4       23.86200     host cpach
 2   hdd  1.81898         osd.2      up  1.00000 1.00000
 3   hdd  1.81898         osd.3      up  1.00000 1.00000
 4   hdd  1.81898         osd.4      up  1.00000 1.00000
 5   hdd  1.81898         osd.5      up  1.00000 1.00000
 6   hdd  1.81898         osd.6      up  1.00000 1.00000
 7   hdd  1.81898         osd.7      up  1.00000 1.00000
 8   hdd  1.81898         osd.8      up  1.00000 1.00000
 15  hdd  1.81898         osd.15     up  1.00000 1.00000
 10  nvme 0.93100         osd.10     up  1.00000 1.00000
 0   ssd  0.93100         osd.0      up  1.00000 1.00000
 9   ssd  0.93100         osd.9      up  1.00000 1.00000 </screen>

  <para>
   In tal modo non vi è alcuna distinzione tra i tipi di dischi. Per suddividere gli OSD in SSD e HDD, è necessario creare una seconda gerarchia nella mappa CRUSH:
  </para>

<screen><prompt>root # </prompt>ceph osd crush add-bucket ssd root</screen>

  <para>
   Avendo creato la nuova radice per gli SSD, è necessario aggiungervi gli host. Vale a dire che occorre creare nuove voci di host. Ma poiché non è possibile che lo stesso nome host compaia più di una volta in una mappa CRUSH, questa utilizza nomi host falsi. Tali nomi falsi non devono essere risolvibili da DNS. Per CRUSH i nomi host non hanno importanza, servono solo per creare le gerarchie corrette. La modifica che <emphasis>deve</emphasis> essere apportata per supportare i nomi host falsi consiste nell'impostare
  </para>

<screen>osd crush update on start = false</screen>

  <para>
   nel file <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/global.conf</filename> e quindi eseguire la fase 3 di DeepSea per distribuire la modifica (per ulteriori informazioni fare riferimento a <xref linkend="ds-custom-cephconf"/>):
  </para>

<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
</screen>

  <para>
   Altrimenti, gli OSD spostati verranno reimpostati successivamente alla rispettiva ubicazione originale nella radice di default e il cluster non si comporta come previsto.
  </para>

  <para>
   Una volta modificata tale impostazione, aggiungere i nuovi host falsi nella radice SSD:
  </para>

<screen><prompt>root # </prompt>ceph osd crush add-bucket node1-ssd host
<prompt>root # </prompt>ceph osd crush move node1-ssd root=ssd
<prompt>root # </prompt>ceph osd crush add-bucket node2-ssd host
<prompt>root # </prompt>ceph osd crush move node2-ssd root=ssd
<prompt>root # </prompt>ceph osd crush add-bucket node3-ssd host
<prompt>root # </prompt>ceph osd crush move node3-ssd root=ssd</screen>

  <para>
   Infine, per ciascun OSD SSD, spostare l'OSD nella radice SSD. Nell'esempio, si presuppone che osd.0, osd.1 e osd.2 siano ospitati fisicamente negli SSD:
  </para>

<screen><prompt>root # </prompt>ceph osd crush add osd.0 1 root=ssd
<prompt>root # </prompt>ceph osd crush set osd.0 1 root=ssd host=node1-ssd
<prompt>root # </prompt>ceph osd crush add osd.1 1 root=ssd
<prompt>root # </prompt>ceph osd crush set osd.1 1 root=ssd host=node2-ssd
<prompt>root # </prompt>ceph osd crush add osd.2 1 root=ssd
<prompt>root # </prompt>ceph osd crush set osd.2 1 root=ssd host=node3-ssd</screen>

  <para>
   La gerarchia CRUSH adesso deve avere il seguente aspetto:
  </para>

<screen><prompt>root # </prompt>ceph osd tree
ID WEIGHT  TYPE NAME                   UP/DOWN REWEIGHT PRIMARY-AFFINITY
-5 3.00000 root ssd
-6 1.00000     host node1-ssd
 0 1.00000         osd.0                    up  1.00000          1.00000
-7 1.00000     host node2-ssd
 1 1.00000         osd.1                    up  1.00000          1.00000
-8 1.00000     host node3-ssd
 2 1.00000         osd.2                    up  1.00000          1.00000
-1 0.11096 root default
-2 0.03699     host node1
 3 0.01849         osd.3                    up  1.00000          1.00000
 6 0.01849         osd.6                    up  1.00000          1.00000
-3 0.03699     host node2
 4 0.01849         osd.4                    up  1.00000          1.00000
 7 0.01849         osd.7                    up  1.00000          1.00000
-4 0.03699     host node3
 5 0.01849         osd.5                    up  1.00000          1.00000
 8 0.01849         osd.8                    up  1.00000          1.00000</screen>

  <para>
   Ora, creare una regola CRUSH che ha come destinazione la radice SSD:
  </para>

<screen><prompt>root # </prompt>ceph osd crush rule create-simple ssd_replicated_ruleset ssd host</screen>

  <para>
   L'impostazione di default originale <option>replicated_ruleset</option> (con ID 0) avrà come destinazione gli HDD. La nuova impostazione <option>ssd_replicated_ruleset</option> (con ID 1) avrà come destinazione gli SSD.
  </para>

  <para>
   Tutti i pool esistenti continueranno a utilizzare gli HDD perché rientrano nella gerarchia di default nella mappa CRUSH. È possibile creare un nuovo pool per utilizzare solo gli SSD:
  </para>

<screen><prompt>root # </prompt>ceph osd pool create ssd-pool 64 64
<prompt>root # </prompt>ceph osd pool set ssd-pool crush_rule ssd_replicated_ruleset</screen>
 </sect1>
</chapter>
