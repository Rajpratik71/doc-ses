<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_cephfs.xml" version="5.0" xml:id="cha-ceph-as-cephfs">

 <title>Installazione di CephFS</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>modifica</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>sì</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  Il file system Ceph (CephFS) è un file system POSIX compatibile che utilizza un cluster di storage Ceph per memorizzare i dati. CephFS utilizza lo stesso sistema di cluster dei dispositivi di blocco Ceph, storage oggetto Ceph con le API S3 e Swift o associazioni native (<systemitem>librados</systemitem>).
 </para>
 <para>
  Per utilizzare CephFS, deve essere presente un cluster di storage Ceph in esecuzione e almeno un <emphasis>server metadati Ceph</emphasis> in esecuzione.
 </para>
 <sect1 xml:id="ceph-cephfs-limitations">
  <title>Scenari e guida CephFS supportati</title>

  <para>
   Con SUSE Enterprise Storage, SUSE introduce il supporto ufficiale per molti scenari in cui si utilizza il CephFS componente distribuito e scalato. Questa voce descrive i limiti concreti e fornisce una guida per i casi d'uso suggeriti.
  </para>

  <para>
   Una distribuzione CephFS supportata deve rispettare questi requisiti:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Almeno un Metadata Server. SUSE consiglia di distribuire diversi nodi con il ruolo MDS. Solo uno sarà "attivo" e il resto "passivo". Ricordare di menzionare tutti i nodi MDS nel comando <command>mount</command> quando si monta il CephFS da un client.
    </para>
   </listitem>
   <listitem>
    <para>
     Le snapshot CephFS sono disattivate (default) e non supportate in questa versione.
    </para>
   </listitem>
   <listitem>
    <para>
     I client sono basati su SUSE Linux Enterprise Server 12 SP2 o SP3, mediante il driver del modulo kernel <literal>cephfs</literal>. Il modulo FUSE non è supportato.
    </para>
   </listitem>
   <listitem>
    <para>
     Le quote CephFS non sono supportate in SUSE Enterprise Storage, in quanto il supporto per le quote è implementato solo nel client FUSE.
    </para>
   </listitem>
   <listitem>
    <para>
     CephFS supporta modifiche del layout del file come documentato in <link xlink:href="http://docs.ceph.com/docs/jewel/cephfs/file-layouts/"/>. Tuttavia, mentre il file system è montato da qualsiasi client, i nuovi pool di dati potrebbero non venire aggiunti a un file system CephFS esistente (<literal>ceph mds add_data_pool</literal>). Possono essere aggiunti solo mentre il file system non è montato.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="ceph-cephfs-mds">
  <title>Metadata Server Ceph</title>

  <para>
   Il Metadata Server (MDS) Ceph memorizza i metadati per il CephFS. I dispositivi di blocco Ceph e lo storage oggetto Ceph <emphasis>non</emphasis> utilizzano MDS. I MDS consentono agli utenti del file system POSIX di eseguire comandi di base, come <command>ls</command> o <command>find</command>, senza caricare eccessivamente il cluster di storage Ceph.
  </para>

  <sect2 xml:id="ceph-cephfs-mdf-add">
   <title>Aggiunta di un Metadata Server</title>
   <para>
    È possibile distribuire MDS durante il processo di distribuzione iniziale del cluster come descritto in <xref linkend="ceph-install-stack"/>, oppure aggiungerlo a un cluster già distribuito come descritto in <xref linkend="salt-adding-nodes"/>.
   </para>
   <para>
    Dopo aver distribuito MDS, consentire al servizio <literal>Ceph OSD/MDS</literal> nelle impostazioni del firewall del server dove è distribuito MDS: avviare <literal>yast</literal>, selezionare <menuchoice> <guimenu>Security and Users (Sicurezza e utenti)</guimenu> <guimenu>Firewall</guimenu> <guimenu>Allowed Services (Servizi consentiti)</guimenu> </menuchoice> e nel menu a discesa <guimenu>Service to Allow (Servizio da consentire)</guimenu> selezionare <guimenu>Ceph OSD/MDS</guimenu>. Se per il nodo Ceph MDS non è consentito traffico completo, il montaggio di un file system non riesce, anche se altre operazioni possono funzionare correttamente.
   </para>
  </sect2>

  <sect2 xml:id="ceph-cephfs-mds-config">
   <title>Configurazione di un Metadata Server</title>
   <para>
    È possibile definire con precisione il comportamento di MDS inserendo opzioni pertinenti nel file di configurazione <filename>ceph.conf</filename>.
   </para>
   <variablelist>
    <title>Dimensione cache MDS</title>
    <varlistentry>
     <term><option>mds cache memory limit</option>
     </term>
     <listitem>
      <para>
       Il limite di memoria software (in byte) che il MDS applica per la cache. Gli amministratori devono utilizzare questa invece della precedente impostazione <option>mds cache size</option>. Il valore predefinito è 1 GB.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>mds cache reservation</option>
     </term>
     <listitem>
      <para>
       La prenotazione della cache (memoria o nodi) da mantenere per la cache MDS. Il MDS, quando inizia a toccare la prenotazione, revoca le capacità del client finché la dimensione della cache si riduce per ripristinare la riserva. Il default è 0,05.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Per un elenco dettagliato delle opzioni di configurazione correlate a MDS, vedere <link xlink:href="http://docs.ceph.com/docs/master/cephfs/mds-config-ref/"/>.
   </para>
   <para>
    Per un elenco dettagliato delle opzioni di configurazione "journaler" di MDS, vedere <link xlink:href="http://docs.ceph.com/docs/master/cephfs/journaler/"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs">
  <title>CephFS</title>

  <para>
   Se si ha un cluster di storage Ceph in stato corretto con almeno un server metadati Ceph, è possibile creare e montare il file system Ceph. Accertare che il client disponga di connettività di rete e di un corretto portachiavi di autenticazione.
  </para>

  <sect2 xml:id="ceph-cephfs-cephfs-create">
   <title>Creazione di CephFS</title>
   <para>
    Un CephFS richiede almeno due pool RADOS: uno per i <emphasis>dati</emphasis> e uno per i <emphasis>metadati</emphasis>. Quando si configurano tali pool, considerare:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      L'utilizzo di un più alto livello di replica per il pool metadati, in quanto eventuali perdite di dati in questo pool possono rendere l'intero file system inaccessibile.
     </para>
    </listitem>
    <listitem>
     <para>
      L'utilizzo di storage a più bassa latenza, come i SSD per il pool di metadati, in quanto verrà migliorata la latenza osservata delle operazioni del file system sui client.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Quando si assegna un <literal>role-mds</literal> in <filename>policy.cfg</filename>, i pool richiesti vengono creati automaticamente. È possibile creare manualmente i pool <literal>cephfs_data</literal> e <literal>cephfs_metadata</literal> per l'ottimizzazione delle prestazioni manuali prima di configurare il Metadata Server. DeepSea non crea questi pool se esistono già.
   </para>
   <para>
    Per ulteriori informazioni sulla gestione dei pool, vedere <xref linkend="ceph-pools"/>.
   </para>
   <para>
    Per creare i due pool richiesti, ad esempio "cephfs_data" e "cephfs_metadata", con le impostazioni di default da utilizzare con CephFS, eseguire i comandi indicati:
   </para>
<screen><prompt>root # </prompt>ceph osd pool create cephfs_data <replaceable>pg_num</replaceable>
<prompt>root # </prompt>ceph osd pool create cephfs_metadata <replaceable>pg_num</replaceable></screen>
   <para>
    È possibile utilizzare i pool EC invece dei pool replicati. Si consiglia di utilizzare solo pool EC per requisiti di basse prestazioni e accesso casuale non frequente, ad esempio storage a freddo, backup, archiviazione. CephFS sui pool EC richiede l'attivazione di BlueStore e il pool deve avere l'opzione <literal>allow_ec_overwrite</literal> impostata. È possibile impostare questa opzione eseguendo <command>ceph osd pool set ec_pool allow_ec_overwrites true</command>.
   </para>
   <para>
    La codifica di cancellazione aggiunge significativo overhead alle operazioni del file system, in particolare i piccoli aggiornamenti. Questo overhead è inerente all'impiego della codifica di cancellazione come meccanismo di tolleranza dell'errore. Questa penalità è un compromesso per overhead di spazio di storage sensibilmente ridotto.
   </para>
   <para>
    Quando si creano i pool, è possibile abilitare il file system con il comando <command>ceph fs new</command>:
   </para>
<screen><prompt>root # </prompt>ceph fs new <replaceable>fs_name</replaceable> <replaceable>metadata_pool_name</replaceable> <replaceable>data_pool_name</replaceable></screen>
   <para>
    Ad esempio:
   </para>
<screen><prompt>root # </prompt>ceph fs new cephfs cephfs_metadata cephfs_data</screen>
   <para>
    È possibile controllare che il file system sia stato creato elencando tutti i CephFS disponibili:
   </para>
<screen><prompt>root # </prompt><command>ceph</command> <option>fs ls</option>
 name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]</screen>
   <para>
    Dopo aver creato il file system, il MDS sarà in grado di entrare in uno stato <emphasis>attivo</emphasis>. Ad esempio, in un singolo sistema MDS:
   </para>
<screen><prompt>root # </prompt><command>ceph</command> <option>mds stat</option>
e5: 1/1/1 up</screen>
   <tip>
    <title>altri argomenti</title>
    <para>
     Ulteriori informazioni su task specifici, ad esempio montaggio, smontaggio e impostazione CephFS avanzata, sono disponibili in <xref linkend="cha-ceph-cephfs"/>.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-cephfs-multimds">
   <title>Dimensione cluster MDS</title>
   <para>
    Un'istanza CephFS può essere servita da più daemon MDS attivi. Tutti i daemon MDS attivi assegnati a un'istanza CephFS distribuiscono la struttura di directory del file system tra di loro e perciò aumentano il carico dei client concorrenti. Per aggiungere un daemon MDS attivo a un'istanza CephFS, ne è necessario uno di standby di riserva. Avviare un daemon aggiuntivo o utilizzare un'istanza standby esistente.
   </para>
   <para>
    Il comando seguente visualizza il numero corrente di daemon MDS attivi e passivi.
   </para>
<screen><prompt>root # </prompt>ceph mds stat</screen>
   <para>
    Il comando seguente imposta il numero di MDS attivi a due in un'istanza del file system.
   </para>
<screen><prompt>root # </prompt>ceph fs set <replaceable>fs_name</replaceable> max_mds 2</screen>
   <para>
    Per ridurre il cluster MDS prima di un aggiornamento, sono necessari due passaggi. Primo, impostare <option>max_mds</option> in modo che rimanga solo un'istanza:
   </para>
<screen><prompt>root # </prompt>ceph fs set <replaceable>fs_name</replaceable> max_mds 1</screen>
   <para>
    disattivare quindi esplicitamente gli altri daemon MDS attivi:
   </para>
<screen><prompt>root # </prompt>ceph mds deactivate <replaceable>fs_name</replaceable>:<replaceable>rank</replaceable></screen>
   <para>
    dove <replaceable>rank</replaceable> è il numero di un daemon MDS attivo di un'istanza del file system, compreso tra 0 e <option>max_mds</option>-1. Per ulteriori informazioni, vedere <link xlink:href="http://docs.ceph.com/docs/luminous/cephfs/multimds/"/>.
   </para>
  </sect2>

  <sect2 xml:id="ceph-cephfs-multimds-updates">
   <title>Aggiornamenti e cluster MDS </title>
   <para>
    Durante gli aggiornamenti di Ceph, i flag della caratteristica su un'istanza del file system possono cambiare (di solito aggiungendo nuove caratteristiche). I daemon incompatibili (ad esempio le versioni precedenti) non sono in grado di funzionare con una caratteristica incompatibile impostata e rifiutano di avviarsi. Questo significa che l'aggiornamento e il riavvio di un daemon può provocare l'arresto e il rifiuto di avvio di tutti gli altri daemon non ancora aggiornati. Per questo motivo, si consiglia di ridurre il cluster MDS attivo alla dimensione uno e di arrestare tutti i daemon di standby prima di aggiornare Ceph. I passaggi manuali di questa procedura di aggiornamento sono i seguenti:
   </para>
   <procedure>
    <step>
     <para>
      Aggiornare i pacchetti relativi al Ceph con <command>zypper</command>.
     </para>
    </step>
    <step>
     <para>
      Ridurre il cluster MDS attivo come descritto sopra a 1 istanza e arrestare tutti i daemon MDS di standby con le unità <systemitem class="daemon">systemd</systemitem> su tutti gli altri nodi:
     </para>
<screen><prompt>root # </prompt>systemctl stop ceph-mds\*.service ceph-mds.target</screen>
    </step>
    <step>
     <para>
      Riavviare quindi solo il singolo daemon MDS rimanente, provocandone il riavvio con il binario aggiornato.
     </para>
<screen><prompt>root # </prompt>systemctl restart ceph-mds\*.service ceph-mds.target</screen>
    </step>
    <step>
     <para>
      Riavviare tutti gli altri daemon MDS e riconfigurare l'impostazione <option>max_mds</option> desiderata.
     </para>
<screen><prompt>root # </prompt>systemctl start ceph-mds.target</screen>
    </step>
   </procedure>
   <para>
    Se si utilizza DeepSea, seguire questa procedura nel caso in cui il pacchetto
    <package>ceph</package> sia stato aggiornato nelle Fasi 0 e 4. È possibile eseguire questa procedura mentre i client hanno l'istanza CephFS montata e l'I/O è in corso. Tenere tuttavia presente che vi sarà una breve pausa di I/O durante il riavvio del MDS. I client si ripristinano automaticamente.
   </para>
   <para>
    È opportuno ridurre al massimo possibile il carico di I/O prima di aggiornare un cluster MDS. Un cluster MDS inattivo passa più rapidamente attraverso questa procedura di aggiornamento. Al contrario, su un cluster molto carico con più daemon MDS è essenziale ridurre il carico in anticipo per impedire di travolgere un singolo daemon MDS da I/O ininterrotti.
   </para>
  </sect2>
 </sect1>
</chapter>
