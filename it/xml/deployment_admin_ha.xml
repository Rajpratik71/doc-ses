<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_admin_ha.xml" version="5.0" xml:id="cha-admin-ha">
 <title>Configurazione nodo admin Ceph HA</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:translation>sì</dm:translation>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  Il <emphasis>nodo admin Ceph</emphasis> è un nodo del cluster Ceph dove è in esecuzione il servizio Salt master. Il nodo admin è un punto centrale del cluster Ceph perché gestisce i nodi residui del cluster tramite interrogazione e istruendo i relativi servizi Salt minion. Comprende in genere anche altri servizi, ad esempio la UI Web openATTIC con il dashboard <emphasis>Grafana</emphasis> supportato dal toolkit di monitoraggio <emphasis>Prometheus</emphasis>.
 </para>
 <para>
  In caso di guasto del nodo admin Ceph, occorre di solito fornire un nuovo hardware funzionante per il nodo e ripristinare lo stack di configurazione cluster completo da un backup recente. Tale metodo richiede tempo e determina l'indisponibilità del cluster.
 </para>
 <para>
  Per evitare l'indisponibilità dell'operatività del cluster Ceph provocata dal guasto del nodo admin, si consiglia di utilizzare il cluster ad alta disponibilità (High Availability, HA) per il nodo admin Ceph.
 </para>
 <sect1 xml:id="admin-ha-architecture">
  <title>Profilo del cluster HA per il nodo admin Ceph</title>

  <para>
   Il concetto di un cluster HA prevede che in caso di guasto di un nodo del cluster, l'altro nodo subentri automaticamente nel ruolo, compreso il nodo admin Ceph virtualizzato. In questo modo, gli altri nodi del cluster Ceph non avvertono il guasto del nodo admin Ceph.
  </para>

  <para>
   La soluzione HA minima per il nodo admin Ceph richiede il seguente hardware:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Due server in grado di eseguire SUSE Linux Enterprise con l'estensione High Availability e virtualizzare il nodo admin Ceph.
    </para>
   </listitem>
   <listitem>
    <para>
     Due o più percorsi di comunicazione di rete ridondanti, ad esempio tramite Network Device Bonding.
    </para>
   </listitem>
   <listitem>
    <para>
     Storage condiviso per ospitare le immagini dei dischi della macchina virtuale del nodo admin Ceph. Lo storage condiviso deve essere accessibile da entrambi i server. Può essere ad esempio un'esportazione NFS, una condivisione Samba o un target iSCSI.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Ulteriori informazioni sui requisiti del cluster sono disponibili all'indirizzo <link xlink:href="https://www.suse.com/documentation/sle-ha-12/install-quick/data/install-quick.html#sec_ha_inst_quick_req"/>.
  </para>

  <figure>
   <title>Cluster HA a 2 nodi per il nodo admin Ceph</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ceph_admin_ha1.png" width="60%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ceph_admin_ha1.png" width="60%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>
 </sect1>
 <sect1 xml:id="admin-ha-cluster">
  <title>Creazione del cluster HA con il nodo admin Ceph</title>

  <para>
   La procedura seguente riepiloga i passaggi più importanti di costruzione del cluster HA per virtualizzare il nodo admin Ceph. Per i dettagli, consultare i collegamenti indicati.
  </para>

  <procedure>
   <step>
    <para>
     Configurare un cluster HA di base a 2 nodi con storage condiviso come descritto in <link xlink:href="https://www.suse.com/documentation/sle-ha-12/install-quick/data/install-quick.html"/>.
    </para>
   </step>
   <step>
    <para>
     Su entrambi i nodi cluster, installare tutti i pacchetti richiesti per eseguire l'ipervisore KVM e il toolkit <systemitem class="library">libvirt</systemitem> come descritto in <link xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_vt_installation_kvm.html"/>.
    </para>
   </step>
   <step>
    <para>
     Sul primo nodo del cluster, creare una nuova macchina virtuale (VM) KVM utilizzando <systemitem class="library">libvirt</systemitem> come descritto in <link xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_libvirt_inst_vmm.html"/>. Utilizzare lo storage condiviso preconfigurato per memorizzare le immagini del disco della VM.
    </para>
   </step>
   <step>
    <para>
     Al termine della configurazione della VM, esportarne la configurazione su un file XML sullo storage condiviso. Usare la seguente sintassi:
    </para>
<screen>
<prompt>root # </prompt>virsh dumpxml <replaceable>VM_NAME</replaceable> &gt; /path/to/shared/vm_name.xml
</screen>
   </step>
   <step>
    <para>
     Creare una risorsa per la VM del nodo admin Ceph. Per informazioni generali sulla creazione di risorse HA, consultare <link xlink:href="https://www.suse.com/documentation/sle-ha-12/book_sleha/data/cha_conf_hawk2.html"/>. Informazioni dettagliate sulla creazione di risorse per una macchina virtuale KVM sono descritte in <link xlink:href="http://www.linux-ha.org/wiki/VirtualDomain_%28resource_agent%29"/>.
    </para>
   </step>
   <step>
    <para>
     Sul nuovo guest VM creato, distribuire il nodo admin Ceph compresi i servizi aggiuntivi necessari. Seguire la procedura pertinente in <xref linkend="ceph-install-stack"/>. Contemporaneamente, distribuire i restanti nodi del cluster Ceph sui server del cluster non HA.
    </para>
   </step>
  </procedure>
 </sect1>
</chapter>
