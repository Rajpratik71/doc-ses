<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_cephfs.xml" version="5.0" xml:id="cha-ceph-cephfs">


 <title>叢集檔案系統</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>編輯</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  本章介紹通常應在完成叢集設定與輸出 CephFS 後執行的管理任務。如需有關設定 CephFS 的詳細資訊，請參閱<xref linkend="cha-ceph-as-cephfs"/>。
 </para>
 <sect1 xml:id="ceph-cephfs-cephfs-mount">
  <title>掛接 CephFS</title>

  <para>
   建立檔案系統後，如果 MDS 處於使用中狀態，您便可以從用戶端主機掛接檔案系統。
  </para>

  <sect2 xml:id="cephfs-client-preparation">
   <title>用戶端準備</title>
   <para>
    如果用戶端主機執行的是 SUSE Linux Enterprise 12 SP2 或 SP3，您可以跳過本節，因為系統無需額外設定即可掛接 CephFS。
   </para>
   <para>
    如果用戶端主機執行的是 SUSE Linux Enterprise 12 SP1，您需要套用所有最新的修補程式，才能掛接 CephFS。
   </para>
   <para>
    無論是哪一種情況，SUSE Linux Enterprise 中都包含了掛接 CephFS 需要的所有項目。不需要 SUSE Enterprise Storage 產品。
   </para>
   <para>
    為了支援完整的 <command>mount</command> 語法，在嘗試掛接 CephFS 之前，應該先安裝
    <package>ceph-common</package> 套件 (於 SUSE Linux Enterprise 中隨附)。
   </para>
  </sect2>

  <sect2 xml:id="Creating-Secret-File">
   <title>建立機密檔案</title>
   <para>
    Ceph 叢集執行時預設會開啟驗證功能。您應該建立一個可用於儲存您的機密金鑰 (不是金鑰圈自身) 的檔案。請執行以下操作，以獲取特定使用者的機密金鑰並建立該檔案︰
   </para>
   <procedure>
    <title>建立機密金鑰</title>
    <step>
     <para>
      在金鑰圈檔案中檢視特定使用者的金鑰︰
     </para>
<screen>cat /etc/ceph/ceph.client.admin.keyring</screen>
    </step>
    <step>
     <para>
      複製要使用所掛接 Ceph FS 檔案系統的使用者的金鑰。金鑰的格式通常類似下方所示︰
     </para>
<screen>AQCj2YpRiAe6CxAA7/ETt7Hcl9IyxyYciVs47w==</screen>
    </step>
    <step>
     <para>
      為使用者 <emphasis>admin</emphasis> 建立一個檔案名稱包含使用者名稱的檔案，例如 <filename>/etc/ceph/admin.secret</filename>。
     </para>
    </step>
    <step>
     <para>
      將金鑰值貼至上一步中建立的檔案中。
     </para>
    </step>
    <step>
     <para>
      設定對該檔案的適當存取權限。該使用者應該是唯一有權讀取該檔案的使用者，其他人不能有任何存取權限。
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ceph-cephfs-krnldrv">
   <title>掛接 CephFS</title>
   <para>
    可以使用 <command>mount</command> 指令掛接 CephFS。您需要指定監控程式的主機名稱或 IP 位址。由於 SUSE Enterprise Storage 中預設會啟用 <systemitem>cephx</systemitem> 驗證，因此，您還需要指定一個使用者名稱及其相關的機密︰
   </para>
<screen>sudo mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secret=AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</screen>
   <para>
    由於上一個指令會保留在外圍程序歷程中，因此更安全的做法是從檔案讀取機密︰
   </para>
<screen>sudo mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    請注意，機密檔案應該僅包含實際的金鑰圈機密。因此，在本範例中，該檔案僅包含下行︰
   </para>
<screen>AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</screen>
   <tip>
    <title>指定多個監控程式</title>
    <para>
     最好能在 <command>mount</command> 指令行中指定多個監控程式並以逗號分隔，以避免某個監控程式在掛接時剛好發生停機的情況。每個監控程式的位址採用<literal>主機[:連接埠]</literal> 格式。如果未指定連接埠，預設會使用連接埠 6789。
    </para>
   </tip>
   <para>
    在本地主機上建立掛接點︰
   </para>
<screen>sudo mkdir /mnt/cephfs</screen>
   <para>
    掛接 CephFS︰
   </para>
<screen>sudo mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    如果要掛接檔案系統的某個子集，可以指定子目錄 <filename>subdir</filename>︰
   </para>
<screen>sudo mount -t ceph ceph_mon1:6789:/subdir /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    可在 <command>mount</command> 指令中指定多個監控程式主機︰
   </para>
<screen>sudo mount -t ceph ceph_mon1,ceph_mon2,ceph_mon3:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <important>
    <title>對根目錄的讀取存取權</title>
    <para>
     如果使用實施了路徑限制的用戶端，則 MDS 功能需要包含對根目錄的讀取存取權。例如，金鑰圈的格式可能如下所示︰
    </para>
<screen>client.bar
 key: supersecretkey
 caps: [mds] allow rw path=/barjail, allow r path=/
 caps: [mon] allow r
 caps: [osd] allow rwx</screen>
    <para>
     <literal>allow r path=/</literal> 部分表示路徑受限的用戶端能夠查看根磁碟區，但無法寫入資料至其中。在要求完全隔離的使用情況下，這可能會造成問題。
    </para>
   </important>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs-unmount">
  <title>卸載 CephFS</title>

  <para>
   若要卸載 CephFS，請使用 <command>umount</command> 指令︰
  </para>

<screen>sudo umount /mnt/cephfs</screen>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs-fstab">
  <title><filename>/etc/fstab</filename> 中的 CephFS</title>

  <para>
   若要在用戶端啟動時自動掛接 CephFS，請在其檔案系統表 <filename>/etc/fstab</filename> 中插入相應的行︰
  </para>

<screen>mon1:6790,mon2:/subdir /mnt/cephfs ceph name=admin,secretfile=/etc/ceph/secret.key,noatime,_netdev 0 2</screen>
 </sect1>
 <sect1 xml:id="ceph-cephfs-activeactive">
  <title>多個使用中 MDS 精靈 (主動/主動 MDS)</title>

  <para>
   依預設，CephFS 是針對單個使用中 MDS 精靈設定的。若要調整大規模系統的中繼資料效能，您可以啟用多個使用中 MDS 精靈，以便互相分擔中繼資料工作負載。
  </para>

  <sect2>
   <title>何時使用主動/主動 MDS</title>
   <para>
    如果依預設設定使用單個 MDS 時中繼資料效能出現瓶頸，可考慮使用多個使用中 MDS 精靈。
   </para>
   <para>
    增加精靈數量並不會提高所有工作負載類型的效能。例如，增加 MDS 精靈的數量不會讓單個用戶端上執行的單個應用程式受益，除非該應用程式在同時執行大量中繼資料操作。
   </para>
   <para>
    一般而言，能夠因大量使用中 MDS 精靈受益的工作負載是使用許多用戶端的工作負載，也許是在許多獨立目錄中運作的工作負載。
   </para>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-increase">
   <title>增加 MDS 使用中叢集的大小</title>
   <para>
    每個 CephFS 檔案系統都有一項 <option>max_mds</option> 設定，用於控制將要建立的階層數。僅當某個備用精靈可用於承擔新階層時，檔案系統中的實際階層數才會增加。例如，如果只有執行一個 MDS 精靈，而 <option>max_mds</option> 設定為兩個，則將不會建立另一個階層。
   </para>
   <para>
    在下面的範例中，我們將 <option>max_mds</option> 選項設定為 2，以便在保留預設階層的情況下再建立一個新階層。若要查看變更，請在設定 <option>max_mds</option> 之前和之後執行 <command>ceph status</command>，然後觀察包含 <literal>fsmap</literal> 的行︰
   </para>
<screen><prompt>root@master # </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby
    [...]
<prompt>root@master # </prompt><command>ceph</command> mds set max_mds 2
<prompt>root@master # </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/2 up  {0=node2=up:active,1=node1=up:active}
    [...]</screen>
   <para>
    新建立的階層 (1) 會經歷「正在建立」狀態，然後進入「使用中」狀態。
   </para>
   <important>
    <title>待命精靈</title>
    <para>
     即使具有多個使用中 MDS 精靈，當任何在執行使用中精靈的伺服器發生故障時，高可用性系統也仍會要求待命精靈接管工作。
    </para>
    <para>
     因此，高可用性系統的 <option>max_mds</option> 合理最大值比系統中的 MDS 伺服器總數小 1。若要在發生多次伺服器故障時保持可用性，可增加系統中待命精靈的數量，使之與不會導致失去可用性的伺服器故障數相符。
    </para>
   </important>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-decrease">
   <title>減小階層數</title>
   <para>
    所有階層 (包括要移除的階層) 首先必須是使用中的。這表示至少需要有 <option>max_mds</option> 個 MDS 精靈可用。
   </para>
   <para>
    首先，將 <option>max_mds</option> 設為一個較小的數字。例如，我們重新使用單個使用中 MDS︰
   </para>
<screen><prompt>root@master # </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/2 up  {0=node2=up:active,1=node1=up:active}
    [...]
<prompt>root@master # </prompt><command>ceph</command> mds set max_mds 1
<prompt>root@master # </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby
    [...]</screen>
   <para>
    請注意，我們仍有兩個使用中 MDS。即使減小 <option>max_mds</option>，階層也仍會存在，因為 <option>max_mds</option> 只會限制新階層的建立。
   </para>
   <para>
    接下來，使用 <command>ceph mds deactivate <replaceable>rank</replaceable></command> 指令移除不需要的階層︰
   </para>
<screen><prompt>root@master # </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/1 up  {0=node2=up:active,1=node1=up:active}
<prompt>root@master # </prompt><command>ceph</command> mds deactivate 1
telling mds.1:1 192.168.58.101:6805/2799214375 to deactivate

<prompt>root@master # </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/1 up  {0=node2=up:active,1=node1=up:stopping}

<prompt>root@master # </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby</screen>
   <para>
    已停用的階層首先會進入「正在停止」狀態並會保持一段時間，期間它會將所分擔的中繼資料負載轉移給其餘使用中精靈。此階段可能需要數秒到數分鐘時間。如果 MDS 看起來像停在「正在停止」狀態，則應該調查原因，以判斷是否有可能存在錯誤。
   </para>
   <para>
    如果一個 MDS 精靈在「正在停止」狀態下當機或終止，待命精靈會接管工作，階層將恢復到「使用中」狀態。當此精靈重新啟動後，您可以嘗試再次將它停用。
   </para>
   <para>
    精靈結束「正在停止」狀態後，將再次啟動，並重新變為待命精靈。
   </para>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-pinning">
   <title>手動將目錄樹關聯到層級</title>
   <para>
    在多個使用中中繼資料伺服器組態中，將會執行一個平衡器，用於在叢集中均衡分配中繼資料負載。這種模式通常足以符合大多數使用者的需求，但有時，使用者需要使用中繼資料到特定階層的明確對應來覆寫動態平衡器。這樣，管理員或使用者便可以在整個叢集上均衡地分攤應用程式負載，或限制使用者的中繼資料要求對整個叢集的影響。
   </para>
   <para>
    針對此目的提供的機制稱為「輸出關聯」。它是目錄的延伸屬性。此延伸屬性名為 <literal>ceph.dir.pin</literal>。使用者可以使用標準指令來設定此屬性︰
   </para>
<screen>setfattr -n ceph.dir.pin -v 2 <replaceable>/path/to/dir</replaceable></screen>
   <para>
    延伸屬性的值 (<option>-v</option>) 是要將目錄子樹指定到的階層。預設值 -1 表示不關聯該目錄。
   </para>
   <para>
    目錄輸出關聯從設定了輸出關聯的最近的父級繼承。因此，對某個目錄設定輸出關聯會影響該目錄的所有子項。但是，可以透過設定子目錄輸出關聯來覆寫父級的關聯。例如︰
   </para>
<screen>mkdir -p a/b                      # "a" and "a/b" start with no export pin set.
setfattr -n ceph.dir.pin -v 1 a/  # "a" and "b" are now pinned to rank 1.
setfattr -n ceph.dir.pin -v 0 a/b # "a/b" is now pinned to rank 0
                                  # and "a/" and the rest of its children
                                  # are still pinned to rank 1.</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-failover">
  <title>管理容錯移轉</title>

  <para>
   如果 MDS 精靈停止與監控程式通訊，監控程式會等待 <option>mds_beacon_grace</option> 秒 (預設為 15 秒)，然後將精靈標示為 <emphasis>laggy</emphasis>。可以設定一或多個「待命」精靈，用於在 MDS 精靈容錯移轉期間接管工作。
  </para>

  <sect2 xml:id="ceph-cephfs-failover-standby">
   <title>設定待命精靈</title>
   <para>
    有多項組態設定可控制精靈處於待命狀態時的行為。您可以在執行 MDS 精靈的主機上的 <filename>ceph.conf</filename> 中指定這些設定。精靈在啟動時會載入這些設定，然後將其傳送至監控程式。
   </para>
   <para>
    依預設，如果不使用其中的任何設定，則不具備階層的所有 MDS 精靈將會做為任一階層的「待命」精靈。
   </para>
   <para>
    將待命精靈與特定名稱或階層加以關聯的設定不保證該精靈只用於該階層。具體而言，當有多個待命精靈可用時，將使用關聯的待命精靈。如果某個階層發生故障，而此時有某個待命精靈可用，則即使該精靈與其他某個階層或指定的精靈相關聯，也會使用該精靈。
   </para>
   <variablelist>
    <varlistentry>
     <term>mds_standby_replay</term>
     <listitem>
      <para>
       如果設定為 true，則待命精靈將持續讀取某個已啟動階層的中繼資料記錄。這就為此階層提供了一個熱中繼資料快取，當為該階層提供服務的精靈發生故障時，此記錄可加快容錯移轉程序的速度。
      </para>
      <para>
       一個已啟動的階層只能指定一個待命重放精靈。如果將兩個精靈都設定為待命重放，則其中任意一個會贏得控制權，另一個將成為正常的非重放待命精靈。
      </para>
      <para>
       當某個精靈進入待命重放狀態時，它只會做為所跟隨階層的待命精靈。如果另一個階層發生故障，系統不會使用此待命重放精靈來取代前者，即使沒有其他可用的待命精靈也是如此。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds_standby_for_name</term>
     <listitem>
      <para>
       如果指定此設定，則僅當最後一個具備故障階層的精靈與此名稱相符時，待命精靈才會接管該故障階層。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds_standby_for_rank</term>
     <listitem>
      <para>
       如果指定此設定，待命精靈只會接管指定的階層。如果另外的階層發生故障，將不會使用此精靈來取代此階層。
      </para>
      <para>
       與 <option>mds_standby_for_fscid</option> 結合使用時，可以指定在使用多個檔案系統時，具體針對哪個檔案系統的階層。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds_standby_for_fscid</term>
     <listitem>
      <para>
       如果設定了 <option>mds_standby_for_rank</option>，則 mds_standby_for_fscid 只是一個用於指出所指檔案系統階層的修飾詞。
      </para>
      <para>
       如果未設定 <option>mds_standby_for_rank</option>，則設定 FSCID 會導致此精靈以指定 FSCID 中的任何階層為目標。如果您希望只在特定的檔案系統中將某個精靈用於任何階層，可使用此設定。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mon_force_standby_active</term>
     <listitem>
      <para>
       在監控程式主機上使用此設定。其預設值為 true。
      </para>
      <para>
       如果值為 false，則 <option>standby_replay 設定為 true</option> 的精靈只有在其已設定要跟隨的階層/名稱發生故障時，才會變為使用中精靈。另一方面，如果此設定為 true，則可將其他某個階層指定給 <option>standby_replay 設定為 true</option> 的精靈。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph-cephfs-failover-examples">
   <title>範例</title>
   <para>
    下面顯示了多個範例 <filename>ceph.conf</filename> 組態。您可將包含所有精靈的組態的 <filename>ceph.conf</filename> 複製到所有伺服器，或者在每部伺服器上建立一個不同的檔案，並在其中包含該伺服器的精靈組態。
   </para>
   <sect3>
    <title>簡單配對</title>
    <para>
     「a」和「b」兩個 MDS 精靈充當一對。其中，目前未指定階層的精靈將是另一個精靈的待命重放跟隨者。
    </para>
<screen>[mds.a]
mds standby replay = true
mds standby for rank = 0

[mds.b]
mds standby replay = true
mds standby for rank = 0</screen>
   </sect3>

  </sect2>
 </sect1>
</chapter>
