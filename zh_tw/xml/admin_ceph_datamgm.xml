<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_datamgm.xml" version="5.0" xml:id="cha-storage-datamgm">
 <title>儲存的資料管理</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>編輯</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  CRUSH 演算法透過運算資料儲存位置來確定如何儲存和擷取資料。利用 CRUSH，Ceph 用戶端無需透過集中式伺服器或仲介，即可直接與 OSD 通訊。借助演算法確定的資料儲存和擷取方法，Ceph 可避免單一故障點、效能瓶頸和延展性實體限制。
 </para>
 <para>
  CRUSH 需要獲取叢集的地圖，它使用 CRUSH 地圖以似隨機方式在 OSD 中儲存和擷取資料，並以一致的方式在整個叢集中分發資料。
 </para>
 <para>
  CRUSH 地圖包含一個 OSD 清單、一個用於將裝置聚合到實體位置的「桶」清單，以及一個指示 CRUSH 應如何在 Ceph 叢集的池中複製資料的規則清單。透過反映安裝的基礎實體組織，CRUSH 可對相關裝置故障的潛在根源塑模，從而解決故障的根源。一般的根源包括實體距離、共用電源和共用網路。透過將這些資訊編碼到叢集地圖中，CRUSH 放置規則可將物件複本分隔在不同的故障網域中，同時維持所需的分發方式。例如，為了消除可能的並行故障，可能需要確定資料複本位於使用不同機架、機櫃、電源、控制器和/或實體位置的裝置上。
 </para>
 <para>
  部署 Ceph 叢集後，將會產生預設的 CRUSH 地圖。這種模式適合 Ceph 沙箱環境。但是，在部署大規模的資料叢集時，強烈建議您考慮建立自訂 CRUSH 地圖，因為這樣做有助於管理 Ceph 叢集、提高效能並確保資料安全。
 </para>
 <para>
  例如，如果某個 OSD 停機，而您需要使用現場支援或更換硬體，則 CRUSH 地圖可協助您定位到發生 OSD 故障的主機所在的實體資料中心、機房、裝置排和機櫃。
 </para>
 <para>
  同樣，CRUSH 可以協助您更快地確定故障。例如，如果特定機櫃中的所有 OSD 同時停機，故障可能是由某個網路交換器或者機櫃或網路交換器的電源所致，而不是發生在 OSD 自身上。
 </para>
 <para>
  當與故障主機關聯的放置群組處於降級狀態時，自訂 CRUSH 地圖還可協助您確定 Ceph 儲存資料備援副本的實體位置。
 </para>
 <para>
  CRUSH 地圖包括三個主要部分。
 </para>
 <itemizedlist mark="bullet" spacing="normal">
  <listitem>
   <para>
    <xref linkend="datamgm-devices" xrefstyle="select: title"/>由任何物件儲存裝置 (即與 <systemitem>ceph-osd</systemitem> 精靈對應的硬碟) 組成。
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="datamgm-buckets" xrefstyle="select: title"/>由儲存位置 (例如裝置排、機櫃、主機等) 及為其指定的權數的階層聚合組成。
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="datamgm-rules" xrefstyle="select: title"/>由桶選取方式組成。
   </para>
  </listitem>
 </itemizedlist>
 <sect1 xml:id="datamgm-devices">
  <title>裝置</title>

  <para>
   為了將放置群組對應到 OSD，CRUSH 地圖需要 OSD 裝置 (OSD 精靈的名稱) 的清單。裝置清單顯示在 CRUSH 地圖的最前面。
  </para>

<screen>#devices
device <replaceable>num</replaceable> <replaceable>osd.name</replaceable></screen>

  <para>
   例如︰
  </para>

<screen>#devices
device 0 osd.0
device 1 osd.1
device 2 osd.2
device 3 osd.3</screen>

  <para>
   一般而言，一個 OSD 精靈對應到一個磁碟。
  </para>
 </sect1>
 <sect1 xml:id="datamgm-buckets">
  <title>桶</title>

  <para>
   CRUSH 地圖包含 OSD 的清單，可將這些 OSD 組織成「桶」，以便將裝置聚合到實體位置。
  </para>

  <informaltable frame="none">
   <tgroup cols="3">
    <colspec colwidth="10*"/>
    <colspec colwidth="30*"/>
    <colspec colwidth="70*"/>
    <tbody>
     <row>
      <entry>
       <para>
        0
       </para>
      </entry>
      <entry>
       <para>
        OSD
       </para>
      </entry>
      <entry>
       <para>
        OSD 精靈 (osd.1、osd.2 等)。
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        1
       </para>
      </entry>
      <entry>
       <para>
        主機
       </para>
      </entry>
      <entry>
       <para>
        包含一或多個 OSD 的主機名稱。
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        2
       </para>
      </entry>
      <entry>
       <para>
        機箱
       </para>
      </entry>
      <entry>
       <para>
        組成機櫃的機箱。
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        3
       </para>
      </entry>
      <entry>
       <para>
        機櫃
       </para>
      </entry>
      <entry>
       <para>
        電腦機櫃。預設值為 <literal>unknownrack</literal>。
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        4
       </para>
      </entry>
      <entry>
       <para>
        裝置排
       </para>
      </entry>
      <entry>
       <para>
        由一系列機櫃組成的裝置排。
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        5
       </para>
      </entry>
      <entry>
       <para>
        Pdu
       </para>
      </entry>
      <entry>
       <para>
        電源分配單位。
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        6
       </para>
      </entry>
      <entry>
       <para>
        Pod
       </para>
      </entry>
      <entry>
       <para/>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        7
       </para>
      </entry>
      <entry>
       <para>
        機房
       </para>
      </entry>
      <entry>
       <para>
        包含主機機櫃和主機排的機房。
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        8
       </para>
      </entry>
      <entry>
       <para>
        資料中心
       </para>
      </entry>
      <entry>
       <para>
        包含機房的實體資料中心。
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        9
       </para>
      </entry>
      <entry>
       <para>
        區域
       </para>
      </entry>
      <entry>
       <para/>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        10
       </para>
      </entry>
      <entry>
       <para>
        根
       </para>
      </entry>
      <entry>
       <para/>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </informaltable>

  <tip>
   <para>
    您可以移除這些類型，並建立自己的桶類型。
   </para>
  </tip>

  <para>
   Ceph 的部署工具可產生 CRUSH 地圖，其中包含每個主機的桶，以及名為「default」的池 (可用於預設的 <literal>rbd</literal> 池)。剩餘的桶類型提供了一種儲存有關節點/桶的實體位置資訊的方法，當 OSD、主機或網路硬體無法正常運作，並且管理員需要存取實體硬體時，這種方法可顯著簡化叢集管理工作。
  </para>

  <para>
   桶具有類型、唯一的名稱 (字串)、以負整數表示的唯一的 ID、相對於其項目總容量/功能的權數、桶演算法 (預設為 <literal>straw</literal>) 和雜湊 (預設為 <literal>0</literal>，反映 CRUSH 雜湊 <literal>rjenkins1</literal>)。一個桶可以包含一或多個項目。項目可由其他桶或 OSD 組成。項目可能會有一個權數來反映該項目的相對權數。
  </para>

<screen>[bucket-type] [bucket-name] {
  id [a unique negative numeric ID]
  weight [the relative capacity/capability of the item(s)]
  alg [the bucket type: uniform | list | tree | straw ]
  hash [the hash type: 0 by default]
  item [item-name] weight [weight]
}</screen>

  <para>
   下面的範例說明如何使用桶來聚合池，以及諸如資料中心、機房、機櫃和裝置排的實體位置。
  </para>

<screen>host ceph-osd-server-1 {
        id -17
        alg straw
        hash 0
        item osd.0 weight 1.00
        item osd.1 weight 1.00
}

row rack-1-row-1 {
        id -16
        alg straw
        hash 0
        item ceph-osd-server-1 weight 2.00
}

rack rack-3 {
        id -15
        alg straw
        hash 0
        item rack-3-row-1 weight 2.00
        item rack-3-row-2 weight 2.00
        item rack-3-row-3 weight 2.00
        item rack-3-row-4 weight 2.00
        item rack-3-row-5 weight 2.00
}

rack rack-2 {
        id -14
        alg straw
        hash 0
        item rack-2-row-1 weight 2.00
        item rack-2-row-2 weight 2.00
        item rack-2-row-3 weight 2.00
        item rack-2-row-4 weight 2.00
        item rack-2-row-5 weight 2.00
}

rack rack-1 {
        id -13
        alg straw
        hash 0
        item rack-1-row-1 weight 2.00
        item rack-1-row-2 weight 2.00
        item rack-1-row-3 weight 2.00
        item rack-1-row-4 weight 2.00
        item rack-1-row-5 weight 2.00
}

room server-room-1 {
        id -12
        alg straw
        hash 0
        item rack-1 weight 10.00
        item rack-2 weight 10.00
        item rack-3 weight 10.00
}

datacenter dc-1 {
        id -11
        alg straw
        hash 0
        item server-room-1 weight 30.00
        item server-room-2 weight 30.00
}

pool data {
        id -10
        alg straw
        hash 0
        item dc-1 weight 60.00
        item dc-2 weight 60.00
}</screen>
 </sect1>
 <sect1 xml:id="datamgm-rules">
  <title>規則集</title>

  <para>
   CRUSH 地圖支援「CRUSH 規則」概念，這些規則確定池的資料放置。對於大型叢集，您可能會建立許多池，其中每個池各自可能具有自己的 CRUSH 規則集和規則。預設的 CRUSH 地圖對每個池使用一條規則，並為每個預設池指定一個規則集。
  </para>

  <note>
   <para>
    大多數情況下，您無需修改預設規則。建立新池時，該池的預設規則集為 0。
   </para>
  </note>

  <para>
   規則採用以下格式︰
  </para>

<screen>rule <replaceable>rulename</replaceable> {

        ruleset <replaceable>ruleset</replaceable>
        type <replaceable>type</replaceable>
        min_size <replaceable>min-size</replaceable>
        max_size <replaceable>max-size</replaceable>
        step <replaceable>step</replaceable>

}</screen>

  <variablelist>
   <varlistentry>
    <term>ruleset</term>
    <listitem>
     <para>
      一個整數。將規則分類，使其屬於一個規則集。透過在池中設定規則集來啟用。必須指定此選項。預設值為 <literal>0</literal>。
     </para>
     <important>
      <para>
       您需要從預設值 0 開始連續遞增規則集編號，否則相關的監控程式可能會當機。
      </para>
     </important>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>type</term>
    <listitem>
     <para>
      一個字串。描述硬碟 (replicated) 或 RAID 的規則。必須指定此選項。預設值為 <literal>replicated</literal>。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>min_size</term>
    <listitem>
     <para>
      一個整數。如果放置群組建立的複本數小於此數字，CRUSH 將不選取此規則。必須指定此選項。預設值為 <literal>2</literal>。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>max_size</term>
    <listitem>
     <para>
      一個整數。如果放置群組建立的複本數大於此數字，CRUSH 將不選取此規則。必須指定此選項。預設值為 <literal>10</literal>。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>step take <replaceable>bucket</replaceable>
    </term>
    <listitem>
     <para>
      採用某個桶名稱，並開始在樹中向下逐一查看。必須指定此選項。如需在樹中逐一查看的說明，請參閱<xref linkend="datamgm-rules-step-iterate"/>。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>step <replaceable>target</replaceable><replaceable>mode</replaceable><replaceable>num</replaceable> type <replaceable>bucket-type</replaceable>
    </term>
    <listitem>
     <para>
      <replaceable>target</replaceable> 可以是 <literal>choose</literal> 或 <literal>chooseleaf</literal>。如果設定為 <literal>choose</literal>，則會選取許多桶。<literal>chooseleaf</literal> 直接從桶集的每個桶的子樹中選取 OSD (葉節點)。
     </para>
     <para>
      <replaceable>mode</replaceable> 可以是 <literal>firstn</literal> 或 <literal>indep</literal>。請參閱<xref linkend="datamgm-rules-step-mode"/>。
     </para>
     <para>
      選取給定類型的桶的數量。其中，N 是可用選項的數量，如果 <replaceable>num</replaceable> &gt; 0 且 &lt; N，則選擇該數量的桶；如果 <replaceable>num</replaceable> &lt; 0，則表示 N - <replaceable>數量</replaceable>；如果 <replaceable>num</replaceable> = 0，則選擇 N 個桶 (全部可用)。跟在 <literal>step take</literal> 或 <literal>step choose</literal> 後使用。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>step emit</term>
    <listitem>
     <para>
      輸出目前的值並清空堆疊。一般在規則的結尾使用，但也可在同一規則中用來構成不同的樹。跟在 <literal>step choose</literal> 後使用。
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <important>
   <para>
    若要將一或多個具有共同規則集編號的規則構成一個池，請將規則集編號設定為該池。
   </para>
  </important>

  <sect2 xml:id="datamgm-rules-step-iterate">
   <title>在節點樹中逐一查看</title>
   <para>
    可採用節點樹的形式來檢視使用桶定義的結構。在此樹中，桶是節點，OSD 是葉。
   </para>
   <para>
    CRUSH 地圖中的規則定義如何從此樹中選取 OSD。規則從某個節點開始，然後在樹中向下逐一查看，以傳回一組 OSD。無法定義需要選取哪個分支。CRUSH 演算法可保證 OSD 集能夠符合複製要求並均衡分發資料。
   </para>
   <para>
    使用 <literal>step take </literal> <replaceable>bucket</replaceable> 時，在節點樹中從給定的桶 (而不是桶類型) 開始逐一查看。如果要傳回樹中所有分支上的 OSD，該桶必須是根桶。否則，以下步驟只會在子樹中逐一查看。
   </para>
   <para>
    完成 <literal>step take</literal> 後，接下來會執行規則定義中的一或多個 <literal>step choose</literal> 項目。每個 <literal>step choose</literal> 會從前面選定的上層節點中選擇指定數量的節點 (或分支)。
   </para>
   <para>
    最後，使用 <literal>step emit</literal> 傳回選定的 OSD。
   </para>
   <para>
    <literal>step chooseleaf</literal> 是一個便捷函數，可直接從給定桶的分支中選取 OSD。
   </para>
   <para>
    <xref linkend="datamgm-rules-step-iterate-figure"/>中提供了展示如何使用 <literal>step</literal> 在樹中逐一查看的範例。在下面的規則定義中，橙色箭頭和數字與 <literal>example1a</literal> 和 <literal>example1b</literal> 對應，藍色箭頭和數字與 <literal>example2</literal> 對應。
   </para>
   <figure xml:id="datamgm-rules-step-iterate-figure">
    <title>範例樹</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="crush-step.svg" width="100%" format="SVG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="crush-step.png" width="100%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
<screen># orange arrows
rule example1a {
        ruleset 0
        type replicated
        min_size 2
        max_size 10
        # orange (1)
        step take rack1
        # orange (2)
        step choose firstn 0 host
        # orange (3)
        step choose firstn 1 osd
        step emit
}

rule example1b {
        ruleset 0
        type replicated
        min_size 2
        max_size 10
        # orange (1)
        step take rack1
        # orange (2) + (3)
        step chooseleaf firstn 0 host
        step emit
}

# blue arrows
rule example2 {
        ruleset 0
        type replicated
        min_size 2
        max_size 10
        # blue (1)
        step take room1
        # blue (2)
        step chooseleaf firstn 0 rack
        step emit
}</screen>
  </sect2>

  <sect2 xml:id="datamgm-rules-step-mode">
   <title>firstn 和 indep</title>
   <para>
    CRUSH 規則定義有故障節點或 OSD 的取代項 (請參閱<xref linkend="datamgm-rules"/>)。關鍵字 <literal>step</literal> 要求使用 <literal>firstn</literal> 或 <literal>indep</literal> 參數。<xref linkend="datamgm-rules-step-mode-indep-figure"/>提供了範例。
   </para>
   <para>
    <literal>firstn</literal> 將取代節點新增至使用中節點清單的結尾。如果某個節點發生故障，其後的正常節點會移位到左側，以填滿有故障節點留下的空缺。這是<emphasis>副本池</emphasis>的預設方法，也是需要採取的方法，因為次要節點已包含所有資料，因此可立即接管主要節點的職責。
   </para>
   <para>
    <literal>indep</literal> 為每個使用中節點選取固定的取代節點。取代有故障節點不會變更剩餘節點的順序。這對於<emphasis>糾刪碼池</emphasis>而言是所需的行為。在糾刪碼池中，節點上儲存的資料取決於在選取節點時它所在的位置。如果節點的順序發生變化，受影響節點上的所有資料都需要重新放置。
   </para>
   <note>
    <title>糾刪池</title>
    <para>
     確定針對每個<emphasis>糾刪碼池</emphasis>設定了使用 <literal>indep</literal> 的規則。
    </para>
   </note>
   <figure xml:id="datamgm-rules-step-mode-indep-figure">
    <title>節點取代方法</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="crush-firstn-indep.svg" width="100%" format="SVG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="crush-firstn-indep.png" width="100%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>
 </sect1>
 <sect1 xml:id="op-crush">
  <title>CRUSH 地圖操作</title>

  <para>
   本節介紹基本的 CRUSH 地圖操作方法，例如編輯 CRUSH 地圖、變更 CRUSH 地圖參數，以及新增/移動/移除 OSD。
  </para>

  <sect2>
   <title>編輯 CRUSH 地圖</title>
   <para>
    若要編輯現有的 CRUSH 地圖，請執行以下操作︰
   </para>
   <procedure>
    <step>
     <para>
      獲取 CRUSH 地圖。要獲取叢集的 CRUSH 地圖，請執行以下指令︰
     </para>
<screen><prompt>root # </prompt>ceph osd getcrushmap -o <replaceable>compiled-crushmap-filename</replaceable></screen>
     <para>
      Ceph 會將編譯的 CRUSH 地圖輸出 (<option>-o</option>) 至您指定名稱的檔案。由於該 CRUSH 地圖採用編譯格式，您必須先將其反編譯，才能對其進行編輯。
     </para>
    </step>
    <step>
     <para>
      反編譯 CRUSH 地圖。若要反編譯 CRUSH 地圖，請執行以下指令︰
     </para>
<screen><prompt>cephadm &gt; </prompt>crushtool -d <replaceable>compiled-crushmap-filename</replaceable> \
 -o <replaceable>decompiled-crushmap-filename</replaceable></screen>
     <para>
      Ceph 將對已編譯的 CRUSH 地圖進行反編譯 (<option>-d</option>)，並將其輸出 (<option>-o</option>) 至您指定名稱的檔案。
     </para>
    </step>
    <step>
     <para>
      至少編輯「裝置」、「桶」和「規則」中的其中一個參數。
     </para>
    </step>
    <step>
     <para>
      編譯 CRUSH 地圖。若要編譯 CRUSH 地圖，請執行以下指令︰
     </para>
<screen><prompt>cephadm &gt; </prompt>crushtool -c <replaceable>decompiled-crush-map-filename</replaceable> \
 -o <replaceable>compiled-crush-map-filename</replaceable></screen>
     <para>
      Ceph 會將編譯的 CRUSH 地圖儲存至您指定名稱的檔案。
     </para>
    </step>
    <step>
     <para>
      設定 CRUSH 地圖。若要設定叢集的 CRUSH 地圖，請執行以下指令︰
     </para>
<screen><prompt>root # </prompt>ceph osd setcrushmap -i <replaceable>compiled-crushmap-filename</replaceable></screen>
     <para>
      Ceph 將輸入您所指定檔案名稱的已編譯 CRUSH 地圖，做為叢集的 CRUSH 地圖。
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="op-crush-addosd">
   <title>新增/移動 OSD</title>
   <para>
    若要在執行中叢集的 CRUSH 地圖中新增或移動 OSD，請執行以下指令︰
   </para>
<screen><prompt>root # </prompt>ceph osd crush set <replaceable>id_or_name</replaceable> <replaceable>weight</replaceable> root=<replaceable>pool-name</replaceable>
<replaceable>bucket-type</replaceable>=<replaceable>bucket-name</replaceable> ...</screen>
   <variablelist>
    <varlistentry>
     <term>ID</term>
     <listitem>
      <para>
       一個整數。OSD 的數字 ID。必須指定此選項。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>name</term>
     <listitem>
      <para>
       一個字串。OSD 的全名。必須指定此選項。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>weight</term>
     <listitem>
      <para>
       一個雙精度浮點數。OSD 的 CRUSH 權數。必須指定此選項。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pool</term>
     <listitem>
      <para>
       一個鍵值組。依預設，CRUSH 階層包含 default 池做為根。必須指定此選項。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bucket-type</term>
     <listitem>
      <para>
       多個鍵值組。您可在 CRUSH 階層中指定 OSD 的位置。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    下面的範例將 <literal>osd.0</literal> 新增至階層，或移動先前某個位置的 OSD。
   </para>
<screen><prompt>root # </prompt>ceph osd crush set osd.0 1.0 root=data datacenter=dc1 room=room1 \
row=foo rack=bar host=foo-bar-1</screen>
  </sect2>

  <sect2 xml:id="op-crush-osdweight">
   <title>調整 OSD 的 CRUSH 權數</title>
   <para>
    若要在執行中叢集的 CRUSH 地圖中調整 OSD 的 CRUSH 權數，請執行以下指令︰
   </para>
<screen><prompt>root # </prompt>ceph osd crush reweight <replaceable>name</replaceable> <replaceable>weight</replaceable></screen>
   <variablelist>
    <varlistentry>
     <term>name</term>
     <listitem>
      <para>
       一個字串。OSD 的全名。必須指定此選項。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>weight</term>
     <listitem>
      <para>
       一個雙精度浮點數。OSD 的 CRUSH 權數。必須指定此選項。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="op-crush-osdremove">
   <title>移除 OSD</title>
   <para>
    若要從執行中叢集的 CRUSH 地圖中移除 OSD，請執行以下指令︰
   </para>
<screen><prompt>root # </prompt>ceph osd crush remove <replaceable>name</replaceable></screen>
   <variablelist>
    <varlistentry>
     <term>name</term>
     <listitem>
      <para>
       一個字串。OSD 的全名。必須指定此選項。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="op-crush-movebucket">
   <title>移動桶</title>
   <para>
    若要將某個桶移到 CRUSH 地圖階層中的其他位置，請執行以下指令︰
   </para>
<screen><prompt>root # </prompt>ceph osd crush move <replaceable>bucket-name</replaceable> <replaceable>bucket-type</replaceable>=<replaceable>bucket-name</replaceable>, ...</screen>
   <variablelist>
    <varlistentry>
     <term>bucket-name</term>
     <listitem>
      <para>
       一個字串。要移動/重新放置的桶的名稱。必須指定此選項。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bucket-type</term>
     <listitem>
      <para>
       多個鍵值組。您可在 CRUSH 階層中指定桶的位置。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="scrubbing">
  <title>整理</title>

  <para>
   除了複製物件的多個副本外，Ceph 還需透過<emphasis>整理</emphasis>放置群組來確保資料完整性。Ceph 的整理類似於在物件儲存層上執行 <command>fsck</command>。對於每個放置群組，Ceph 都會產生一個包含所有物件的目錄，並比較每個主要物件及其複本，以確定不會有遺失或不相符的物件。每天的淺層整理會檢查物件大小和屬性，而每週的深層整理則會讀取資料，並使用檢查總數來確保資料完整性。
  </para>

  <para>
   整理對於維護資料完整性非常重要，但該操作可能會降低效能。您可以透過調整以下設定來增加或減少整理操作︰
  </para>

  <variablelist>
   <varlistentry>
    <term><option>osd max scrubs</option>
    </term>
    <listitem>
     <para>
      針對 Ceph OSD 同時執行的最大整理操作數量。預設值為 1。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub begin hour</option>、<option>osd scrub end hour</option>
    </term>
    <listitem>
     <para>
      依小時定義一天內可以執行整理的時間範圍 (0 至 24)。預設開始時間為 0，結束時間為 24。
     </para>
     <important>
      <para>
       如果放置群組的整理間隔超出 <option>osd scrub max interval</option> 設定的值，則無論您定義的整理時間範圍為何，都將執行整理。
      </para>
     </important>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub during recovery</option>
    </term>
    <listitem>
     <para>
      允許復原期間執行整理。若將此選項設定為「false」，則當有復原程序處於使用中狀態時，將禁止排程新的整理。已在執行的整理將繼續執行。此選項有助於降低忙碌叢集上的負載。預設值為「true」。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub thread timeout</option>
    </term>
    <listitem>
     <para>
      整理線串逾時前的最長時間 (以秒計)。預設值為 60。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub finalize thread timeout</option>
    </term>
    <listitem>
     <para>
      整理完成線串逾時前的最長時間 (以秒計)。預設值為 60*10。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub load threshold</option>
    </term>
    <listitem>
     <para>
      正常化的最大負載。當系統負載 (由 <literal>getloadavg ()</literal> 與 <literal>online cpus</literal> 數量之比定義) 高於此數字時，Ceph 將不會執行整理。預設值為 0.5。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub min interval</option>
    </term>
    <listitem>
     <para>
      當 Ceph 叢集負載較低時，整理 Ceph OSD 的最短間隔 (以秒計)。預設值為 60*60*24 (一天一次)。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub max interval</option>
    </term>
    <listitem>
     <para>
      無論叢集負載如何都整理 Ceph OSD 的最長間隔 (以秒計)。7*60*60*24 (一週一次)。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub chunk min</option>
    </term>
    <listitem>
     <para>
      單個操作期間要整理的物件儲存區塊的最小數量。整理期間，Ceph 會阻止向單個區塊寫入資料。預設值為 5。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub chunk max</option>
    </term>
    <listitem>
     <para>
      單個操作期間要整理的物件儲存區塊的最大數量。預設值為 25。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub sleep</option>
    </term>
    <listitem>
     <para>
      整理下一組區塊之前睡眠的時間。增大此值會降低整個整理操作的速度，但對用戶端操作的影響較小。預設值為 0。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd deep scrub interval</option>
    </term>
    <listitem>
     <para>
      深層整理 (完整讀取所有資料) 的間隔。<option>osd scrub load threshold</option> 選項不會影響此設定。預設值為 60*60*24*7 (一週一次)。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub interval randomize ratio</option>
    </term>
    <listitem>
     <para>
      在排程放置群組的下一次整理工作時，為 <option>osd scrub min interval</option> 值新增一個隨機延遲。該延遲為一個隨機的值，小於 <option>osd scrub min interval</option> * <option>osd scrub interval randomized ratio</option> 所得結果。因此，該預設設定實際上是將整理隨機地排程在允許的時間範圍 [1, 1.5] * <option>osd scrub min interval</option> 內執行。預設值為 0.5
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd deep scrub stride</option>
    </term>
    <listitem>
     <para>
      執行深層整理時讀取的大小。預設值為 524288 (512 kB)。
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="op-mixed-ssd-hdd">
  <title>在同一個節點上混用 SSD 和 HDD</title>

  <para>
   有時，使用者可能需要設定這樣一個 Ceph 叢集︰在每個節點上混用 SSD 和 HDD，並將一個儲存池放在速度較快的 SSD 上，將另一個儲存池放在速度較慢的 HDD 上。若要實現此目的，需要編輯 CRUSH 地圖。
  </para>

  <para>
   預設的 CRUSH 地圖採用簡單的階層，其中，預設根包含主機，而主機包含 OSD，例如︰
  </para>

<screen><prompt>root # </prompt>ceph osd tree
 ID CLASS WEIGHT   TYPE NAME      STATUS REWEIGHT PRI-AFF
 -1       83.17899 root default
 -4       23.86200     host cpach
 2   hdd  1.81898         osd.2      up  1.00000 1.00000
 3   hdd  1.81898         osd.3      up  1.00000 1.00000
 4   hdd  1.81898         osd.4      up  1.00000 1.00000
 5   hdd  1.81898         osd.5      up  1.00000 1.00000
 6   hdd  1.81898         osd.6      up  1.00000 1.00000
 7   hdd  1.81898         osd.7      up  1.00000 1.00000
 8   hdd  1.81898         osd.8      up  1.00000 1.00000
 15  hdd  1.81898         osd.15     up  1.00000 1.00000
 10  nvme 0.93100         osd.10     up  1.00000 1.00000
 0   ssd  0.93100         osd.0      up  1.00000 1.00000
 9   ssd  0.93100         osd.9      up  1.00000 1.00000 </screen>

  <para>
   這樣，就無法區分磁碟類型。若要將 OSD 分為 SSD 和 HDD，需在 CRUSH 地圖中建立另一個階層︰
  </para>

<screen><prompt>root # </prompt>ceph osd crush add-bucket ssd root</screen>

  <para>
   為 SSD 建立新根後，我們需要新增主機至該根。也就是說，需要建立新的主機項目。但是，由於同一個主機名稱不能在 CRUSH 地圖中出現多次，此處使用了虛構的主機名稱。DNS 無需解析這些虛構的主機名稱。CRUSH 不關心主機名稱是什麼，只需建立適當的階層即可。若要支援虛構的主機名稱，<emphasis>真正</emphasis>需要進行的一項變更就是必須設定
  </para>

<screen>osd crush update on start = false</screen>

  <para>
    (在 <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/global.conf</filename> 檔案中)，然後執行 DeepSea 階段 3 以分發該項變更 (如需詳細資訊，請參閱<xref linkend="ds-custom-cephconf"/>)︰
  </para>

<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
</screen>

  <para>
   否則，您移動的 OSD 稍後將重設到其在預設根中的原始位置，並且叢集不會依預期方式運作。
  </para>

  <para>
   變更該設定後，請將新的虛構主機新增至 SSD 根︰
  </para>

<screen><prompt>root # </prompt>ceph osd crush add-bucket node1-ssd host
<prompt>root # </prompt>ceph osd crush move node1-ssd root=ssd
<prompt>root # </prompt>ceph osd crush add-bucket node2-ssd host
<prompt>root # </prompt>ceph osd crush move node2-ssd root=ssd
<prompt>root # </prompt>ceph osd crush add-bucket node3-ssd host
<prompt>root # </prompt>ceph osd crush move node3-ssd root=ssd</screen>

  <para>
   最後，針對每個 SSD OSD，將 OSD 移到 SSD 根。在本範例中，我們假設 osd.0、osd.1 和 osd.2 都實體在 SSD 上代管︰
  </para>

<screen><prompt>root # </prompt>ceph osd crush add osd.0 1 root=ssd
<prompt>root # </prompt>ceph osd crush set osd.0 1 root=ssd host=node1-ssd
<prompt>root # </prompt>ceph osd crush add osd.1 1 root=ssd
<prompt>root # </prompt>ceph osd crush set osd.1 1 root=ssd host=node2-ssd
<prompt>root # </prompt>ceph osd crush add osd.2 1 root=ssd
<prompt>root # </prompt>ceph osd crush set osd.2 1 root=ssd host=node3-ssd</screen>

  <para>
   CRUSH 階層現在應類似下方所示︰
  </para>

<screen><prompt>root # </prompt>ceph osd tree
ID WEIGHT  TYPE NAME                   UP/DOWN REWEIGHT PRIMARY-AFFINITY
-5 3.00000 root ssd
-6 1.00000     host node1-ssd
 0 1.00000         osd.0                    up  1.00000          1.00000
-7 1.00000     host node2-ssd
 1 1.00000         osd.1                    up  1.00000          1.00000
-8 1.00000     host node3-ssd
 2 1.00000         osd.2                    up  1.00000          1.00000
-1 0.11096 root default
-2 0.03699     host node1
 3 0.01849         osd.3                    up  1.00000          1.00000
 6 0.01849         osd.6                    up  1.00000          1.00000
-3 0.03699     host node2
 4 0.01849         osd.4                    up  1.00000          1.00000
 7 0.01849         osd.7                    up  1.00000          1.00000
-4 0.03699     host node3
 5 0.01849         osd.5                    up  1.00000          1.00000
 8 0.01849         osd.8                    up  1.00000          1.00000</screen>

  <para>
   現在，建立一條針對 SSD 根的 CRUSH 規則︰
  </para>

<screen><prompt>root # </prompt>ceph osd crush rule create-simple ssd_replicated_ruleset ssd host</screen>

  <para>
   原始預設規則集 <option>replicated_ruleset</option> (ID 為 0) 針對的是 HDD。新規則集 <option>ssd_replicated_ruleset</option> (ID 為 1) 針對的是 SSD。
  </para>

  <para>
   所有現有池仍將使用 HDD，因為它們位於 CRUSH 地圖的預設階層中。可以建立一個僅使用 SSD 的新池︰
  </para>

<screen><prompt>root # </prompt>ceph osd pool create ssd-pool 64 64
<prompt>root # </prompt>ceph osd pool set ssd-pool crush_rule ssd_replicated_ruleset</screen>
 </sect1>
</chapter>
