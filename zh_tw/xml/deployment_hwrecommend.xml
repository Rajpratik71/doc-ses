<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_hwrecommend.xml" version="5.0" xml:id="storage-bp-hwreq">
 <title>硬體要求和建議</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:translation>yes</dm:translation>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  Ceph 的硬體要求在很大程度上取決於 IO 工作負載。在開始進行詳細規劃時，應考慮以下硬體要求和建議。
 </para>
 <para>
  一般情況下，本節所述的建議是按程序提出的。如果同一部機器上有多個程序，則需要提高 CPU、RAM、磁碟和網路要求。
 </para>
 <sect1 xml:id="deployment-osd-recommendation">
  <title>物件儲存節點</title>

  <sect2 xml:id="sysreq-osd">
   <title>最低需求</title>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      至少需要 4 個 OSD 節點，每個節點包含 8 個 OSD 磁碟。
     </para>
    </listitem>
    <listitem>
     <para>
      對於<emphasis>不</emphasis>使用 BlueStore 的 OSD 而言，在每個 OSD 儲存節點上，至少需要為每 TB 的原始 OSD 容量提供 1 GB RAM。建議為每 TB 原始 OSD 容量提供 1.5 GB RAM。在復原期間，為每 TB 原始 OSD 容量提供 2 GB RAM 可獲得最佳效能。
     </para>
     <para>
      對於<emphasis>使用</emphasis> BlueStore 的 OSD 而言，首先要計算出針對不使用 BlueStore 的 OSD 建議的 RAM 大小，然後計算 2 GB 加上每個 OSD 程序建議的 RAM 的 BlueStore 快取大小，並在這兩個結果中選擇 RAM 值較大的那一個。請注意，對於 HDD，預設的 BlueStore 快取為 1 GB；而對於 SSD 裝置，預設則為 3 GB。總之，請從下面兩個值中選擇較大的那個︰
     </para>
<screen>[1GB * OSD count * OSD size]</screen>
     <para>
      或
     </para>
<screen>[(2 + BS cache) * OSD count]</screen>
    </listitem>
    <listitem>
     <para>
      對於每個 OSD 精靈程序，至少需要為每個 OSD 提供 1.5 GHz 的邏輯 CPU 核心。建議為每個 OSD 精靈程序提供 2 GHz 核心。請注意，Ceph 為每個儲存磁碟執行一個 OSD 精靈程序；請不要將專門用做 OSD 記錄、WAL 記錄、omap 中繼資料或此三者任意組合的保留磁碟空間計算在內。
     </para>
    </listitem>
    <listitem>
     <para>
      10 Gb 乙太網路 (與多個交換器結合的兩個網路介面)。
     </para>
    </listitem>
    <listitem>
     <para>
      採用 JBOD 組態的 OSD 磁碟。
     </para>
    </listitem>
    <listitem>
     <para>
      OSD 磁碟應該專門由 SUSE Enterprise Storage 使用。
     </para>
    </listitem>
    <listitem>
     <para>
      作業系統專屬的磁碟/SSD，最好採用 RAID 1 組態。
     </para>
    </listitem>
    <listitem>
     <para>
      如果此 OSD 主機將要代管用於快取分層的一部分快取池，請至少額外配置 4 GB RAM。
     </para>
    </listitem>
    <listitem>
     <para>
      出於磁碟效能原因，OSD 節點應是未虛擬化的裸機。
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="ses-bp-mindisk">
   <title>最小磁碟大小</title>
   <para>
    需要在 OSD 上執行以下兩種類型的磁碟空間︰磁碟記錄 (針對 FileStore) 或 WAL/DB 裝置 (針對 BlueStore) 的空間以及儲存資料的主要空間。記錄/WAL/DB 的最小 (預設) 值為 6 GB。資料的最小空間為 5 GB，因為系統會自動為小於 5 GB 的分割區指定權數 0。
   </para>
   <para>
    因此，儘管 OSD 的最小磁碟空間為 11 GB，但不建議使用小於 20 GB 的磁碟，即使在測試中也是如此。
   </para>
  </sect2>

  <sect2 xml:id="rec-waldb-size">
   <title>BlueStore 的 WAL 和 DB 裝置的建議大小</title>
   <tip>
    <title>更多資訊</title>
    <para>
     如需 BlueStore 的詳細資訊，請參閱<xref linkend="about-bluestore"/>。
    </para>
   </tip>
   <para>
    下面是針對 WAL/DB 裝置大小的幾條規則。使用 DeepSea 部署 OSD 和 BlueStore 時，它會自動應用建議的規則，並向管理員告知事實。
   </para>
   <itemizedlist>
    <listitem>
     <para>
      每 TB 的 OSD 容量為 10GB 的 DB 裝置 (OSD 的 1/100)。
     </para>
    </listitem>
    <listitem>
     <para>
      WAL 裝置介於 500MB 到 2GB 之間。WAL 大小取決於資料流量和工作負載，而不是取決於 OSD 大小。如果您知道 OSD 實際上能在輸送量極高時處理小型寫入和覆寫，則 WAL 最好較大而不是較小。1GB WAL 裝置是一個較好的折衷方案，可滿足大多數部署要求。
     </para>
    </listitem>
    <listitem>
     <para>
      如果您打算將 WAL 和 DB 裝置置於同一磁碟，建議您為這兩個裝置使用一個分割區，而不是為每個裝置使用單獨的分割區。這樣，Ceph 便可以使用 DB 裝置來執行 WAL 操作。這對於磁碟空間的管理也會更有效，因為 Ceph 只會在需要時才會為 WAL 使用 DB 分割區。另一個好處是 WAL 分割區填滿的可能性很小，當該分割區未完全填滿時，將其用於 DB 操作不會浪費空間。
     </para>
     <para>
      若要與 WAL 共用 DB 裝置，請<emphasis>不要</emphasis>指定 WAL 裝置，而是僅指定 DB 裝置︰
     </para>
<screen>
bluestore_block_db_path = "/path/to/db/device"
bluestore_block_db_size = 10737418240
bluestore_block_wal_path = ""
bluestore_block_wal_size = 0
</screen>
    </listitem>
    <listitem>
     <para>
      您也可以將 WAL 置於其自己的獨立裝置上。在這種情況下，建議針對 WAL 操作使用最快的裝置。
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="ses-bp-share-ssd-journal">
   <title>使用 SSD 儲存 OSD 記錄</title>
   <para>
    固態硬碟 (SSD) 不包含移動部件。這可以減少隨機存取時間和讀取延遲，同時加快資料輸送量。由於 SSD 的每 MB 價格大大高於旋轉型硬碟，SSD 只適用於較小規模的儲存。
   </para>
   <para>
    如果將記錄儲存在 SSD 上，並將物件資料儲存在獨立的硬碟上，OSD 的效能會得到大幅提高。
   </para>
   <tip>
    <title>為多個記錄共用 SSD</title>
    <para>
     由於記錄資料佔用的空間相對較小，因此您可以將多個記錄目錄掛接至一個 SSD 磁碟。請注意，每共用一個記錄，SSD 磁碟的效能就會有所下降。不建議在同一個 SSD 磁碟中共用 6 個以上的記錄，或者在 NVMe 磁碟中共用 12 個以上的記錄。
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="maximum-count-of-disks-osd">
   <title>磁碟的最大建議數量</title>
   <para>
    您可以在一部伺服器上使用所允許的任意數量的磁碟。規劃每部伺服器的磁碟數量時，需要考慮以下幾點︰
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <emphasis>網路頻寬︰</emphasis>在一部伺服器中使用的磁碟越多，執行磁碟寫入操作時必須透過網路卡傳輸的資料就越多。
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>記憶體︰</emphasis>為獲取最佳效能，請每安裝 1 TB 的磁碟空間至少保留 2 GB RAM。
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>容錯︰</emphasis>如果整部伺服器發生故障，則伺服器包含的磁碟越多，叢集暫時丟失的 OSD 就越多。此外，為了確保複製規則的執行，需要將發生故障伺服器中的所有資料複製到叢集中的其他節點。
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sysreq-mon">
  <title>監控程式節點</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     至少需要三個 Ceph Monitor 節點。監控程式數量應永遠為奇數 (1+2n)。
    </para>
   </listitem>
   <listitem>
    <para>
     4 GB RAM。
    </para>
   </listitem>
   <listitem>
    <para>
     具有四個邏輯核心的處理器。
    </para>
   </listitem>
   <listitem>
    <para>
     強烈建議您為監控程式使用 SSD 或其他速度足夠快的儲存類型，特別是針對每個監控程式節點上的 <filename>/var/lib/ceph</filename> 路徑，因為最低核准人數可能不穩定且磁碟延遲較高。建議提供兩個採用 RAID 1 組態的磁碟來進行備援。建議為監控程式程序使用獨立的磁碟，或者至少是獨立的磁碟分割區，以防止記錄檔案增大等問題導致監控程式的可用磁碟空間不足。
    </para>
   </listitem>
   <listitem>
    <para>
     每個節點只能有一個監控程式程序。
    </para>
   </listitem>
   <listitem>
    <para>
     僅當有足夠的硬體資源可用時，才支援混用 OSD、監控程式或物件閘道節點。這意味著，對於所有服務需提高相應要求。
    </para>
   </listitem>
   <listitem>
    <para>
     與多個交換器結合的兩個網路介面。
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sysreq-rgw">
  <title>物件閘道節點</title>

  <para>
   物件閘道節點應有 6 到 8 個 CPU 核心和 32 GB RAM (建議 64 GB)。如果將其他程序併置在同一部機器上，則需要提高相應要求。
  </para>
 </sect1>
 <sect1 xml:id="sysreq-mds">
  <title>中繼資料伺服器節點</title>

  <para>
   中繼資料伺服器節點的適當大小取決於特定使用案例。一般而言，中繼資料伺服器需要處理的開啟檔案越多，所需要的 CPU 和 RAM 就越多。以下是最低要求︰
  </para>

  <itemizedlist>
   <listitem>
    <para>
     每個中繼資料伺服器精靈需要 3G RAM。
    </para>
   </listitem>
   <listitem>
    <para>
     結合網路介面。
    </para>
   </listitem>
   <listitem>
    <para>
     2.5 GHz CPU，至少具有兩個核心。
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sysreq-smaster">
  <title>Salt Master</title>

  <para>
   至少需要 4 GB RAM 和四核 CPU。這包括在 Salt Master 上執行 openATTIC 的要求。對於包含數百個節點的大型叢集，建議提供 6 GB RAM。
  </para>
 </sect1>
 <sect1 xml:id="sysreq-iscsi">
  <title>iSCSI 節點</title>

  <para>
   iSCSI 節點應有 6 到 8 個 CPU 核心和 16 GB RAM。
  </para>
 </sect1>
 <sect1 xml:id="ceph-install-ceph-deploy-network">
  <title>網路建議</title>

  <para>
   要執行 Ceph 的網路環境最好是至少包含兩個網路介面結合的組合，該組合使用 VLAN 邏輯分割成公共部分和可信的內部部分。如果可能，建議採用 802.3ad 結合模式，以提供最高的頻寬和復原能力。
  </para>

  <para>
   公共 VLAN 用於向客戶提供服務，而內部部分則用於已驗證的 Ceph 網路通訊。建議採用此模式的主要原因在於，儘管 Ceph 可提供驗證並能在建立機密金鑰後防範攻擊，但用於設定這些金鑰的訊息可能會公開傳輸，因而容易受到攻擊。
  </para>

  <tip>
   <title>透過 DHCP 設定的節點</title>
   <para>
    如果儲存節點是透過 DHCP 設定的，則預設逾時可能會不夠長，無法保證能在各個 Ceph 精靈啟動前正確設定網路。如果發生此問題，Ceph MON 和 OSD 將不會正常啟動 (執行 <command>systemctl status ceph\*</command> 會導致「無法結合」錯誤)。為避免此問題發生，建議您在儲存叢集的每個節點上，將 DHCP 用戶端逾時增加到至少 30 秒。為此，可在每個節點上變更以下設定︰
   </para>
   <para>
    在 <filename>/etc/sysconfig/network/dhcp</filename> 中，設定
   </para>
<screen>DHCLIENT_WAIT_AT_BOOT="30"</screen>
   <para>
    在 <filename>/etc/sysconfig/network/config</filename> 中，設定
   </para>
<screen>WAIT_FOR_INTERFACES="60"</screen>
  </tip>

  <sect2 xml:id="storage-bp-net-private">
   <title>將私人網路新增到執行中的叢集</title>
   <para>
    如果您在 Ceph 部署期間未指定叢集網路，則系統假設使用的是單一公用網路環境。儘管 Ceph 可在公用網路中正常運作，但如果您設定了另一個私人叢集網路，Ceph 的效能和安全性將會得到提升。若要支援兩個網路，每個 Ceph 節點上至少需有兩個網路卡。
   </para>
   <para>
    需要對每個 Ceph 節點套用以下變更。對小型叢集執行此操作的速度相對較快，但如果叢集包含數百甚至數千個節點，則此程序可能十分耗時。
   </para>
   <procedure>
    <step>
     <para>
      在每個叢集節點上停止 Ceph 相關的服務。
     </para>
     <para>
      在 <filename>/etc/ceph/ceph.conf</filename> 中新增一行以定義叢集網路，例如︰
     </para>
<screen>cluster network = 10.0.0.0/24</screen>
     <para>
      如果需要指定具體的靜態 IP 位址或覆寫 <option>cluster network</option> 設定，您可以使用選擇性的 <option>cluster addr</option> 執行此操作。
     </para>
    </step>
    <step>
     <para>
      檢查私人叢集網路在 OS 層級是否如預期般運作。
     </para>
    </step>
    <step>
     <para>
      在每個叢集節點上啟動 Ceph 相關的服務。
     </para>
<screen>sudo systemctl start ceph.target</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="storage-bp-net-subnets">
   <title>不同子網路中的監控程式節點</title>
   <para>
    如果監控程式節點位於多個子網路中，例如，位於不同的房間並由不同的交換器提供服務，則您需要相應地調整 <filename>ceph.conf</filename> 檔案。例如，如果節點的 IP 位址為 192.168.123.12、1.2.3.4 和 242.12.33.12，請將以下幾行新增到 global 區段︰
   </para>
<screen>[global]
[...]
mon host = 192.168.123.12, 1.2.3.4, 242.12.33.12
mon initial members = MON1, MON2, MON3
[...]</screen>
   <para>
    此外，如果您需要指定每個監控程式的公用位址或網路，則需要為每個監控程式新增 <literal>[mon.<replaceable>X</replaceable>]</literal> 區段︰
   </para>
<screen>[mon.MON1]
public network = 192.168.123.0/24

[mon.MON2]
public network = 1.2.3.0/24

[mon.MON3]
public network = 242.12.33.12/0</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sysreq-naming">
  <title>命名限制</title>

  <para>
   一般情況下，Ceph 不支援在組態檔案、池名稱、使用者名稱等內容中使用非 ASCII 字元。設定 Ceph 叢集時，建議您在所有 Ceph 物件/組態名稱中僅使用簡單的英數字元 (A-Z、a-z、0-9) 和最少量的標點符號 (「.」、「-」、「_」)。
  </para>
 </sect1>
 <sect1 xml:id="ses-bp-diskshare">
  <title>共用一部伺服器的 OSD 和監控程式</title>

  <para>
   儘管從技術上講可以在測試環境中的同一部伺服器上執行 Ceph OSD 和監控程式，但強烈建議您在生產環境中為每個監控程式節點使用獨立的伺服器。這樣做的主要原因在於效能 — 叢集包含的 OSD 越多，監控程式節點需要執行的 I/O 操作就越多。另外，當監控程式節點與 OSD 之間共用一部伺服器時，OSD I/O 操作便將成為監控程式節點的限制因素。
  </para>

  <para>
   另一個考慮因素是，是否要在 OSD、監控程式節點與伺服器上的作業系統之間共用磁碟。答案非常簡單︰如果可能，請將一個獨立的磁碟專門用於 OSD，並將一部獨立的伺服器用於監控程式節點。
  </para>

  <para>
   儘管 Ceph 支援基於目錄的 OSD，但 OSD 應永遠具有一個專屬磁碟，而不能與作業系統共用一個磁碟。
  </para>

  <tip>
   <para>
    如果<emphasis>確實</emphasis>有必要在同一部伺服器上執行 OSD 和監控程式節點，請將一個獨立磁碟掛接至 <filename>/var/lib/ceph/mon</filename> 目錄以在該磁碟上執行監控程式，這樣可以稍微提升一些效能。
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="ses-bp-minimum-cluster">
  <title>最低叢集組態</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     四個物件儲存節點
    </para>
    <itemizedlist>
     <listitem>
      <para>
       10 Gb 乙太網路 (與多個交換器結合的兩個網路)
      </para>
     </listitem>
     <listitem>
      <para>
       每個儲存叢集有 32 個 OSD
      </para>
     </listitem>
     <listitem>
      <para>
       OSD 記錄可以存放在 OSD 磁碟上
      </para>
     </listitem>
     <listitem>
      <para>
       每個物件儲存節點具有專屬的 OS 磁碟
      </para>
     </listitem>
     <listitem>
      <para>
       在每個物件儲存節點上，為每 TB 的原始 OSD 容量提供 1 GB RAM
      </para>
     </listitem>
     <listitem>
      <para>
       為每個物件儲存節點上的每個 OSD 提供 1.5 GHz
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Monitor、閘道和中繼資料伺服器可以存放在物件儲存節點上
      </para>
      <itemizedlist>
       <listitem>
        <para>
         三個 Ceph Monitor 節點 (需要使用 SSD 做為專屬 OS 磁碟機)
        </para>
       </listitem>
       <listitem>
        <para>
         Ceph Monitor、物件閘道和中繼資料伺服器節點需要備援部署
        </para>
       </listitem>
       <listitem>
        <para>
         iSCSI 閘道、物件閘道和中繼資料伺服器需要增量 4 GB RAM 和四個核心
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     具有 4 GB RAM、四個核心和 1 TB 容量的獨立管理節點
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="ses-bp-production-cluster">
  <title>建議的生產叢集組態</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     七個物件儲存節點
    </para>
    <itemizedlist>
     <listitem>
      <para>
       單一節點不超出總容量的 15% 左右
      </para>
     </listitem>
     <listitem>
      <para>
       10 Gb 乙太網路 (與多個交換器結合的四個實體網路)
      </para>
     </listitem>
     <listitem>
      <para>
       每個儲存叢集有 56 個以上的 OSD
      </para>
     </listitem>
     <listitem>
      <para>
       每個 OSD 儲存節點具有 RAID 1 OS 磁碟
      </para>
     </listitem>
     <listitem>
      <para>
       依據 6:1 的 SSD 記錄與 OSD 的比率為記錄提供 SSD
      </para>
     </listitem>
     <listitem>
      <para>
       在每個物件儲存節點上，為每 TB 的原始 OSD 容量提供 1.5 GB RAM
      </para>
     </listitem>
     <listitem>
      <para>
       為每個物件儲存節點上的每個 OSD 提供 2 GHz
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     專屬的實體基礎架構節點
    </para>
    <itemizedlist>
     <listitem>
      <para>
       三個 Ceph Monitor 節點︰4 GB RAM，四核處理器，RAID 1 SSD 磁碟
      </para>
     </listitem>
     <listitem>
      <para>
       一個 SES 管理節點︰4 GB RAM，四核處理器，RAID 1 SSD 磁碟
      </para>
     </listitem>
     <listitem>
      <para>
       閘道或中繼資料伺服器節點的備援實體部署︰
      </para>
      <itemizedlist>
       <listitem>
        <para>
         物件閘道節點︰32 GB RAM，八核處理器，RAID 1 SSD 磁碟
        </para>
       </listitem>
       <listitem>
        <para>
         iSCSI 閘道節點︰16 GB RAM，四核處理器，RAID 1 SSD 磁碟
        </para>
       </listitem>
       <listitem>
        <para>
         中繼資料伺服器節點 (一個主動/一個熱待命)︰32 GB RAM，八核處理器，RAID 1 SSD 磁碟
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </itemizedlist>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="req-ses-other">
  <title>SUSE Enterprise Storage 以及其他 SUSE 產品</title>

  <para>
   本節包含將 SUSE Enterprise Storage 與其他 SUSE 產品整合的重要資訊。
  </para>

  <sect2 xml:id="req-ses-suma">
   <title>SUSE Manager</title>
   <para>
    SUSE Manager 與 SUSE Enterprise Storage 未整合，因此 SUSE Manager 目前無法管理 SUSE Enterprise Storage 叢集。
   </para>
  </sect2>
 </sect1>
</chapter>
