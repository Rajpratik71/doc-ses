<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_operating_pools.xml" version="5.0" xml:id="ceph-pools">
 <title>管理儲存池</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:translation>yes</dm:translation>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  Ceph 將資料儲存在池中。池是用於儲存物件的邏輯群組。如果您先部署叢集而不建立池，Ceph 會使用預設池來儲存資料。池為您提供︰
 </para>
 <itemizedlist mark="bullet" spacing="normal">
  <listitem>
   <para>
    <emphasis>韌性</emphasis>︰您可以設定允許多少個 OSD 發生故障而不會遺失資料。對於副本池，它是物件的所需副本/複本數。建立新池時，會將預設複本數設定為 3。因為一般的組態會儲存一個物件和一個額外的副本，所以您需要將複本數設定為 2。對於糾刪碼池，該計數為編碼區塊數 (在糾刪碼設定檔中，設定為 <emphasis>m=2</emphasis>)。
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>放置群組</emphasis>︰用於跨 OSD 將資料儲存在某個池中的內部資料結構。CRUSH 地圖中定義了 Ceph 將資料儲存到 PG 中的方式。您可以設定池的放置群組數。一般的組態為每個 OSD 使用約 100 個放置群組，以提供最佳平衡而又不會耗費太多運算資源。設定多個池時，請務必將池和叢集做為整體進行考量，以確保設定合理的放置群組數。
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>CRUSH 規則</emphasis>︰在池中儲存資料時，對應到池的 CRUSH 規則集可讓 CRUSH 識別將物件及其複本 (對於糾刪碼池，則為區塊) 放置在叢集中的規則。您可以為池建立自訂 CRUSH 規則。
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>快照</emphasis>︰使用 <command>ceph osd pool mksnap</command> 建立快照時，可高效建立特定池的快照。
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>設定擁有權</emphasis>︰可將某個使用者 ID 設定為池的擁有者。
   </para>
  </listitem>
 </itemizedlist>
 <para>
  若要將資料組織到池中，您可以列出、建立和移除池。您還可以檢視每個池的使用量統計資料。
 </para>
 <sect1 xml:id="ceph-pools-associate">
  <title>將池與應用程式關聯</title>

  <para>
   在使用池之前，需要將它們與應用程式關聯。將與 CephFS 搭配使用或由物件閘道自動建立的池會自動相關聯。需要使用 <command>rbd</command> 工具啟始化要與 RBD 搭配使用的池 (如需詳細資訊，請參閱<xref linkend="ceph-rbd-commands"/>)。
  </para>

  <para>
   對於其他情況，可以手動將自由格式的應用程式名稱與池關聯︰
  </para>

<screen><prompt>root # </prompt>ceph osd pool application enable <replaceable>pool_name</replaceable> <replaceable>application_name</replaceable></screen>

  <tip>
   <title>預設應用程式名稱</title>
   <para>
    CephFS 使用應用程式名稱 <literal>cephfs</literal>，RADOS 區塊裝置使用 <literal>rbd</literal>，物件閘道使用 <literal>rgw</literal>。
   </para>
  </tip>

  <para>
   一個池可以與多個應用程式關聯，而每個應用程式都可具有自己的中繼資料。您可以使用以下指令顯示給定池的應用程式中繼資料︰
  </para>

<screen><prompt>root # </prompt>ceph osd pool application get <replaceable>pool_name</replaceable></screen>
 </sect1>
 <sect1 xml:id="ceph-pools-operate">
  <title>操作池</title>

  <para>
   本節介紹對池執行基本任務的特定資訊。您可以瞭解如何列出、建立和刪除池，以及如何顯示池統計資料或管理池快照。
  </para>

  <sect2>
   <title>列出池</title>
   <para>
    若要列出叢集的池，請執行以下指令︰
   </para>
<screen><prompt>root # </prompt>ceph osd lspools
0 rbd, 1 photo_collection, 2 foo_pool,</screen>
  </sect2>

  <sect2 xml:id="ceph-pools-operate-add-pool">
   <title>建立池</title>
   <para>
    若要建立副本池，請執行以下指令︰
   </para>
<screen><prompt>root # </prompt>ceph osd pool create <replaceable>pool_name</replaceable> <replaceable>pg_num</replaceable> <replaceable>pgp_num</replaceable> replicated <replaceable>crush_ruleset_name</replaceable> \
<replaceable>expected_num_objects</replaceable></screen>
   <para>
    若要建立糾刪碼池，請執行以下指令︰
   </para>
<screen><prompt>root # </prompt>ceph osd pool create <replaceable>pool_name</replaceable> <replaceable>pg_num</replaceable> <replaceable>pgp_num</replaceable> erasure <replaceable>erasure_code_profile</replaceable> \
 <replaceable>crush_ruleset_name</replaceable> <replaceable>expected_num_objects</replaceable></screen>
   <para>
    如果超出了每個 OSD 的放置群組限制，則 <command>ceph osd pool create</command> 可能會失敗。使用 <option>mon_max_pg_per_osd</option> 選項可設定該限制。
   </para>
   <variablelist>
    <varlistentry>
     <term>pool_name</term>
     <listitem>
      <para>
       池的名稱，必須是唯一的。必須指定此選項。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       池的放置群組總數，必須指定此選項。預設值為 8。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       用於放置資料的放置群組總數。此數量應該與放置群組總數相等，放置群組拆分情況除外。必須指定此選項。預設值為 8。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_type</term>
     <listitem>
      <para>
       池類型，可以是 <emphasis>replicated</emphasis> (用於保留物件的多個副本，以便從失敗的 OSD 復原) 或 <emphasis>erasure</emphasis> (用於獲得某種通用 RAID5 功能)。副本池需要的原始儲存較多，但可實作所有 Ceph 操作。糾刪碼池需要的原始儲存較少，但僅實作一部分可用的操作。預設值為「replicated」。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crush_ruleset_name</term>
     <listitem>
      <para>
       此池的 crush 規則集名稱。如果指定的規則集不存在，則建立副本池的操作將會失敗，並顯示 -ENOENT。但副本池將使用指定的名稱建立新的糾刪規則集。對於糾刪碼池，預設值為「erasure-code」。對於副本池，將選取 Ceph 組態變數 <option>osd_osd_pool_default_crush_replicated_ruleset</option>。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>erasure_code_profile=profile</term>
     <listitem>
      <para>
       僅適用於糾刪碼池。使用糾刪碼設定檔。該設定檔必須是 <command>osd erasure-code-profile set</command> 所定義的現有設定檔。
      </para>
      <para>
       當您建立池時，請將放置群組數設定為合理的值 (例如 100)。還需考慮每個 OSD 的放置群組總數。放置群組在運算方面的開銷很高，因此如果您的許多池都包含很多放置群組 (例如有 50 個池，每個池各有 100 個放置群組)，效能將會下降。下降點的恢復視 OSD 主機效能而定。
      </para>
      <para>
       如需有關計算池的合適放置群組數量的詳細資料，請參閱<link xlink:href="http://docs.ceph.com/docs/master/rados/operations/placement-groups/">放置群組</link>。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>expected_num_objects</term>
     <listitem>
      <para>
       此池的預期物件數。如果設定此值，PG 資料夾拆分發生於池建立時。這可避免因執行時期資料夾拆分導致的延遲影響。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2>
   <title>設定池定額</title>
   <para>
    您可以設定池定額，限定每個池的最大位元組數和/或最大物件數。
   </para>
<screen><prompt>root # </prompt>ceph osd pool set-quota <replaceable>pool-name</replaceable> <replaceable>max_objects</replaceable> <replaceable>obj-count</replaceable> <replaceable>max_bytes</replaceable> <replaceable>bytes</replaceable></screen>
   <para>
    例如︰
   </para>
<screen><prompt>root # </prompt>ceph osd pool set-quota data max_objects 10000</screen>
   <para>
    若要移除定額，請將其值設定為 0。
   </para>
  </sect2>

  <sect2 xml:id="ceph-pools-operate-del-pool">
   <title>刪除池</title>
   <warning>
    <title>刪除池的操作不可逆</title>
    <para>
     池中可能包含重要資料。刪除池會導致池中的所有資料消失，且無法復原。
    </para>
   </warning>
   <para>
    不小心刪除池十分危險，因此 Ceph 實作了兩個機制來防止刪除池。若要刪除池，必須先停用這兩個機制。
   </para>
   <para>
    第一個機制是 <literal>NODELETE</literal> 旗標。每個池都有這個旗標，其預設值為「false」。若要確定某個池的此旗標值，請執行以下指令︰
   </para>
<screen><prompt>root # </prompt>ceph osd pool get <replaceable>pool_name</replaceable> nodelete</screen>
   <para>
    如果指令輸出 <literal>nodelete: true</literal>，則只有在使用以下指令變更該旗標後，才能刪除池︰
   </para>
<screen>ceph osd pool set <replaceable>pool_name</replaceable> nodelete false</screen>
   <para>
    第二個機制是叢集範圍的組態參數 <option>mon allow pool delete</option>，其預設值為「false」。這表示預設不能刪除池。顯示的錯誤訊息是︰
   </para>
<screen>Error EPERM: pool deletion is disabled; you must first set the
mon_allow_pool_delete config option to true before you can destroy a pool</screen>
   <para>
    若要規避此安全設定以刪除池，可以暫時將 <option>mon allow pool delete</option> 設定為「true」，刪除池，然後將該參數恢復為「false」︰
   </para>
<screen><prompt>root # </prompt>ceph tell mon.* injectargs --mon-allow-pool-delete=true
<prompt>root # </prompt>ceph osd pool delete pool_name pool_name --yes-i-really-really-mean-it
<prompt>root # </prompt>ceph tell mon.* injectargs --mon-allow-pool-delete=false</screen>
   <para>
    <command>injectargs</command> 指令會顯示以下訊息︰
   </para>
<screen>injectargs:mon_allow_pool_delete = 'true' (not observed, change may require restart)</screen>
   <para>
    這主要用於確認該指令已成功執行。它不是錯誤。
   </para>
   <para>
    如果為您建立的池建立了自己的規則集和規則，則應該考慮在不再需要該池時移除規則集和規則。如果您建立了僅對不再存在的池具有權限的使用者，則應該考慮也刪除那些使用者。
   </para>
  </sect2>

  <sect2>
   <title>重新命名池</title>
   <para>
    若要重新命名池，請執行以下指令︰
   </para>
<screen><prompt>root # </prompt>ceph osd pool rename <replaceable>current-pool-name</replaceable> <replaceable>new-pool-name</replaceable></screen>
   <para>
    如果重新命名了某個池，且為經過驗證的使用者使用了依池能力，則必須用新的池名稱更新使用者的能力。
   </para>
  </sect2>

  <sect2>
   <title>顯示池統計資料</title>
   <para>
    若要顯示池的使用量統計資料，請執行以下指令︰
   </para>
<screen><prompt>root # </prompt>rados df
pool name  category  KB  objects   lones  degraded  unfound  rd  rd KB  wr  wr KB
cold-storage    -   228   1         0      0          0       0   0      1   228
data            -    1    4         0      0          0       0   0      4    4
hot-storage     -    1    2         0      0          0       15  10     5   231
metadata        -    0    0         0      0          0       0   0      0    0
pool1           -    0    0         0      0          0       0   0      0    0
rbd             -    0    0         0      0          0       0   0      0    0
total used          266268          7
total avail       27966296
total space       28232564</screen>
  </sect2>

  <sect2 xml:id="ceph-pools-values">
   <title>設定池的值</title>
   <para>
    若要設定池的值，請執行以下指令︰
   </para>
<screen><prompt>root # </prompt>ceph osd pool set <replaceable>pool-name</replaceable> <replaceable>key</replaceable> <replaceable>value</replaceable></screen>
   <para>
    您可以設定以下鍵的值︰
   </para>
   <variablelist>
    <varlistentry>
     <term>size</term>
     <listitem>
      <para>
       設定池中物件的複本數。如需更多詳細資料，請參閱<xref linkend="ceph-pools-options-num-of-replicas"/>。僅限副本池。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>min_size</term>
     <listitem>
      <para>
       設定 I/O 所需的最小複本數。如需更多詳細資料，請參閱<xref linkend="ceph-pools-options-num-of-replicas"/>。僅限副本池。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crash_replay_interval</term>
     <listitem>
      <para>
       允許用戶端重送已確認但未提交的要求的秒數。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       池的放置群組數。如果將 OSD 新增至叢集，則應該提高放置群組的值，如需詳細資料，請參閱<xref linkend="storage-bp-cluster-mntc-add-pgnum"/>。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       計算資料放置時要使用的放置群組的有效數量。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crush_ruleset</term>
     <listitem>
      <para>
       用於在叢集中對應物件放置的規則集。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hashpspool</term>
     <listitem>
      <para>
       為給定池設定 (1) 或取消設定 (0) HASHPSPOOL 旗標。啟用此旗標會變更演算法，以採用更佳的方式將 PG 分佈到 OSD 之間。對之前 HASHPSPOOL 旗標設為 0 的池啟用此旗標後，叢集會開始回填，以使所有 PG 都可再次正確放置。請注意，這可能會在叢集上產生相當高的 I/O 負載，因此對高負載生產叢集必須進行妥善規劃。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nodelete</term>
     <listitem>
      <para>
       防止移除池。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nopgchange</term>
     <listitem>
      <para>
       防止變更池的 <option>pg_num</option> 和 <option>pgp_num</option>。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nosizechange</term>
     <listitem>
      <para>
       防止變更池的大小。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>write_fadvise_dontneed</term>
     <listitem>
      <para>
       對給定池設定/取消設定 <literal>WRITE_FADVISE_DONTNEED</literal> 旗標。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>noscrub、nodeep-scrub</term>
     <listitem>
      <para>
       停用 (深層) 整理特定池的資料，以解決暫時性的高 I/O 負載問題。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_type</term>
     <listitem>
      <para>
       對快取池啟用命中集追蹤。請參閱<link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">布隆過濾器</link>以瞭解更多資訊。此選項可用的值如下︰<literal>bloom</literal>、<literal>explicit_hash</literal>、<literal>explicit_object</literal>。預設值為 <literal>bloom</literal>，其他值僅用於測試。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_count</term>
     <listitem>
      <para>
       要為快取池儲存的命中集數。該數值越高，<systemitem>ceph-osd</systemitem> 精靈耗用的 RAM 越多。預設值為 <literal>0</literal>。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_period</term>
     <listitem>
      <para>
       快取池的命中集期間的時長 (以秒計)。該數值越高，<systemitem>ceph-osd</systemitem> 精靈耗用的 RAM 越多。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_fpp</term>
     <listitem>
      <para>
       布隆命中集類型的誤報率。請參閱<link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">布隆過濾器</link>以瞭解更多資訊。有效範圍是 0.0 - 1.0，預設值為 <literal>0.05</literal>
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>use_gmt_hitset</term>
     <listitem>
      <para>
       為快取分層建立命中集時，強制 OSD 使用 GMT (格林威治標準時間) 時戳。這可確保在不同時區中的節點傳回相同的結果。預設值為 <literal>1</literal>。不應該變更此值。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_dirty_ratio</term>
     <listitem>
      <para>
       在快取分層代理程式將已修改 (髒) 物件衝洗到支援儲存池之前，包含此類物件的快取池百分比。預設值為 <literal>.4</literal>
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_dirty_high_ratio</term>
     <listitem>
      <para>
       在快取分層代理程式將已修改 (髒) 物件衝洗到速度更快的支援儲存池之前，包含此類物件的快取池百分比。預設值為 <literal>.6</literal>。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_full_ratio</term>
     <listitem>
      <para>
       在快取分層代理程式將未修改 (乾淨) 物件從快取池逐出之前，包含此類物件的快取池百分比。預設值為 <literal>.8</literal>
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>target_max_bytes</term>
     <listitem>
      <para>
       觸發 <option>max_bytes</option> 閾值後，Ceph 將會開始衝洗或逐出物件。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>target_max_objects</term>
     <listitem>
      <para>
       觸發 <option>max_objects</option> 閾值後，Ceph 將會開始衝洗或逐出物件。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_grade_decay_rate</term>
     <listitem>
      <para>
       兩次連續的 <literal>hit_set</literal> 之間的溫度降低率。預設值為 <literal>20</literal>。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_search_last_n</term>
     <listitem>
      <para>
       計算溫度時在 <literal>hit_set</literal> 中對出現的項最多計 <literal>N</literal> 次。預設值為 <literal>1</literal>。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_min_flush_age</term>
     <listitem>
      <para>
       在快取分層代理程式將物件從快取池衝洗到儲存池之前的時間 (以秒計)。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_min_evict_age</term>
     <listitem>
      <para>
       在快取分層代理程式將物件從快取池中逐出之前的時間 (以秒計)。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>fast_read</term>
     <listitem>
      <para>
       如果對糾刪碼池啟用此旗標，則讀取要求會向所有分區發出子讀取指令，並一直等到接收到足夠解碼的分區，才會為用戶端提供服務。對於 <emphasis>jerasure</emphasis> 和 <emphasis>isa</emphasis> 糾刪外掛程式，前 <literal>K</literal> 個複本傳回時，就會使用從這些複本解碼的資料立即處理用戶端的要求。這有助於獲得一些資源以提高效能。目前，此旗標僅支援用於糾刪碼池。預設值為 <literal>0</literal>。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>scrub_min_interval</term>
     <listitem>
      <para>
       叢集負載低時整理池的最小間隔 (以秒計)。預設值 <literal>0</literal> 表示使用來自 Ceph 組態檔案的 <option>osd_scrub_min_interval</option> 值。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>scrub_max_interval</term>
     <listitem>
      <para>
       無論叢集負載如何都整理池的最大間隔 (以秒計)。預設值 <literal>0</literal> 表示使用來自 Ceph 組態檔案的 <option>osd_scrub_max_interval</option> 值。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>deep_scrub_interval</term>
     <listitem>
      <para>
       <emphasis>深層</emphasis>整理池的間隔 (以秒計)。預設值 <literal>0</literal> 表示使用來自 Ceph 組態檔案的 <option>osd_deep_scrub</option> 值。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2>
   <title>獲取池的值</title>
   <para>
    若要獲取池中的值，請執行以下指令︰
   </para>
<screen><prompt>root # </prompt>ceph osd pool get <replaceable>pool-name</replaceable> <replaceable>key</replaceable></screen>
   <para>
    您可以獲取<xref linkend="ceph-pools-values"/>中所列鍵以及下列鍵的值︰
   </para>
   <variablelist>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       池的放置群組數。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       計算資料放置時要使用的放置群組的有效數量。有效範圍小於或等於 <literal>pg_num</literal>。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph-pools-options-num-of-replicas">
   <title>設定物件複本數</title>
   <para>
    若要設定副本池上的物件複本數，請執行以下指令︰
   </para>
<screen><prompt>root # </prompt>ceph osd pool set <replaceable>poolname</replaceable> size <replaceable>num-replicas</replaceable></screen>
   <para>
    <replaceable>num-replicas</replaceable> 包括物件自身。例如，如果您想用物件和物件的兩個副本組成物件的三個例項，請指定 3。
   </para>
   <para>
    如果將 <replaceable>num-replicas</replaceable> 設定為 2，資料將只有<emphasis>一個</emphasis>副本。例如，如果您遺失了一個物件例項，則需要在復原期間確定自上次<link xlink:href="http://ceph.com/docs/master/rados/configuration/osd-config-ref/#scrubbing">整理</link>後，另一個副本沒有損毀。
   </para>
   <para>
    將池設定為具有一個複本意味著池中的資料物件只有<emphasis>一個</emphasis>例項。如果 OSD 發生故障，您將遺失資料。若要短時間儲存臨時資料，可能就會用到只有一個副本的池。
   </para>
   <para>
    為池設定三個以上副本只能小幅提高可靠性，但在極少數情況下可能適用。請記住，複本越多，儲存物件副本所需的磁碟空間就越多。如果您需要最高的資料安全性，則建議使用糾刪碼池。如需詳細資訊，請參閱<xref linkend="cha-ceph-erasure"/>。
   </para>
   <warning>
    <title>建議使用兩個以上複本</title>
    <para>
     強烈建議不要僅使用 2 個複本。如果一個 OSD 發生故障，復原期間的高負載很可能會導致第二個 OSD 也發生故障。
    </para>
   </warning>
   <para>
    例如︰
   </para>
<screen><prompt>root # </prompt>ceph osd pool set data size 3</screen>
   <para>
    您可針對每個池執行此指令。
   </para>
   <note>
    <para>
     物件可接受降級模式下複本數量低於 <literal>pool size</literal> 的 I/O。若要設定 I/O 所需複本的最小數量，您應該使用 <literal>min_size</literal> 設定。例如︰
    </para>
<screen><prompt>root # </prompt>ceph osd pool set data min_size 2</screen>
    <para>
     這可確保資料池中沒有物件會接收到複本數量低於 <literal>min_size</literal> 的 I/O。
    </para>
   </note>
  </sect2>

  <sect2>
   <title>獲取物件複本數</title>
   <para>
    若要獲取物件複本數，請執行以下指令︰
   </para>
<screen><prompt>root # </prompt>ceph osd dump | grep 'replicated size'</screen>
   <para>
    Ceph 將列出池，並反白顯示 <literal>replicated size</literal> 屬性。Ceph 預設會建立物件的兩個複本 (共三個副本，或者大小為 3)。
   </para>
  </sect2>

  <sect2 xml:id="storage-bp-cluster-mntc-add-pgnum">
   <title>增加放置群組數</title>
   <para>
    建立新池時，需指定池的放置群組數 (請參閱<xref linkend="ceph-pools-operate-add-pool"/>)。將更多 OSD 新增至叢集後，出於效能和資料持久性原因，通常還需要增加放置群組數。對於每個放置群組，OSD 和監控程式節點無論何時都需要用到記憶體、網路和 CPU，在復原期間需求量甚至更大。因此，最大限度地減少放置群組數可節省相當大的資源量。
   </para>
   <warning>
    <title><option>pg_num</option> 的值過高</title>
    <para>
     變更池的 <option>pg_num</option> 值時，新的放置群組數有可能會超出允許的限制。例如
    </para>
<screen><prompt>root # </prompt>ceph osd pool set rbd pg_num 4096
 Error E2BIG: specified pg_num 3500 is too large (creating 4096 new PGs \
 on ~64 OSDs exceeds per-OSD max of 32)</screen>
    <para>
     該限制可防止放置群組過度拆分，它從 <option>mon_osd_max_split_count</option> 值衍生。
    </para>
   </warning>
   <para>
    為已調整大小的叢集確定合適的新放置群組數是一項複雜的任務。一種方法是不斷增加放置群組數，直到達到叢集效能的最佳狀態。若要確定增加後的新放置群組數，您需要獲取 <option>mon_osd_max_split_count</option> 參數的值，並將它與目前的放置群組數相加。若要瞭解基本原理，請查看下面的程序檔︰
   </para>
<screen><prompt>cephadm &gt; </prompt>max_inc=`ceph daemon mon.a config get mon_osd_max_split_count 2&gt;&amp;1 \
  | tr -d '\n ' | sed 's/.*"\([[:digit:]]\+\)".*/\1/'`
<prompt>cephadm &gt; </prompt>pg_num=`ceph osd pool get rbd pg_num | cut -f2 -d: | tr -d ' '`
<prompt>cephadm &gt; </prompt>echo "current pg_num value: $pg_num, max increment: $max_inc"
<prompt>cephadm &gt; </prompt>next_pg_num="$(($pg_num+$max_inc))"
<prompt>cephadm &gt; </prompt>echo "allowed increment of pg_num: $next_pg_num"</screen>
   <para>
    確定新的放置群組數之後，使用以下指令來增加該數量︰
   </para>
<screen><prompt>root # </prompt>ceph osd pool set <replaceable>pool_name</replaceable> pg_num <replaceable>next_pg_num</replaceable></screen>
  </sect2>

  <sect2 xml:id="storage-bp-cluster-mntc-add-pool">
   <title>新增池</title>
   <para>
    當您首次部署叢集之後，Ceph 會使用預設池來儲存資料。之後，您可以使用以下指令來建立新的池︰
   </para>
<screen><prompt>root # </prompt>ceph osd pool create</screen>
   <para>
    如需建立叢集池的詳細資訊，請參閱<xref linkend="ceph-pools-operate-add-pool"/>。
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="pools-migration">
  <title>池移轉</title>

  <para>
   建立池 (請參閱<xref linkend="ceph-pools-operate-add-pool"/>) 時，您需要指定池的初始參數，例如池類型或放置群組數量。如果您在池內放置資料後，又決定變更任何初始參數，則需要將池資料移轉至參數適合您的部署的另一個池中。
  </para>

  <para>
   移轉池的方法有多種。建議使用<emphasis>快取層</emphasis>，因為該方法是透明的，可減少叢集停機時間，並避免複製所有池的資料。
  </para>

  <sect2 xml:id="pool-migrate-cache-tier">
   <title>使用快取層移轉</title>
   <para>
    該方法的原理十分簡單，只需將需要移轉的池依相反的順序加入快取層中即可。如需快取層的詳細資訊，請參閱<xref linkend="cha-ceph-tiered"/>。例如，若要將副本池「testpool」移轉至糾刪碼池，請執行以下步驟︰
   </para>
   <procedure>
    <title>將副本池移轉至糾刪碼池</title>
    <step>
     <para>
      建立一個名為「newpool」的新糾刪碼池︰
     </para>
<screen>
<prompt>root@minion &gt; </prompt>ceph osd pool create newpool 4096 4096 erasure default
</screen>
     <para>
      您現在有兩個池，即填滿資料的原始副本池「testpool」和新的空白糾刪碼池「newpool」︰
     </para>
     <figure>
      <title>移轉前的池</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate1.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate1.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      設定快取層，並將副本池「testpool」設定為快取池︰
     </para>
<screen>
<prompt>root@minion &gt; </prompt>ceph osd tier add newpool testpool --force-nonempty
<prompt>root@minion &gt; </prompt>ceph osd cache-mode testpool forward
</screen>
     <para>
      自此之後，所有新物件都將建立在新池中︰
     </para>
     <figure>
      <title>快取層設定</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate2.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate2.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      強制快取池將所有物件移到新池中︰
     </para>
<screen>
<prompt>root@minion &gt; </prompt>rados -p testpool cache-flush-evict-all
</screen>
     <figure>
      <title>資料衝洗</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate3.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate3.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      將所有用戶端切換到新池。您需要指定一個重疊層，以便在舊池中搜尋物件，直到所有資料都已衝洗到新的糾刪碼池。
     </para>
<screen>
<prompt>root@minion &gt; </prompt>ceph osd tier set-overlay newpool testpool
</screen>
     <para>
      有了重疊層，所有操作都會轉到舊的副本池「testpool」︰
     </para>
     <figure>
      <title>設定重疊層</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate4.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate4.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      現在，您可以將所有用戶端都切換為存取新池中的物件。
     </para>
    </step>
    <step>
     <para>
      所有資料都移轉至糾刪碼池「newpool」後，移除重疊層和舊快取池「testpool」︰
     </para>
<screen>
<prompt>root@minion &gt; </prompt>ceph osd tier remove-overlay newpool
<prompt>root@minion &gt; </prompt>ceph osd tier remove newpool testpool
</screen>
     <figure>
      <title>完成移轉</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate5.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate5.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="cha-ceph-snapshots-pool">
  <title>池快照</title>

  <para>
   池快照是整個 Ceph 池的狀態快照。透過池快照，您可以保留池狀態的歷程。建立池快照可能需要大量儲存空間，具體取決於池的大小。在建立池快照之前，請務必檢查相關儲存是否有足夠的磁碟空間。
  </para>

  <sect2>
   <title>建立池快照</title>
   <para>
    若要建立池快照，請執行以下指令︰
   </para>
<screen><prompt>root # </prompt>ceph osd pool mksnap <replaceable>pool-name</replaceable> <replaceable>snap-name</replaceable></screen>
   <para>
    例如︰
   </para>
<screen><prompt>root # </prompt>ceph osd pool mksnap pool1 snapshot1
created pool pool1 snap snapshot1</screen>
  </sect2>

  <sect2>
   <title>移除池快照</title>
   <para>
    若要移除池快照，請執行以下指令︰
   </para>
<screen><prompt>root # </prompt>ceph osd pool rmsnap <replaceable>pool-name</replaceable> <replaceable>snap-name</replaceable></screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-ceph-pool-compression">
  <title>資料壓縮</title>

  <para>
   從 SUSE Enterprise Storage 5 開始，BlueStore 提供即時資料壓縮，以節省磁碟空間。
  </para>

  <sect2 xml:id="sec-ceph-pool-compression-enable">
   <title>啟用壓縮</title>
   <para>
    可使用以下指令啟用池的資料壓縮︰
   </para>
<screen><prompt>root # </prompt><command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> ompression_algorithm snappy
<prompt>root # </prompt><command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> compression_mode aggressive</screen>
   <para>
    以要啟用壓縮的池取代 <replaceable>POOL_NAME</replaceable>。
   </para>
  </sect2>

  <sect2 xml:id="sec-ceph-pool-compression-options">
   <title>池壓縮選項</title>
   <para>
    完整的壓縮設定清單︰
   </para>
   <variablelist>
    <varlistentry>
     <term>compression_algorithm</term>
     <listitem>
      <para>
       值︰<literal>none</literal>、<literal>zstd</literal>、<literal>snappy</literal>。預設值︰<literal>snappy</literal>。
      </para>
      <para>
       使用的壓縮演算法取決於特定使用案例。下面是幾點建議︰
      </para>
      <itemizedlist>
       <listitem>
        <para>
         不要使用 <literal>zlib</literal>，其餘幾種演算法更好。
        </para>
       </listitem>
       <listitem>
        <para>
         如果需要較好的壓縮率，請使用 <literal>zstd</literal>。注意，由於 <literal>zstd</literal> 在壓縮少量資料時 CPU 負擔較重，建議不要將其用於 BlueStore。
        </para>
       </listitem>
       <listitem>
        <para>
         如果需要較低的 CPU 使用率，請使用 <literal>lz4</literal> 或 <literal>snappy</literal>。
        </para>
       </listitem>
       <listitem>
        <para>
         針對實際資料的樣本執行這些演算法的基準測試，同時觀察叢集的 CPU 和記憶體使用率。
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_mode</term>
     <listitem>
      <para>
       值︰{<literal>none</literal>、<literal>aggressive</literal>、<literal>passive</literal>、<literal>force</literal>}。預設值：<literal>none</literal>。
      </para>
      <itemizedlist>
       <listitem>
        <para>
         <literal>none</literal>︰永不壓縮
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>passive</literal>︰如果提示 <literal>COMPRESSIBLE</literal>，則壓縮
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>aggressive</literal>︰除非提示 <literal>INCOMPRESSIBLE</literal>，才壓縮
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>force</literal>︰永遠都壓縮
        </para>
       </listitem>
      </itemizedlist>
      <para>
       如需如何設定 <literal>COMPRESSIBLE</literal> 或 <literal>INCOMPRESSIBLE</literal> 旗標的資訊，請參閱 <link xlink:href="http://docs.ceph.com/docs/doc-12.2.0-major-changes/rados/api/librados/#rados_set_alloc_hint"/>。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_required_ratio</term>
     <listitem>
      <para>
       值︰雙精度浮點數，比率 = SIZE_COMPRESSED / SIZE_ORIGINAL。預設值︰<literal>.875</literal>
      </para>
      <para>
       由於淨增益低，儲存高於此比率的物件時不會壓縮。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_max_blob_size</term>
     <listitem>
      <para>
       值︰不帶正負號的整數，大小以位元組計。預設值︰<literal>0</literal>
      </para>
      <para>
       所壓縮物件的最大大小。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_min_blob_size</term>
     <listitem>
      <para>
       值︰不帶正負號的整數，大小以位元組計。預設值︰<literal>0</literal>
      </para>
      <para>
       所壓縮物件的最小大小。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="sec-ceph-pool-bluestore-compression-options">
   <title>全域壓縮選項</title>
   <para>
    可在 Ceph 組態中設定以下組態選項，並將其套用於所有 OSD 而不僅僅是單個池。<xref linkend="sec-ceph-pool-compression-options"/>中列出的池特定組態優先。
   </para>
   <variablelist>
    <varlistentry>
     <term>bluestore_compression_algorithm</term>
     <listitem>
      <para>
       值︰<literal>none</literal>、<literal>zstd</literal>、<literal>snappy</literal>、<literal>zlib</literal>。預設值︰<literal>snappy</literal>。
      </para>
      <para>
       使用的壓縮演算法取決於特定使用案例。下面是幾點建議︰
      </para>
      <itemizedlist>
       <listitem>
        <para>
         不要使用 <literal>zlib</literal>，其餘幾種演算法更好。
        </para>
       </listitem>
       <listitem>
        <para>
         如果需要較好的壓縮率，請使用 <literal>zstd</literal>。注意，由於 <literal>zstd</literal> 在壓縮少量資料時 CPU 負擔較重，建議不要將其用於 BlueStore。
        </para>
       </listitem>
       <listitem>
        <para>
         如果需要較低的 CPU 使用率，請使用 <literal>lz4</literal> 或 <literal>snappy</literal>。
        </para>
       </listitem>
       <listitem>
        <para>
         針對實際資料的樣本執行這些演算法的基準測試，同時觀察叢集的 CPU 和記憶體使用率。
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_mode</term>
     <listitem>
      <para>
       值︰{<literal>none</literal>、<literal>aggressive</literal>、<literal>passive</literal>、<literal>force</literal>}。預設值：<literal>none</literal>。
      </para>
      <itemizedlist>
       <listitem>
        <para>
         <literal>none</literal>︰永不壓縮
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>passive</literal>︰如果提示 <literal>COMPRESSIBLE</literal>，則壓縮。
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>aggressive</literal>︰除非提示 <literal>INCOMPRESSIBLE</literal>，才壓縮
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>force</literal>︰永遠都壓縮
        </para>
       </listitem>
      </itemizedlist>
      <para>
       如需如何設定 <literal>COMPRESSIBLE</literal> 或 <literal>INCOMPRESSIBLE</literal> 旗標的資訊，請參閱 <link xlink:href="http://docs.ceph.com/docs/doc-12.2.0-major-changes/rados/api/librados/#rados_set_alloc_hint"/>。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_required_ratio</term>
     <listitem>
      <para>
       值︰雙精度浮點數，比率 = SIZE_COMPRESSED / SIZE_ORIGINAL。預設值︰<literal>.875</literal>
      </para>
      <para>
       由於淨增益低，儲存高於此比率的物件時不會壓縮。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size</term>
     <listitem>
      <para>
       值︰不帶正負號的整數，大小以位元組計。預設值︰<literal>0</literal>
      </para>
      <para>
       所壓縮物件的最小大小。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size</term>
     <listitem>
      <para>
       值︰不帶正負號的整數，大小以位元組計。預設值︰<literal>0</literal>
      </para>
      <para>
       所壓縮物件的最大大小。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size_ssd</term>
     <listitem>
      <para>
       值︰不帶正負號的整數，大小以位元組計。預設值︰<literal>8K</literal>
      </para>
      <para>
       壓縮並儲存在固態硬碟上的物件的最小大小。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size_ssd</term>
     <listitem>
      <para>
       值︰不帶正負號的整數，大小以位元組計。預設值︰<literal>64K</literal>
      </para>
      <para>
       壓縮並儲存在固態硬碟上的物件的最大大小。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size_hdd</term>
     <listitem>
      <para>
       值︰不帶正負號的整數，大小以位元組計。預設值︰<literal>128K</literal>
      </para>
      <para>
       壓縮並儲存在普通硬碟上的物件的最小大小。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size_hdd</term>
     <listitem>
      <para>
       值︰不帶正負號的整數，大小以位元組計。預設值︰<literal>512K</literal>
      </para>
      <para>
       壓縮並儲存在普通硬碟上的物件的最大大小。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
</chapter>
