<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_install_salt.xml" version="5.0" xml:id="ceph-install-saltstack">
 <title>Bereitstellen mit DeepSea/Salt</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:translation>Ja</dm:translation>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <note>
  <title><command>ceph-deploy</command> in SUSE Enterprise Storage 5 entfernt</title>
  <para>
   Das Cluster-Bereitstellungswerkzeug <command>ceph-deploy</command> wurde in SUSE Enterprise Storage 4 als veraltet gekennzeichnet und ab SUSE Enterprise Storage 5 zugunsten von DeepSea vollständig entfernt.
  </para>
 </note>
 <para>
  Salt bildet zusammen mit DeepSea ein <emphasis>Paket</emphasis> von Komponenten, die bei der Bereitstellung und Verwaltung von Serverinfrastruktur nützlich sind. Es ist hoch skalierbar, schnell und lässt sich relativ leicht in Betrieb nehmen. Lesen Sie die folgenden Überlegungen, bevor Sie mit der Bereitstellung des Clusters mit Salt beginnen:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    <emphasis>Salt Minions</emphasis> sind die Nodes, die von einem dedizierten Node namens Salt Master gesteuert werden. Salt Minions verfügen über Rollen wie zum Beispiel Ceph OSD, Ceph Monitor, Ceph Manager, Object Gateway, iSCSI Gateway oder NFS Ganesha.
   </para>
  </listitem>
  <listitem>
   <para>
    Ein Salt Master führt seinen eigenen Salt Minion aus. Er ist erforderlich zum Ausführen von privilegierten Aufgaben, beispielsweise Erstellen, Autorisieren und Kopieren von Schlüsseln für Minions, sodass Remote Minions niemals privilegierte Aufgaben ausführen müssen.
   </para>
   <tip>
    <title>Freigeben mehrerer Rollen pro Server</title>
    <para>
     Sie erreichen die optimale Leistung Ihres Ceph Clusters, wenn jede Rolle in einem separaten Node bereitgestellt wird. Manchmal ist es jedoch bei Bereitstellungen erforderlich, einen Node für mehrere Rollen freizugeben. Stellen Sie die Ceph OSD-, Metadata Server- oder Ceph Monitor-Rolle nicht auf dem Salt Master bereit, um Probleme mit der Leistung und dem Upgrade-Vorgang zu vermeiden.
    </para>
   </tip>
  </listitem>
  <listitem>
   <para>
    Salt Minions müssen den Hostnamen des Salt Master im gesamten Netzwerk korrekt auflösen. Standardmäßig suchen sie nach dem Hostnamen <systemitem>salt</systemitem>, Sie können jedoch auch in Datei <filename>/etc/salt/minion</filename> jeden vom Netzwerk erreichbaren Hostnamen angeben. Weitere Informationen hierzu finden Sie in <xref linkend="ceph-install-stack"/>.
   </para>
  </listitem>
 </itemizedlist>
 <sect1 xml:id="cha-ceph-install-relnotes">
  <title>Lesen Sie die Versionshinweise</title>

  <para>
   In den Versionshinweisen finden Sie zusätzliche Informationen zu den Änderungen, die seit der vorigen Version von SUSE Enterprise Storage vorgenommen wurden. Informieren Sie sich in den Versionshinweisen über Folgendes:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Sind bei der Hardware besondere Überlegungen zu beachten?
    </para>
   </listitem>
   <listitem>
    <para>
     Wurden erhebliche Änderungen an den verwendeten Software-Paketen vorgenommen?
    </para>
   </listitem>
   <listitem>
    <para>
     Gelten besondere Vorsichtsmaßnahmen für die vorliegende Installation?
    </para>
   </listitem>
  </itemizedlist>

  <para>
   In den Versionshinweisen finden Sie auch Informationen, die erst nach der Fertigstellung des Handbuchs bekannt wurden. Auch bekannte Probleme werden beschrieben.
  </para>

  <para>
   Nach Installation des Pakets <package>release-notes-ses</package>finden Sie die Versionshinweise lokal im Verzeichnis <filename>/usr/share/doc/release-notes</filename> oder online unter <link xlink:href="https://www.suse.com/releasenotes/"/>.
  </para>
 </sect1>
 <sect1 xml:id="deepsea-description">
  <title>Einführung zu DeepSea</title>

  <para>
   Das Ziel von DeepSea besteht darin, dem Administrator Zeit zu sparen und komplexe Operationen im Ceph Cluster zuverlässig durchzuführen.
  </para>

  <para>
   Ceph ist eine umfassend konfigurierbare Softwarelösung. Systemadministratoren gewinnen dadurch mehr Freiheit, haben aber auch mehr Verantwortung.
  </para>

  <para>
   Die minimale Einrichtung von Ceph eignet sich gut für Demonstrationszwecke, zeigt jedoch nicht die interessanten Funktionen von Ceph, die bei einer großen Anzahl von Nodes zum Tragen kommen.
  </para>

  <para>
   DeepSea erfasst und speichert Daten über individuelle Server, wie Adressen und Gerätenamen. Bei einem dezentralen Speichersystem wie Ceph müssen gegebenenfalls Hunderte solcher Elemente erfasst und gespeichert werden. Manuelles Erfassen von Informationen und Eingeben der Daten in ein Konfigurationsmanagement-Tool ist ermüdend und fehleranfällig.
  </para>

  <para>
   Die zum Vorbereiten des Servers, Erfassen der Konfiguration sowie zum Konfigurieren und Bereitstellen von Ceph erforderlichen Schritte sind weitgehend dieselben. Dies trifft jedoch nicht auf das Verwalten der unterschiedlichen Funktionen zu. Bei täglich anfallenden Vorgängen braucht man unbedingt die Möglichkeit, Hardware problemlos zu einer vorliegenden Funktion hinzuzufügen und sie wieder zu entfernen.
  </para>

  <para>
   DeepSea setzt hierfür folgende Strategie ein: DeepSea konsolidiert die Entscheidungen des Administrators in einer einzelnen Datei. Zu den Entscheidungen zählen Cluster-Zuweisung, Rollenzuweisung und Profilzuweisung. Und DeepSea fasst jede Aufgabengruppe zu einem einfachen Ziel zusammen. Jedes Ziel ist eine <emphasis>Phase</emphasis>:
  </para>

  <itemizedlist xml:id="deepsea-stage-description">
   <title>Beschreibung von DeepSea-Phasen</title>
   <listitem>
    <para>
     <emphasis role="bold">Phase 0</emphasis> – <emphasis role="bold">Vorbereitung</emphasis> – in dieser Phase werden alle erforderlichen Updates angewendet und Ihr System wird möglicherweise neugestartet.
    </para>
    <important>
     <title>Phase 0 nach dem Neustart von Salt Master erneut ausführen</title>
     <para>
      Wenn in Phase 0 der Salt Master neustartet, um die neue Kernel-Version zu laden, müssen Sie Phase 0 erneut ausführen, ansonsten werden die Minions nicht adressiert.
     </para>
    </important>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Phase 1</emphasis> – <emphasis role="bold">Ermittlung</emphasis> – hier ermitteln Sie die gesamte Hardware in Ihrem Cluster und erfassen die erforderlichen Informationen für die Ceph-Konfiguration. Ausführliche Informationen zur Konfiguration finden Sie in <xref linkend="deepsea-pillar-salt-configuration"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Phase 2</emphasis> – <emphasis role="bold">Konfiguration</emphasis> – Konfigurationsdaten müssen in einem bestimmten Format vorbereitet werden.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Phase 3</emphasis> – <emphasis role="bold">Bereitstellung</emphasis> – erstellt einen einfachen Ceph Cluster mit obligatorischen Ceph Services. Eine Liste der Services finden Sie in <xref linkend="storage-intro-core-nodes"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Phase 4</emphasis> – <emphasis role="bold">Services</emphasis>– zusätzliche Funktionen von Ceph wie iSCSI, Object Gateway und CephFS können in dieser Phase installiert werden. Jede der Funktionen ist optional.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Phase 5</emphasis> – Entfernen. Diese Phase ist nicht obligatorisch und bei der ersten Einrichtung normalerweise nicht erforderlich. In dieser Phase werden die Rollen der Minions und auch die Cluster-Konfiguration entfernt. Sie müssen diese Phase ausführen, wenn Sie einen Speicher-Node von Ihrem Cluster entfernen müssen. Weitere Informationen finden Sie im <xref linkend="salt-node-removing"/>.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Eine detailliertere Einführung zu DeepSea finden Sie unter <link xlink:href="https://github.com/suse/deepsea/wiki"/>.
  </para>

  <sect2 xml:id="deepsea-organisation-locations">
   <title>Organisation und wichtige Standorte</title>
   <para>
    Salt wendet für Ihren Master Node einige standardmäßige Standorte und Benennungskonventionen an:
   </para>
   <variablelist>
    <varlistentry>
     <term><filename>/srv/pillar</filename>
     </term>
     <listitem>
      <para>
       In diesem Verzeichnis werden Konfigurationsdaten für Ihre Cluster Minions gespeichert. <emphasis>Pillar</emphasis> ist eine Schnittstelle zum Bereitstellen globaler Konfigurationswerte für alle Cluster Minions.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/salt/</filename>
     </term>
     <listitem>
      <para>
       In diesem Verzeichnis speichert Salt die Zustandsdateien (auch <emphasis>sls</emphasis>-Dateien genannt). Zustandsdateien sind formatierte Beschreibungen zum Zustand, in dem sich der Cluster befinden sollte. Weitere Informationen dazu finden Sie in der <link xlink:href="https://docs.saltstack.com/en/latest/topics/tutorials/starting_states.html">Salt-Dokumentation</link>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/module/runners</filename>
     </term>
     <listitem>
      <para>
       In diesem Verzeichnis werden Python-Skripts gespeichert, die als Ausführungsprogramme (runner) bezeichnet werden. Die Ausführungsprogramme werden im Master Node ausgeführt.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/salt/_modules</filename>
     </term>
     <listitem>
      <para>
       In diesem Verzeichnis werden Python-Skripts gespeichert, die als Module bezeichnet werden. Die Module werden auf alle Minions in Ihrem Cluster angewendet.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/pillar/ceph</filename>
     </term>
     <listitem>
      <para>
       Dieses Verzeichnis wird von DeepSea verwendet. Erfasste Konfigurationsdaten werden hier gespeichert.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/salt/ceph</filename>
     </term>
     <listitem>
      <para>
       Ein von DeepSea verwendetes Verzeichnis. In ihm werden sls-Dateien gespeichert, die unterschiedliche Formate aufweisen, doch jedes Unterverzeichnis enthält sls-Dateien. Jedes Unterverzeichnis enthält nur einen Typ von sls-Datei. Beispiel: <filename>/srv/salt/ceph/stage</filename> enthält Orchestrierungsdateien, die durch <command>salt-run state.orchestrate</command> ausgeführt werden.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ds-minion-targeting">
   <title>Adressieren der Minions</title>
   <para>
    DeepSea-Kommandos werden über die Salt-Infrastruktur ausgeführt. Für das <command>salt</command>-Kommando müssen Sie eine Gruppe von Salt Minions angeben, auf die das Kommando zutreffen soll. Wir beschreiben diese Gruppe von Minions als <emphasis>target</emphasis> für das<command>salt</command>-Kommando. In den folgenden Abschnitten werden mögliche Methoden zum Adressieren der Minions beschrieben.
   </para>
   <sect3 xml:id="ds-minion-targeting-name">
    <title>Abgleichen des Minion-Namens</title>
    <para>
     Sie können einen Minion oder eine Gruppe von Minions adressieren, indem Sie deren Namen abgleichen. Der Name eines Minions ist normalerweise der kurze Hostname des Nodes, in dem die Minions ausgeführt werden. Diese Adressierungsmethode ist für Salt spezifisch und trifft nicht auf DeepSea zu. Sie können den Bereich der Minion-Namen durch Verwenden von Platzhaltern, reguläre Ausdrücke oder Listen eingrenzen. Im Allgemeinen sieht die Syntax folgendermaßen aus:
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> example.module</screen>
    <tip>
     <title>Nur im Ceph Cluster</title>
     <para>
      Wenn alle Salt Minions in Ihrer Umgebung zu Ihrem Ceph Cluster gehören, können Sie <replaceable>target</replaceable> problemlos durch <literal>"*"</literal> ersetzen, um <emphasis>alle</emphasis> registrierten Minions einzubeziehen.
     </para>
    </tip>
    <para>
     Gleichen Sie alle Minions in der Domäne "example.net" ab (unter der Annahme, dass alle Minion-Namen mit den "vollständigen" Hostnamen identisch sind):
    </para>
<screen><prompt>root@master # </prompt>salt '*.example.net' test.ping</screen>
    <para>
     Gleichen Sie die "web1"- bis "web5"-Minions ab:
    </para>
<screen><prompt>root@master # </prompt>salt 'web[1-5]' test.ping</screen>
    <para>
     Gleichen Sie sowohl die "web1-prod"- als auch die "web1-devel"-Minions mit einem regulären Ausdruck ab.
    </para>
<screen><prompt>root@master # </prompt>salt -E 'web1-(prod|devel)' test.ping</screen>
    <para>
     Gleichen Sie eine einfache Liste von Minions ab:
    </para>
<screen><prompt>root@master # </prompt>salt -L 'web1,web2,web3' test.ping</screen>
    <para>
     Gleichen Sie alle Minions im Cluster ab:
    </para>
<screen><prompt>root@master # </prompt>salt '*' test.ping</screen>
   </sect3>
   <sect3 xml:id="ds-minion-targeting-grain">
    <title>Adressieren mithilfe eines "deepsea" Grains</title>
    <para>
     In einer heterogenen Salt-verwalteten Umgebung, in der SUSE Enterprise Storage in einer Teilmenge von Nodes zusammen mit anderen Cluster-Lösungen bereitgestellt wurde, ist es eine gute Idee, die relevanten Minions zu "markieren", indem Sie ein "deepsea" Grain darauf anwenden. Auf diese Weise werden DeepSea Minions leicht in Umgebungen adressiert, in denen der Abgleich anhand es Minion-Namens problematisch ist.
    </para>
    <para>
     Führen Sie zum Anwenden des "deepsea" Grain auf eine Gruppe von Minions folgendes Kommando aus:
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> grains.append deepsea default</screen>
    <para>
     Führen Sie zum Entfernen des "deepsea" Grain von einer Gruppe von Minions folgendes Kommando aus:
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> grains.delval deepsea destructive=True</screen>
    <para>
     Nach Anwenden des "deepsea" Grain auf die relevanten Minions adressieren Sie diese wie folgt:
    </para>
<screen><prompt>root@master # </prompt>salt -G 'deepsea:*' test.ping</screen>
    <para>
     Das folgende Kommando funktioniert gleichermaßen:
    </para>
<screen><prompt>root@master # </prompt>salt -C 'G@deepsea:*' test.ping</screen>
   </sect3>
   <sect3 xml:id="ds-minion-targeting-dsminions">
    <title>Festlegen der Option <option>deepsea_minions</option></title>
    <para>
     Für die DeepSea-Bereitstellungen muss das Ziel der Option <option>deepsea_minions</option> festgelegt werden. DeepSea gibt den Minions während der Ausführung der Phasen darüber Anweisungen (weitere Informationen hierzu finden Sie in der <xref linkend="deepsea-stage-description"/>).
    </para>
    <para>
     Bearbeiten Sie zum Festlegen oder Ändern der Option <option>deepsea_minions</option> die Datei <filename>/srv/pillar/ceph/deepsea_minions.sls</filename> auf dem Salt Master und fügen Sie die folgende Zeile hinzu oder ersetzen Sie sie:
    </para>
<screen>deepsea_minions: <replaceable>target</replaceable></screen>
    <tip>
     <title><option>deepsea_minions</option> Target</title>
     <para>
      Als <replaceable>target</replaceable> für die Option <option>deepsea_minions</option> können Sie eine der beiden Adressierungsmethoden anwenden: <xref linkend="ds-minion-targeting-name" xrefstyle="select: title"/> und <xref linkend="ds-minion-targeting-grain" xrefstyle="select: title"/>.
     </para>
     <para>
      Gleichen Sie alle Salt Minions im Cluster ab:
     </para>
<screen>deepsea_minions: '*'</screen>
     <para>
      Gleichen Sie alle Minions mit "deepsea" Grain ab:
     </para>
<screen>deepsea_minions: 'G@deepsea:*'</screen>
    </tip>
   </sect3>
   <sect3>
    <title>Weiterführende Informationen</title>
    <para>
     Über die Salt-Infrastruktur können Sie anspruchsvollere Methoden zur Adressierung von Minions anwenden. Unter <link xlink:href="https://docs.saltstack.com/en/latest/topics/targeting/"/> finden Sie eine Beschreibung für alle Adressierungsmethoden.
    </para>
    <para>
     Auf der Handbuchseite "deepsea-minions" finden Sie außerdem weitere Informationen zur DeepSea-Adressierung (<command>man 7 deepsea_minions</command>).
    </para>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-install-stack">
  <title>Cluster-Bereitstellung</title>

  <para>
   Der Cluster-Bereitstellungsprozess besteht aus mehreren Phasen. Zunächst müssen Sie alle Nodes des Clusters durch Konfigurieren von Salt vorbereiten und dann Ceph bereitstellen und konfigurieren.
  </para>

  <tip>
   <title>Bereitstellen von Monitor Nodes ohne OSD-Profile zu definieren</title>
   <para>
    Wenn sie die Definition der OSD-Profile überspringen und die Monitor Nodes vorher bereitstellen müssen, können Sie dazu die Variable <option>DEV_ENV</option> festlegen. Sie ermöglicht die Bereitstellung von Monitors, auch wenn das Verzeichnis<filename>profile/</filename> nicht vorhanden ist, sowie die Bereitstellung eines Clusters mit mindestens <emphasis>einem</emphasis> Speicher-, Monitor- und Manager-Node.
   </para>
   <para>
    Zum Festlegen der Umgebungsvariable aktivieren Sie diese entweder global, indem Sie sie in Datei <filename>/srv/pillar/ceph/stack/global.yml</filename> festlegen, oder sie legen sie nur für die aktuelle Shell-Sitzung fest:
   </para>
<screen><prompt>root@master # </prompt>export DEV_ENV=true</screen>
  </tip>

  <para>
   Das folgende Verfahren beschreibt die Cluster-Vorbereitung im Detail.
  </para>

  <procedure>
   <step>
    <para>
     Installieren und registrieren Sie SUSE Linux Enterprise Server 12 SP3 zusammen mit der SUSE Enterprise Storage-Erweiterung auf jedem Node des Clusters.
    </para>
   </step>
   <step>
    <para>
     Verifizieren Sie, dass die entsprechenden Produkte installiert und registriert sind. Listen Sie dazu die bestehenden Software-Repositorys auf. Die Liste entspricht in etwa dieser Ausgabe:
    </para>
<screen>
 <prompt>root@minion &gt; </prompt>zypper lr -E
#  | Alias   | Name                              | Enabled | GPG Check | Refresh
---+---------+-----------------------------------+---------+-----------+--------
 4 | [...]   | SUSE-Enterprise-Storage-5-Pool    | Yes     | (r ) Yes  | No
 6 | [...]   | SUSE-Enterprise-Storage-5-Updates | Yes     | (r ) Yes  | Yes
 9 | [...]   | SLES12-SP3-Pool                   | Yes     | (r ) Yes  | No
11 | [...]   | SLES12-SP3-Updates                | Yes     | (r ) Yes  | Yes
</screen>
   </step>
   <step>
    <para>
     Konfigurieren Sie Netzwerkeinstellungen einschließlich der ordnungsgemäßen DNS-Namensauflösung auf jedem Node. Der Salt Master und alle Salt Minions müssen sich gegenseitig anhand ihrer Hostnamen auflösen. Weitere Informationen zum Konfigurieren eines Netzwerks finden Sie unter <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/sec_basicnet_yast.html"/>. Weitere Informationen zum Konfigurieren eines DNS Servers finden Sie unter <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/cha_dns.html"/>.
    </para>
   </step>
   <step>
    <para>
     Konfigurieren, aktivieren und starten Sie den NTP-Zeitsynchronisierungsserver:
    </para>
<screen><prompt>root@master # </prompt>systemctl enable ntpd.service
<prompt>root@master # </prompt>systemctl start ntpd.service</screen>
    <para>
     Weitere Informationen zum Einrichten von NTP finden Sie unter <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/sec_netz_xntp_yast.html"/>.
    </para>
   </step>
   <step>
    <para>
     Prüfen Sie, ob der AppArmor Service ausgeführt wird und deaktivieren Sie ihn auf jedem Cluster Node. Starten Sie das YaST AppArmor-Modul, wählen Sie <guimenu>Einstellungen</guimenu> aus und deaktivieren Sie dann das Kontrollkästchen für <guimenu>AppArmor aktivieren</guimenu>. Bestätigen Sie den Vorgang mit <guimenu>Fertig</guimenu>.
    </para>
    <para>
     Beachten Sie, dass SUSE Enterprise Storage <emphasis>nicht</emphasis> funktioniert, wenn AppArmor aktiviert ist.
    </para>
   </step>
   <step>
    <para>
     Installieren Sie die Pakete <literal>salt-master</literal> und <literal>salt-minion</literal> im Salt Master Node:
    </para>
<screen><prompt>root@master # </prompt>zypper in salt-master salt-minion</screen>
    <para>
     Überprüfen Sie, ob der <systemitem>salt-master</systemitem> Service aktiviert und gestartet wurde und aktivieren und starten Sie ihn gegebenenfalls:
    </para>
<screen><prompt>root@master # </prompt>systemctl enable salt-master.service
<prompt>root@master # </prompt>systemctl start salt-master.service</screen>
   </step>
   <step>
    <para>
     Falls Sie beabsichtigen, eine Firewall zu verwenden, verifizieren Sie, dass im Salt Master Node die Ports 4505 und 4506 für alle Salt Minion Nodes offen sind. Wenn die Ports geschlossen sind, können Sie diese mit dem Kommando <command>yast2 firewall</command> öffnen, indem Sie den <guimenu>SaltStack</guimenu> Service zulassen.
    </para>
    <warning>
     <title>DeepSea-Phasen werden bei aktiver Firewall nicht durchgeführt</title>
     <para>
      Die DeepSea-Phasen zur Bereitstellung werden nicht ausgeführt, wenn die Firewall aktiv ist (und sogar konfiguriert). Um die Phasen korrekt abzuschließen, müssen Sie entweder die Firewall durch Ausführen von
     </para>
<screen>
<prompt>root@master # </prompt>systemctl stop SuSEfirewall2.service
</screen>
     <para>
      ausschalten oder die Option <option>FAIL_ON_WARNING</option> in <filename>/srv/pillar/ceph/stack/global.yml</filename> auf "False" festlegen:
     </para>
<screen>
FAIL_ON_WARNING: False
</screen>
    </warning>
   </step>
   <step>
    <para>
     Installieren Sie das Paket <literal>salt-minion</literal> in allen Minion Nodes.
    </para>
<screen><prompt>root@minion &gt; </prompt>zypper in salt-minion</screen>
    <para>
     Vergewissern Sie sich, dass der <emphasis>vollqualifizierte Domänenname</emphasis> eines Nodes von allen anderen Nodes zur IP-Adresse des öffentlichen Netzwerks aufgelöst werden kann.
    </para>
   </step>
   <step>
    <para>
     Konfigurieren Sie alle Minions (einschließlich des Master Minion) zur Herstellung einer Verbindung zum Master. Wenn Ihr Salt Master mit dem Hostnamen <literal>salt</literal> nicht erreichbar ist, bearbeiten Sie die Datei <filename>/etc/salt/minion</filename> oder erstellen Sie eine neue Datei <filename>/etc/salt/minion.d/master.conf</filename> mit folgendem Inhalt:
    </para>
<screen>master: <replaceable>host_name_of_salt_master</replaceable></screen>
    <para>
     Wenn Sie an den oben genannten Konfigurationsdateien Änderungen vorgenommen haben, starten Sie den Salt Service auf allen Salt Minions neu:
    </para>
<screen><prompt>root@minion &gt; </prompt>systemctl restart salt-minion.service</screen>
   </step>
   <step>
    <para>
     Überprüfen Sie, ob der <systemitem>salt-minion</systemitem> Service in allen Nodes aktiviert und gestartet wurde. Aktivieren und starten Sie ihn, falls erforderlich:
    </para>
<screen><prompt>root@minion &gt; </prompt>systemctl enable salt-minion.service
<prompt>root@minion &gt; </prompt>systemctl start salt-minion.service</screen>
   </step>
   <step>
    <para>
     Verifizieren Sie den Fingerabdruck der einzelnen Salt Minions und akzeptieren Sie alle Salt-Schlüssel am Salt Master, wenn die Fingerabdrücke übereinstimmen.
    </para>
    <para>
     Zeigen Sie den Fingerabdruck der einzelnen Minions an:
    </para>
<screen><prompt>root@minion &gt; </prompt>salt-call --local key.finger
local:
3f:a3:2f:3f:b4:d3:d9:24:49:ca:6b:2c:e1:6c:3f:c3:83:37:f0:aa:87:42:e8:ff...</screen>
    <para>
     Nachdem Sie die Fingerabdrücke aller Salt Minions gesammelt haben, listen Sie die Fingerabdrücke aller nicht akzeptierten Minion-Schlüssel am Salt Master auf:
    </para>
<screen><prompt>root@master # </prompt>salt-key -F
[...]
Unaccepted Keys:
minion1:
3f:a3:2f:3f:b4:d3:d9:24:49:ca:6b:2c:e1:6c:3f:c3:83:37:f0:aa:87:42:e8:ff...</screen>
    <para>
     Wenn die Fingerabdrücke der Minions übereinstimmen, akzeptieren Sie sie:
    </para>
<screen><prompt>root@master # </prompt>salt-key --accept-all</screen>
   </step>
   <step>
    <para>
     Verifizieren Sie, dass die Schlüssel akzeptiert wurden:
    </para>
<screen><prompt>root@master # </prompt>salt-key --list-all</screen>
   </step>
   <step xml:id="deploy-wiping-disk">
    <para>
     Vergewissern Sie sich vor Bereitstellung von SUSE Enterprise Storage, dass alle Festplatten, die von früheren Clustern als OSD verwendet wurden, leer sind und keine Partitionen aufweisen. Um dies sicherzustellen, müssen Sie alle Festplatten manuell löschen. Denken Sie daran, "X" durch den korrekten Festplattenbuchstaben zu ersetzen:
    </para>
    <substeps>
     <step>
      <para>
       Stoppen Sie alle Prozesse, die die spezifische Festplatte verwenden.
      </para>
     </step>
     <step>
      <para>
       Verifizieren Sie, ob eine Partition auf der Festplatte eingehängt ist, und hängen Sie sie gegebenenfalls aus.
      </para>
     </step>
     <step>
      <para>
       Wenn die Festplatte von LVM verwaltet wird, deaktivieren und löschen Sie die gesamte LVM-Infrastruktur. Weitere Informationen finden Sie in <link xlink:href="https://www.suse.com/documentation/sles-12/stor_admin/data/cha_lvm.html"/>.
      </para>
     </step>
     <step>
      <para>
       Wenn die Festplatte Teil von MD RAID ist, deaktivieren Sie RAID. Weitere Informationen finden Sie in <link xlink:href="https://www.suse.com/documentation/sles-12/stor_admin/data/part_software_raid.html"/>.
      </para>
     </step>
     <step>
      <tip>
       <title>Server neustarten</title>
       <para>
        Wenn Sie in den folgenden Schritten Fehlermeldungen erhalten wie "Partition wird verwendet" oder "Kernel kann nicht mit der neuen Partitionstabelle aktualisiert werden", dann neustarten Sie den Server.
       </para>
      </tip>
      <para>
       Löschen Sie den Anfang jeder Partition:
      </para>
<screen>for partition in /dev/sdX[0-9]*
do
  dd if=/dev/zero of=$partition bs=4096 count=1 oflag=direct
done</screen>
     </step>
     <step>
      <para>
       Löschen Sie die Partitionstabelle:
      </para>
<screen>sgdisk -Z --clear -g /dev/sdX</screen>
     </step>
     <step>
      <para>
       Löschen Sie die gesicherten Partitionstabellen:
      </para>
<screen>size=`blockdev --getsz /dev/sdX`
position=$((size/4096 - 33))
dd if=/dev/zero of=/dev/sdX bs=4M count=33 seek=$position oflag=direct</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Installieren Sie DeepSea im Salt Master Node:
    </para>
<screen><prompt>root@master # </prompt>zypper in deepsea</screen>
   </step>
   <step>
    <para>
     Überprüfen Sie, ob die Datei <filename>/srv/pillar/ceph/master_minion.sls</filename> am Salt Master auf Ihren Salt Master verweist. Wenn Ihr Salt Master über mehrere Hostnamen erreichbar ist, verwenden Sie den Namen, der für den Speicher-Cluster am besten geeignet ist. Wenn Sie den Standard-Hostnamen für Ihren Salt Master (<emphasis>salt</emphasis>) in der <emphasis>ses</emphasis>-Domäne verwendet haben, dann sieht die Datei folgendermaßen aus:
    </para>
<screen>master_minion: salt.ses</screen>
   </step>
  </procedure>

  <para>
   Nun stellen Sie Ceph bereit und konfigurieren es. Alle Schritte sind obligatorisch, falls nicht anders angegeben.
  </para>

  <note>
   <title>Salt-Kommandokonventionen</title>
   <para>
    Sie haben zwei Möglichkeiten zum Ausführen von <command>salt-run state.orch</command>. Die eine ist mit <literal>stage.&lt;Phasennummer&gt;</literal>, bei der anderen wird der Name der Phase verwendet. Beide Schreibweisen haben dieselbe Wirkung. Es liegt ganz bei Ihnen, welches Kommando Sie verwenden.
   </para>
  </note>

  <procedure xml:id="ds-depl-stages">
   <title>Ausführen von Bereitstellungsphasen</title>
   <step>
    <para>
     Beziehen Sie die Salt Minions mit ein, die zum Ceph Cluster gehören, den Sie aktuell bereitstellen. In <xref linkend="ds-minion-targeting-name"/> finden Sie weitere Informationen zum Adressieren der Minions.
    </para>
   </step>
   <step>
    <para>
     Bereiten Sie Ihren Cluster vor. Weitere Informationen finden Sie in <xref linkend="deepsea-stage-description"/>.
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.0</screen>
    <para>
     oder
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.prep</screen>
    <note>
     <title>Phasen mit DeepSea CLI ausführen oder überwachen</title>
     <para>
      Mit DeepSea CLI können Sie den Fortschritt der Phasenausführung in Echtzeit verfolgen. Führen Sie dazu DeepSea CLI im Überwachungsmodus aus oder führen Sie die Phase direkt über DeepSea CLI aus. Weitere Informationen finden Sie im <xref linkend="deepsea-cli"/>.
     </para>
    </note>
   </step>
   <step>
    <para>
     <emphasis>Optional</emphasis>: Erstellen Sie Btrfs Sub-Volumes für <filename>/var/lib/ceph/</filename>. Dieser Schritt sollte erst nach Ausführung der nächsten Phasen von DeepSea ausgeführt werden. Informationen zum Migrieren der bestehenden Verzeichnisse oder weitere Details finden Sie im <xref linkend="storage-tips-ceph-btrfs-subvol"/>.
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.migrate.subvolume</screen>
   </step>
   <step>
    <para>
     In der Ermittlungsphase werden Daten von allen Minions erfasst und Konfigurationsfragmente erstellt, die im Verzeichnis <filename>/srv/pillar/ceph/proposals</filename> gespeichert sind. Die Daten werden im YAML-Format in SLS- oder YML-Dateien gespeichert.
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.1</screen>
    <para>
     oder
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.discovery</screen>
   </step>
   <step>
    <para>
     Erstellen Sie nach erfolgreicher Ausführung des vorigen Kommandos eine <filename>policy.cfg</filename>-Datei in <filename>/srv/pillar/ceph/proposals</filename>. Weitere Informationen finden Sie im <xref linkend="policy-configuration"/>.
    </para>
    <tip>
     <para>
      Wenn Sie die Netzwerkeinstellungen des Clusters ändern müssen, bearbeiten Sie die Datei <filename>/srv/pillar/ceph/stack/ceph/cluster.yml</filename> und passen Sie die Zeilen an, die mit <literal>cluster_network:</literal> und <literal>public_network:</literal> beginnen.
     </para>
    </tip>
   </step>
   <step>
    <para>
     In der Konfigurationsphase wird die <filename>policy.cfg</filename>-Datei analysiert und die enthaltenen Dateien in der finalen Form zusammengeführt. Auf Cluster und Rolle bezogene Inhalte werden in  <filename>/srv/pillar/ceph/cluster</filename> hinzugefügt, Ceph-spezifische Inhalte dagegen in <filename>/srv/pillar/ceph/stack/default</filename>.
    </para>
    <para>
     Führen Sie folgendes Kommando aus, um die Konfigurationsphase auszulösen:
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.2</screen>
    <para>
     oder
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.configure</screen>
    <para>
     Der Konfigurationsschritt kann einige Sekunden dauern. Nach Ausführung des Kommandos sehen Sie die Pillar-Daten für die angegebenen Minions (beispielsweise mit Namen <literal>ceph_minion1</literal>, <literal>ceph_minion2</literal> usw.), indem Sie folgendes Kommando ausführen:
    </para>
<screen><prompt>root@master # </prompt>salt 'ceph_minion*' pillar.items</screen>
    <note>
     <title>Überschreiben von Standardwerten</title>
     <para>
      Sobald das Kommando ausgeführt ist, sehen Sie die Standardkonfiguration und können sie entsprechend Ihrer Anforderungen ändern. Weitere Informationen finden Sie im <xref linkend="ceph-deploy-ds-custom"/>.
     </para>
    </note>
   </step>
   <step>
    <para>
     Nun führen Sie die Bereitstellungsphase aus. In dieser Phase wird der Pillar validiert und Monitors und ODS Daemons werden in den Speicher-Nodes gestartet. Führen Sie folgendes Kommando aus, um die Phase zu starten:
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.3</screen>
    <para>
     oder
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.deploy
    </screen>
    <para>
     Die Ausführung des Kommandos kann einige Minuten dauern. Wenn es nicht ausgeführt wird, müssen Sie das Problem beheben und die vorigen Phasen erneut ausführen. Führen Sie nach erfolgreicher Ausführung des Kommandos das folgende Kommando aus, um den Status zu prüfen:
    </para>
<screen><prompt>root@master # </prompt>ceph -s</screen>
   </step>
   <step>
    <para>
     Die letzte Phase der Ceph Cluster-Bereitstellung ist die <emphasis>Services</emphasis>-Phase. Hier instanziieren Sie die gewünschten Services, die aktuell unterstützt werden: iSCSI Gateway, CephFS, Object Gateway, openATTIC und NFS Ganesha. In dieser Phase werden die erforderlichen Pools, die autorisierenden Schlüsselbunde und Start-Services erstellt. Führen Sie folgendes Kommando aus, um die Phase zu starten:
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.4</screen>
    <para>
     oder
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.services</screen>
    <para>
     Je nach Einrichtung kann die Ausführung des Kommandos einige Minuten dauern.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="deepsea-cli">
  <title>DeepSea CLI</title>

  <para>
   DeepSea stellt auch eine Kommandozeilenschnittstelle (Command Line Interface, CLI) zur Verfügung, mit dem Benutzer Phasen überwachen oder ausführen, während sie den Ausführungsfortschritt in Echtzeit visualisieren.
  </para>

  <para>
   Zwei Modi werden zur Visualisierung des Ausführungsfortschritts einer Phase unterstützt:
  </para>

  <itemizedlist xml:id="deepsea-cli-modes">
   <title>DeepSea CLI-Modi</title>
   <listitem>
    <para>
     <emphasis role="bold">Überwachungsmodus</emphasis>: visualisiert den Ausführungsfortschritt einer DeepSea-Phase, die vom <command>salt-run</command>-Kommando ausgelöst wurde, das wiederum in einer anderen Terminalsitzung ausgestellt wurde.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Stand-Alone-Modus</emphasis>: führt eine DeepSea-Phase aus bei gleichzeitiger Echtzeit-Visualisierung der Komponentenschritte während ihrer Ausführung.
    </para>
   </listitem>
  </itemizedlist>

  <important>
   <title>DeepSea CLI-Kommandos</title>
   <para>
    Die DeepSea CLI-Kommandos können nur im Salt Master Node ausgeführt werden und es sind <systemitem class="username">root</systemitem>-Berechtigungen dazu erforderlich.
   </para>
  </important>

  <sect2 xml:id="deepsea-cli-monitor">
   <title>DeepSea CLI: Überwachungsmodus</title>
   <para>
    Die Fortschrittsüberwachung bietet eine detaillierte Echtzeit-Visualisierung der Vorgänge bei der Ausführung von Phasen mit <command>salt-run state.orch</command>-Kommandos in anderen Terminalsitzungen.
   </para>
   <para>
    Sie müssen die Überwachung starten, bevor Sie ein <command>salt-run state.orch</command>-Kommando ausführen, damit die Überwachung den Start der Phasenausführung erkennt.
   </para>
   <para>
    Wenn Sie die Überwachung nach Ausgabe des Kommandos <command>salt-run state.orch</command> starten, wird kein Ausführungsfortschritt angezeigt.
   </para>
   <para>
    Sie können den Überwachungsmodus durch Ausführen des folgenden Kommandos starten:
   </para>
<screen><prompt>root@master # </prompt>deepsea monitor</screen>
   <para>
    Weitere Informationen zu den verfügbaren Kommandozeilenoptionen des <command>deepsea monitor</command>-Kommandos finden Sie auf der entsprechenden Handbuchseite:
   </para>
<screen><prompt>root@master # </prompt>man deepsea-monitor</screen>
  </sect2>

  <sect2 xml:id="deepsea-cli-standalone">
   <title>DeepSea CLI: Stand-Alone-Modus</title>
   <para>
    Im Stand-Alone-Modus wird über DeepSea CLI eine DeepSea-Phase ausgeführt und die Ausführung wird in Echtzeit angezeigt.
   </para>
   <para>
    Das Kommando zur Ausführung einer DeepSea-Phase an der DeepSea CLI weist folgende Form auf:
   </para>
<screen><prompt>root@master # </prompt>deepsea stage run <replaceable>stage-name</replaceable></screen>
   <para>
    <replaceable>stage-name</replaceable> stellt dabei die Methode dar, wie die Zustandsdateien der Salt-Orchestrierung referenziert werden. Beispiel: Phase <emphasis role="bold">Ermittlung</emphasis>, die dem Verzeichnis in Pfad <filename>/srv/salt/ceph/stage/deploy</filename> entspricht, wird als <emphasis role="bold">ceph.stage.deploy</emphasis> referenziert.
   </para>
   <para>
    Dieses Kommando stellt eine Alternative zu den Salt-basierten Kommandos zur Ausführung von DeepSea-Phasen (oder von Zustandsdateien der DeepSea-Orchestrierung) dar.
   </para>
   <para>
    Das Kommando <command>deepsea stage run ceph.stage.0</command> entspricht <command>salt-run state.orch ceph.stage.0</command>.
   </para>
   <para>
    Weitere Informationen zu den verfügbaren Kommandozeilenoptionen, die vom <command>deepsea stage run</command>-Kommando akzeptiert werden, finden Sie auf der entsprechenden Handbuchseite:
   </para>
<screen><prompt>root@master # </prompt>man deepsea-stage run</screen>
   <para>
    Die folgende Abbildung zeigt ein Beispiel der Ausgabe der DeepSea CLI bei Ausführung von <emphasis role="underline">Phase 2</emphasis>:
   </para>
   <figure>
    <title>Ausgabe des Fortschritts der Phasenausführung an der DeepSea CLI</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="deepsea-cli-stage2-screenshot.png" width="70%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="deepsea-cli-stage2-screenshot.png" width="70%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <sect3 xml:id="deepsea-cli-run-alias">
    <title>DeepSea CLI-Alias für <command>stage run</command></title>
    <para>
     Für fortgeschrittene Benutzer von Salt unterstützen wir einen Alias für die Ausführung einer DeepSea-Phase. Dieser betrachtet das Salt-Kommando, das zur Ausführung einer Phase verwendet wird (beispielsweise <command>salt-run state.orch <replaceable>stage-name</replaceable></command>) als Kommando der DeepSea CLI.
    </para>
    <para>
     Beispiel:
    </para>
<screen><prompt>root@master # </prompt>deepsea salt-run state.orch <replaceable>stage-name</replaceable></screen>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="deepsea-pillar-salt-configuration">
  <title>Konfiguration und Anpassung</title>

  <sect2 xml:id="policy-configuration">
   <title>Die Datei <filename>policy.cfg</filename></title>
   <para>
    Mit der Konfigurationsdatei <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> werden Rollen einzelner Cluster Nodes ermittelt. Beispielsweise welcher Node als OSD fungiert oder welcher als Monitor Node. Bearbeiten Sie <filename>policy.cfg</filename>, um die gewünschte Cluster-Einrichtung widerzuspiegeln. Die Reihenfolge der Abschnitte ist beliebig, doch der Inhalt der enthaltenen Zeilen überschreibt die passenden Schlüssel vom Inhalt der vorigen Zeilen.
   </para>
   <tip>
    <title>Beispiele für <filename>policy.cfg</filename></title>
    <para>
     Sie finden verschiedene Beispiele von fertiggestellten Richtliniendateien im Verzeichnis <filename>/usr/share/doc/packages/deepsea/examples/</filename>.
    </para>
   </tip>
   <sect3 xml:id="policy-cluster-assignment">
    <title>Cluster-Zuweisung</title>
    <para>
     Im Abschnitt <emphasis role="bold">cluster</emphasis> wählen Sie Minions für Ihren Cluster aus. Sie können alle Minions auswählen oder Minions auf eine Blacklist oder Whitelist setzen. Beispiele für einen Cluster namens <emphasis role="bold">ceph</emphasis> folgen.
    </para>
    <para>
     Fügen Sie die folgenden Zeilen hinzu, um <emphasis role="bold">alle</emphasis> Minions einzubeziehen:
    </para>
<screen>cluster-ceph/cluster/*.sls</screen>
    <para>
     So setzen Sie einen bestimmten Minion auf eine <emphasis role="bold">weiße Liste</emphasis>:
    </para>
<screen>cluster-ceph/cluster/abc.domain.sls</screen>
    <para>
     oder eine Gruppe von Minions, mit Shell Glob-Abgleich:
    </para>
<screen>cluster-ceph/cluster/mon*.sls</screen>
    <para>
     Um Minions auf eine <emphasis role="bold">schwarze Liste</emphasis> zu setzen, legen Sie sie als <literal>unassigned</literal> (nicht zugewiesen) fest:
    </para>
<screen>cluster-unassigned/cluster/client*.sls</screen>
   </sect3>
   <sect3 xml:id="policy-role-assignment">
    <title>Rollenzuweisung</title>
    <para>
     In diesem Abschnitt erhalten Sie detaillierte Informationen zur Zuweisung von "Rollen" zu Ihren Cluster Nodes. Eine "Rolle" bezeichnet in diesem Kontext einen Service, den Sie zur Ausführung im Node benötigen, wie zum Beispiel Ceph Monitor, Object Gateway, iSCSI Gateway oder openATTIC. Keine Rolle wird automatisch zugewiesen. Es werden nur Rollen bereitgestellt, die zu <command>policy.cfg</command> hinzugefügt wurden.
    </para>
    <para>
     Die Zuweisung erfolgt nach dem folgenden Muster:
    </para>
<screen>role-<replaceable>ROLE_NAME</replaceable>/<replaceable>PATH</replaceable>/<replaceable>FILES_TO_INCLUDE</replaceable></screen>
    <para>
     Die einzelnen Elemente haben die folgende Bedeutung und Werte:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <replaceable>ROLE_NAME</replaceable> bezeichnet eines der folgenden Elemente: "master", "admin", "mon", "mgr", "mds", "igw", "rgw", "ganesha" oder "openattic".
      </para>
     </listitem>
     <listitem>
      <para>
       <replaceable>PATH</replaceable> ist ein relativer Verzeichnispfad zu SLS- oder YML-Dateien. Bei SLS-Dateien lautet er normalerweise <filename>cluster</filename>, YML-Dateien befinden sich unter <filename>stack/default/ceph/minions</filename>.
      </para>
     </listitem>
     <listitem>
      <para>
       <replaceable>FILES_TO_INCLUDE</replaceable> bezeichnet die Salt-Zustandsdateien oder die YAML-Konfigurationsdateien. Sie bestehen normalerweise aus den Hostnamen der Salt Minions, beispielsweise <filename>ses5min2.yml</filename>. Ein weiterer spezifischen Abgleich kann über Shell Globbing erfolgen.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Nachfolgend sehen Sie ein Beispiel für jede Rolle:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis>master</emphasis> - der Node hat Admin-Schlüsselbunde zu allen Ceph Clustern. Aktuell wird nur ein einzelner Ceph Cluster unterstützt. Da die <emphasis>master</emphasis>-Rolle obligatorisch ist, fügen Sie immer eine ähnliche Zeile wie die folgende hinzu:
      </para>
<screen>role-master/cluster/master*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>admin</emphasis> - der Minion verfügt über einen Admin-Schlüsselbund. Sie definieren die Rolle wie folgt:
      </para>
<screen>role-admin/cluster/abc*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>mon</emphasis> - der Minion stellt den Überwachungs-Service für den Ceph Cluster zur Verfügung. Diese Rolle benötigt Adressen der zugewiesenen Minions. Ab SUSE Enterprise Storage 5 werden die öffentlichen Adressen dynamisch berechnet und sind im Salt Pillar nicht mehr erforderlich.
      </para>
<screen>role-mon/cluster/mon*.sls</screen>
      <para>
       Im Beispiel wird die Überwachungsrolle einer Gruppe von Minions zugewiesen.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>mgr</emphasis> - der Ceph Manager Daemon, der alle Zustandsinformationen des gesamten Clusters erfasst. Stellen Sie ihn auf allen Minions bereit, auf denen Sie die Ceph Monitor-Rolle bereitstellen möchten.
      </para>
<screen>role-mgr/cluster/mgr*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>mds</emphasis> - der Minion stellt den Metadaten-Service zur Unterstützung von CephFS bereit.
      </para>
<screen>role-mds/cluster/mds*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>igw</emphasis> - der Minion fungiert als iSCSI Gateway. Diese Rolle benötigt Adressen der zugewiesenen Minions. Daher müssen Sie auch die Dateien aus dem <filename>stack</filename>-Verzeichnis hinzufügen:
      </para>
<screen>role-igw/stack/default/ceph/minions/xyz.domain.yml
role-igw/cluster/*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>rgw</emphasis> - der Minion fungiert als Object Gateway.
      </para>
<screen>role-rgw/cluster/rgw*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>openattic</emphasis> - der Minion fungiert als openATTIC Server:
      </para>
<screen>role-openattic/cluster/openattic*.sls</screen>
      <para>
       Weitere Informationen finden Sie unter <xref linkend="ceph-oa"/>.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>ganesha</emphasis> - der Minion fungiert als NFS Ganesha Server. Für die "ganesha"-Rolle ist eine "rgw"- oder "mds"-Rolle im Cluster erforderlich. Andernfalls wird die Validierung in Phase 3 nicht durchgeführt.
      </para>
      <para>
       Für die erfolgreiche Installation von NFS Ganesha ist eine weitere Konfiguration erforderlich. Wenn Sie NFS Ganesha verwenden möchten, lesen Sie <xref linkend="cha-as-ganesha"/>, bevor Sie Phase 2 und 4 ausführen. Es ist jedoch möglich, NFS Ganesha später zu installieren.
      </para>
      <para>
       In einigen Fällen kann es nützlich sein, benutzerdefinierte Rollen für NFS Ganesha Nodes zu definieren. Weitere Informationen finden Sie in <xref linkend="ceph-nfsganesha-customrole"/>.
      </para>
     </listitem>
    </itemizedlist>
    <note>
     <title>Mehrere Rollen von Cluster Nodes</title>
     <para>
      Sie können einem einzelnen Node verschiedene Rollen zuweisen. Beispielsweise können Sie die mds-Rollen zu Monitor Nodes zuweisen:
     </para>
<screen>role-mds/cluster/mon[1,2]*.sls</screen>
    </note>
   </sect3>
   <sect3 xml:id="policy-common-configuration">
    <title>Allgemeine Konfiguration</title>
    <para>
     Im Abschnitt zur allgemeinen Konfiguration sind Konfigurationsdateien enthalten, die in der Phase der <emphasis>Ermittlung (Phase 1)</emphasis> generiert wurden. In diesen Konfigurationsdateien werden Parameter wie <literal>fsid</literal> oder <literal>public_network</literal> gespeichert. Fügen Sie die folgenden Zeilen hinzu, um die erforderliche allgemeine Ceph-Konfiguration einzubeziehen:
    </para>
<screen>config/stack/default/global.yml
config/stack/default/ceph/cluster.yml</screen>
   </sect3>
   <sect3 xml:id="policy-profile-assignment">
    <title>Profilzuweisung</title>
    <para>
     In Ceph würde eine einzelne Speicherrolle nicht ausreichen, um die vielen Festplattenkonfigurationen zu beschreiben, die auf ein und derselben Hardware verfügbar sind. DeepSea Phase 1 generiert einen Vorschlag eines Standardspeicherprofils. Standardmäßig ist dieser Vorschlag ein <literal>bluestore</literal>-Profil. Er versucht, die Konfiguration mit der optimalen Leistung für die vorliegende  Hardware-Einrichtung vorzuschlagen. Beispielsweise werden externe Journale einer einzelnen Festplatte mit Objekten und Metadaten vorgezogen. Festkörperspeicher werden im Vergleich zu drehenden Festplatten priorisiert. Profile werden in der <filename>policy.cfg</filename>-Datei zugewiesen, ähnlich den Rollen.
    </para>
    <para>
     Der Standardvorschlag ist im "profile-default"-Verzeichnisbaum zu finden. Fügen Sie die folgenden zwei Zeilen zu <filename>policy.cfg</filename> hinzu, um diesen Vorschlag zu übernehmen. 
    </para>
<screen>profile-default/cluster/*.sls
profile-default/stack/default/ceph/minions/*.yml</screen>
    <para>
     Sie können auch ein eigenes benutzerdefiniertes Speicherprofil über die Vorschlagsausführung erstellen. Dieses Ausführungsprogramm bietet drei Methoden: "help", "peek" und "populate".
    </para>
    <para>
     <command>salt-run proposal.help</command> druckt den Hilfetext des Ausführungsprogramms zu den verschiedenen Argumenten, die akzeptiert werden.
    </para>
    <para>
     <command>salt-run proposal.peek</command> zeigt den generierten Vorschlag gemäß der übergebenen Argumente.
    </para>
    <para>
     <command>salt-run proposal.populate</command> schreibt den Vorschlag in das Unterverzeichnis <filename>/srv/pillar/ceph/proposals</filename>. Übergeben Sie <option>name=myprofile</option>, um das Speicherprofil zu benennen. Dadurch wird ein "profile-myprofile"-Unterverzeichnis erstellt.
    </para>
    <para>
     Sehen Sie sich bei allen anderen Argumenten die Ausgabe von <command>salt-run proposal.help</command> an.
    </para>
   </sect3>
   <sect3 xml:id="ds-profile-osd-encrypted">
    <title>Bereitstellen verschlüsselter OSDs</title>
    <para>
     Ab SUSE Enterprise Storage 5 werden OSDs standardmäßig mit BlueStore statt mit FileStore bereitgestellt. Obwohl BlueStore eine Verschlüsselung unterstützt, werden Ceph OSDs standardmäßig unverschlüsselt bereitgestellt. Nehmen wir an, dass sowohl die Daten-Festplatten als auch die WAL/DB-Festplatten, die für die OSD-Bereitstellung verwendet werden sollen, leer und nicht partitioniert sind. Falls die Festplatten vorher bereits verwendet wurden, löschen Sie sie vollständig anhand des in <xref linkend="deploy-wiping-disk"/> beschriebenen Verfahrens.
    </para>
    <para>
     Verwenden Sie das Ausführungsprogramm <literal>proposal.populate</literal> mit dem Argument <option>encryption=dmcrypt</option>, um verschlüsselte OSDs für Ihre neue Bereitstellung zu verwenden:
    </para>
<screen>
<prompt>root@master # </prompt>salt-run proposal.populate encryption=dmcrypt
</screen>
   </sect3>
   <sect3 xml:id="deepsea-policy-filtering">
    <title>Filtern von Elementen</title>
    <para>
     Manchmal ist es nicht praktisch, alle Dateien eines vorliegenden Verzeichnisses zum *.sls Globbing hinzuzufügen. Das Dateianalyseprogramm <filename>policy.cfg</filename> versteht die folgenden Filter:
    </para>
    <warning>
     <title>Erweiterte Methoden</title>
     <para>
      In diesem Abschnitt werden Filtermethoden für fortgeschrittene Benutzer beschrieben. Wenn die Filter nicht korrekt verwendet werden, können sie Probleme verursachen, beispielsweise wenn sich die Node-Nummerierung ändert.
     </para>
    </warning>
    <variablelist>
     <varlistentry>
      <term>slice=[start:end]</term>
      <listitem>
       <para>
        Mit dem Slice-Filter beziehen Sie nur die Elemente <emphasis>start</emphasis> bis <emphasis>end-1</emphasis> ein. Beachten Sie, dass die im vorliegenden Verzeichnis vorhandenen Elemente alphanumerisch sortiert sind. Die folgende Zeile enthält die dritte bis fünfzigste Datei des Unterverzeichnisses <filename>role-mon/cluster/</filename>:
       </para>
<screen>role-mon/cluster/*.sls slice[3:6]</screen>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>re=regexp</term>
      <listitem>
       <para>
        Mit dem regex Filter werden nur Elemente einbezogen, die den vorliegenden regulären Ausdrücken entsprechen. Beispiel:
       </para>
<screen>role-mon/cluster/mon*.sls re=.*1[135]\.subdomainX\.sls$</screen>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="deepsea-example-policy-cfg">
    <title>Beispiel einer <filename>policy.cfg</filename>-Datei</title>
    <para>
     Nachfolgend sehen Sie ein Beispiel einer einfachen <filename>policy.cfg</filename>-Datei:
    </para>
<screen>## Cluster Assignment
cluster-ceph/cluster/*.sls <co xml:id="co-policy-1"/>

## Roles
# ADMIN
role-master/cluster/examplesesadmin.sls <co xml:id="co-policy-2"/>
role-admin/cluster/sesclient*.sls <co xml:id="co-policy-3"/>

# MON
role-mon/cluster/ses-example-[123].sls <co xml:id="co-policy-5"/>

# MGR
role-mgr/cluster/ses-example-[123].sls <co xml:id="co-policy-mgr"/>

# MDS
role-mds/cluster/ses-example-4.sls <co xml:id="co-policy-6"/>

# IGW
role-igw/stack/default/ceph/minions/ses-example-4.yml <co xml:id="co-policy-7"/>
role-igw/cluster/ses-example-4.sls <co xml:id="co-policy-10"/>

# RGW
role-rgw/cluster/ses-example-4.sls <co xml:id="co-policy-11"/>

# openATTIC
role-openattic/cluster/openattic*.sls <co xml:id="co-policy-oa"/>

# COMMON
config/stack/default/global.yml <co xml:id="co-policy-8"/>
config/stack/default/ceph/cluster.yml <co xml:id="co-policy-13"/>

## Profiles
profile-default/cluster/*.sls <co xml:id="co-policy-9"/>
profile-default/stack/default/ceph/minions/*.yml <co xml:id="co-policy-12"/></screen>
    <calloutlist>
     <callout arearefs="co-policy-1">
      <para>
       Gibt an, dass alle Minions im Ceph Cluster enthalten sind. Wenn Sie über Minions verfügen, die nicht im Ceph Cluster enthalten sein sollen, verwenden Sie:
      </para>
<screen>cluster-unassigned/cluster/*.sls
cluster-ceph/cluster/ses-example-*.sls</screen>
      <para>
       Die erste Zeile kennzeichnet alle Minions als nicht zugewiesen. Die zweite Zeile überschreibt Minions, die "ses-example-*.sls" entsprechen, und weist sie dem Ceph Cluster zu.
      </para>
     </callout>
     <callout arearefs="co-policy-2">
      <para>
       Der Minion namens "examplesesadmin" verfügt über die "master"-Rolle. Dies bedeutet übrigens, dass er Admin-Schlüssel für den Cluster erhält.
      </para>
     </callout>
     <callout arearefs="co-policy-3">
      <para>
       Alle Minions, die "sesclient*" entsprechen, erhalten ebenfalls Admin-Schlüssel.
      </para>
     </callout>
     <callout arearefs="co-policy-5">
      <para>
       Alle Minions, die "ses-example-[123]" entsprechen (vermutlich drei Minions: ses-example-1, ses-example-2 und ses-example-3), werden als MON Nodes eingerichtet.
      </para>
     </callout>
     <callout arearefs="co-policy-mgr">
      <para>
       Alle Minions, die "ses-example-[123]" entsprechen (alle MON Nodes in diesem Beispiel), werden als MGR-Nodes eingerichtet.
      </para>
     </callout>
     <callout arearefs="co-policy-6">
      <para>
       Minion "ses-example-4" erhält die MDS-Rolle.
      </para>
     </callout>
     <callout arearefs="co-policy-7">
      <para>
       Stellt sicher, dass DeepSea die IP-Adresse des IGW Nodes kennt.
      </para>
     </callout>
     <callout arearefs="co-policy-10">
      <para>
       Minion "ses-example-4" erhält die IGW-Rolle.
      </para>
     </callout>
     <callout arearefs="co-policy-11">
      <para>
       Minion "ses-example-4" erhält die RGW-Rolle.
      </para>
     </callout>
     <callout arearefs="co-policy-oa">
      <para>
       Gibt vor, dass die openATTIC-Bedienoberfläche zur Verwaltung des Ceph Clusters bereitgestellt wird. Weitere Einzelheiten finden Sie im <xref linkend="ceph-oa"/>.
      </para>
     </callout>
     <callout arearefs="co-policy-8">
      <para>
       Bedeutet, dass wir die Standardwerte für allgemeine Konfigurationsparameter wie <option>fsid</option> und <option>public_network</option> akzeptieren.
      </para>
     </callout>
     <callout arearefs="co-policy-13">
      <para>
       Bedeutet, dass wir die Standardwerte für allgemeine Konfigurationsparameter wie <option>fsid</option> und <option>public_network</option> akzeptieren.
      </para>
     </callout>
     <callout arearefs="co-policy-9">
      <para>
       Wir weisen DeepSea an, das Standard-Hardwareprofil für jeden Minion zu verwenden. Die Wahl des Standard-Hardwareprofils bedeutet, dass wir alle anderen Festplatten (als die Root-Festplatte) als OSDs einrichten möchten.
      </para>
     </callout>
     <callout arearefs="co-policy-12">
      <para>
       Wir weisen DeepSea an, das Standard-Hardwareprofil für jeden Minion zu verwenden. Die Wahl des Standard-Hardwareprofils bedeutet, dass wir alle anderen Festplatten (als die Root-Festplatte) als OSDs einrichten möchten.
      </para>
     </callout>
    </calloutlist>
   </sect3>
  </sect2>

  <sect2>
   <title>Benutzerdefinierte Datei <filename>ceph.conf</filename></title>
   <para>
    Wenn Sie benutzerdefinierte Einstellungen zur Datei <filename>ceph.conf</filename> hinzufügen müssen, finden Sie die entsprechenden Informationen dazu im <xref linkend="ds-custom-cephconf"/>.
   </para>
  </sect2>
 </sect1>
</chapter>
