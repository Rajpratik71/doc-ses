<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_cephfs.xml" version="5.0" xml:id="cha-ceph-as-cephfs">

 <title>Installation des CephFS</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>Bearbeiten</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>Ja</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  Das Ceph-Dateisystem (CephFS) ist ein POSIX-fähiges Dateisystem, das seine Daten in einem Ceph Storage Cluster speichert. CephFS verwendet dasselbe Cluster-System wie Ceph-Blockgeräte, Ceph-Objektspeicher mit seinen S3 und Swift APIs, oder systemeigene Bindungen (<systemitem>librados</systemitem>).
 </para>
 <para>
  Zur Verwendung eines CephFS muss ein Ceph Storage Cluster und mindestens ein <emphasis>Ceph Metadata Server</emphasis> ausgeführt werden.
 </para>
 <sect1 xml:id="ceph-cephfs-limitations">
  <title>Unterstützte CephFS-Szenarios und Anleitungen</title>

  <para>
   Mit SUSE Enterprise Storage führt SUSE den offiziellen Support für viele Szenarios ein, in denen die dezentrale Scale-Out-Komponente CephFS verwendet wird. Dieser Eintrag beschreibt klare Grenzen und stellt Anleitungen für die vorgeschlagenen Anwendungsfälle zur Verfügung.
  </para>

  <para>
   Eine unterstützte CephFS-Bereitstellung muss die folgenden Anforderungen erfüllen:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Mindestens einen Metadatenserver. SUSE empfiehlt die Bereitstellung von mehreren Nodes mit der MDS-Rolle. Nur einer davon ist "aktiv", die anderen sind "passiv". Denken Sie daran, alle MDS Nodes im Kommando <command>mount</command> anzugeben, wenn Sie das CephFS von einem Client aus einhängen.
    </para>
   </listitem>
   <listitem>
    <para>
     CephFS Snapshots sind deaktiviert (Standardeinstellung) und werden in dieser Version nicht unterstützt.
    </para>
   </listitem>
   <listitem>
    <para>
     Clients basieren auf SUSE Linux Enterprise Server 12 SP2 oder SP3 und verwenden den Kernel-Modultreiber <literal>cephfs</literal>. Das FUSE-Modul wird nicht unterstützt.
    </para>
   </listitem>
   <listitem>
    <para>
     CephFS-Quoten werden in SUSE Enterprise Storage nicht unterstützt. Die Unterstützung für Quoten ist nur im FUSE Client implementiert.
    </para>
   </listitem>
   <listitem>
    <para>
     CephFS unterstützt Änderungen des Datei-Layouts wie unter <link xlink:href="http://docs.ceph.com/docs/jewel/cephfs/file-layouts/"/> dokumentiert. Weil das Dateisystem jedoch von einem beliebigen Client aus eingehängt wird, werden neue Daten-Pools möglicherweise nicht zu einem bestehenden CephFS-Dateisystem hinzugefügt (<literal>ceph mds add_data_pool</literal>). Sie dürfen nur hinzugefügt werden, wenn das Dateisystem ausgehängt ist.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="ceph-cephfs-mds">
  <title>Ceph Metadata Server</title>

  <para>
   Der Ceph Metadata Server (MDS) speichert Metadaten für das CephFS. Ceph-Blockgeräte und Ceph-Objektspeicher verwenden <emphasis>keinen</emphasis> MDS. MDSs ermöglichen es Benutzern eines POSIX-Dateisystems, einfache Kommandos auszuführen wie <command>ls</command> oder <command>find</command>, ohne dass der Ceph Storage Cluster übermäßig belastet wird.
  </para>

  <sect2 xml:id="ceph-cephfs-mdf-add">
   <title>Hinzufügen eines Metadatenservers</title>
   <para>
    Einen MDS stellen Sie im Zug der ersten Cluster-Bereitstellung bereit. Eine Beschreibung hierzu finden Sie in <xref linkend="ceph-install-stack"/>. Alternativ können Sie ihn auch zu einem bereits bereitgestellten Cluster hinzufügen wie in <xref linkend="salt-adding-nodes"/> beschrieben.
   </para>
   <para>
    Nach der Bereitstellung des MDS müssen Sie den <literal>Ceph OSD/MDS</literal>-Service in den Firewall-Einstellungen des Servers zulassen, auf dem der MDS bereitgestellt ist: Starten Sie <literal>yast</literal>, navigieren Sie zu<menuchoice> <guimenu>Security and Users (Sicherheit und Benutzer)</guimenu> <guimenu>Firewall</guimenu> <guimenu>Allowed Services (Zugelassene Services)</guimenu> </menuchoice> und wählen Sie im Dropdown-Menü <guimenu>Service to Allow (Zuzulassender Service)</guimenu> die Option <guimenu>Ceph OSD/MDS</guimenu> aus. Wenn im Ceph MDS Node kein umfassender Datenverkehr zugelassen ist, tritt beim Einhängen eines Dateisystems ein Fehler auf, auch wenn andere Operationen ordnungsgemäß funktionieren.
   </para>
  </sect2>

  <sect2 xml:id="ceph-cephfs-mds-config">
   <title>Konfigurieren eines Metadata Server</title>
   <para>
    Sie können das Verhalten des MDS durch Einfügen relevanter Optionen in der <filename>ceph.conf</filename>-Konfigurationsdatei weiter anpassen.
   </para>
   <variablelist>
    <title>Größe des MDS Cache</title>
    <varlistentry>
     <term><option>mds cache memory limit</option>
     </term>
     <listitem>
      <para>
       Das Softlimit des Arbeitsspeichers (in Byte), das der MDS für seinen Cache erzwingt. Administratoren sollten dies anstelle der alten Einstellung <option>mds cache size</option> verwenden. Der Standardwert beträgt 1 GB.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>mds cache reservation</option>
     </term>
     <listitem>
      <para>
       Die Cache-Reservierung (Arbeitsspeicher oder Inode) für den MDS Cache, die beibehalten werden soll. Wenn der MDS seinen reservierten Cache nahezu auslastet, stellt er den Client-Zustand vorübergehend wieder her, bis sich die Größe des Cache so weit verringert hat, dass die Reservierung wiederhergestellt wird. Der Standardwert ist 0,05.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Eine detaillierte Liste der MDS-Konfigurationsoptionen finden Sie unter <link xlink:href="http://docs.ceph.com/docs/master/cephfs/mds-config-ref/"/>.
   </para>
   <para>
    Eine detaillierte Liste der Konfigurationsoptionen der MDS-Journalerstellung finden Sie unter <link xlink:href="http://docs.ceph.com/docs/master/cephfs/journaler/"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs">
  <title>CephFS</title>

  <para>
   Wenn Ihr Ceph Storage Cluster mit mindestens einem Ceph Metadata Server ordnungsgemäß funktioniert, können Sie Ihr Ceph-Dateisystem erstellen und einhängen. Stellen Sie sicher, dass Ihr Client mit dem Netzwerk verbunden ist und über einen ordnungsgemäßen Schlüsselbund zur Authentifizierung verfügt.
  </para>

  <sect2 xml:id="ceph-cephfs-cephfs-create">
   <title>Erstellen eines CephFS</title>
   <para>
    Ein CephFS benötigt mindestens zwei RADOS-Pools: einen für <emphasis>Daten</emphasis> und einen für <emphasis>Metadaten</emphasis>. Beim Konfigurieren dieser Pools sollten Sie Folgendes in Erwägung ziehen:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Eine höhere Reproduktionsstufe für den Metadaten-Pool, da jeglicher Datenverlust in diesem Pool dazu führen kann, dass auf das gesamte Dateisystem nicht mehr zugegriffen werden kann.
     </para>
    </listitem>
    <listitem>
     <para>
      Ein Speicher mit geringerer Latenz für den Metadaten-Pool wie SSDs, weil dadurch die beobachtete Latenz der Dateisystemoperationen an Clients verbessert wird.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Bei der Zuweisung eines <literal>role-mds</literal> in Datei <filename>policy.cfg</filename> werden die erforderlichen Pools automatisch erstellt. Sie können vor dem Einrichten des Metadata Server die Pools <literal>cephfs_data</literal> und <literal>cephfs_metadata</literal> manuell erstellen, um die Leistung manuell anzupassen. Diese Pools werden von DeepSea nicht erstellt, wenn sie bereits vorhanden sind.
   </para>
   <para>
    Weitere Informationen zur Verwaltung von Pools finden Sie im <xref linkend="ceph-pools"/>.
   </para>
   <para>
    Führen Sie die folgenden Kommandos aus, um die beiden erforderlichen Pools (z. B. "cephfs_data" und "cephfs_metadata") mit Standardeinstellungen für das CephFS zu erstellen:
   </para>
<screen><prompt>root # </prompt>ceph osd pool create cephfs_data <replaceable>pg_num</replaceable>
<prompt>root # </prompt>ceph osd pool create cephfs_metadata <replaceable>pg_num</replaceable></screen>
   <para>
    Es ist möglich, EC-Pools anstelle von reproduzierten Pools zu verwenden. Wir empfehlen, EC-Pools nur für geringe Leistungsanforderungen und gelegentlichen zufälligen Zugriff zu verwenden, beispielsweise für Cold Storage, Sicherungen oder Archivierung. Für die Aktivierung von CephFS in EC-Pools ist BlueStore erforderlich und die Option <literal>allow_ec_overwrite</literal> muss für den Pool festgelegt sein. Diese Option kann durch Ausführung des Kommandos <command>ceph osd pool set ec_pool allow_ec_overwrites true</command> festgelegt werden.
   </para>
   <para>
    Ein Erasure Coding (EC) vergrößert erheblich den Overhead für Dateisystemoperationen, insbesondere kleine Updates. Dieser Overhead entsteht zwangsläufig, wenn Erasure Coding als Fehlertoleranzmechanismus verwendet wird. Diese Einbuße ist der Ausgleich für einen erheblich reduzierten Speicherplatz-Overhead.
   </para>
   <para>
    Wenn die Pools erstellt sind, können Sie das Dateisystem mit dem Kommando <command>ceph fs new</command> aktivieren:
   </para>
<screen><prompt>root # </prompt>ceph fs new <replaceable>fs_name</replaceable> <replaceable>metadata_pool_name</replaceable> <replaceable>data_pool_name</replaceable></screen>
   <para>
    Beispiel:
   </para>
<screen><prompt>root # </prompt>ceph fs new cephfs cephfs_metadata cephfs_data</screen>
   <para>
    Durch Auflisten aller verfügbarer CephFSs prüfen Sie, ob das Dateisystem erstellt wurde:
   </para>
<screen><prompt>root # </prompt><command>ceph</command> <option>fs ls</option>
 name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]</screen>
   <para>
    Wenn das Dateisystem erstellt wurde, kann der MDS den Zustand <emphasis>aktiv</emphasis> annehmen. Beispielsweise in einem einzelnen MDS-System:
   </para>
<screen><prompt>root # </prompt><command>ceph</command> <option>mds stat</option>
e5: 1/1/1 up</screen>
   <tip>
    <title>Weitere Themen</title>
    <para>
     Weitere Informationen zu spezifischen Aufgaben (beispielsweise Einhängen, Aushängen und erweiterte CephFS-Einrichtung) finden Sie im <xref linkend="cha-ceph-cephfs"/>.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-cephfs-multimds">
   <title>Größe des MDS Clusters</title>
   <para>
    Eine CephFS-Instanz kann von mehreren aktiven MDS Daemons bedient werden. Alle aktiven MDS Daemons, die einer CephFS-Instanz zugewiesen sind, teilen den Verzeichnisbaum des Dateisystems unter sich auf. Dadurch wird die Last gleichmäßig auf die gleichzeitig ausgeführten Clients verteilt. Zum Hinzufügen eines aktiven MDS Daemon zu einer CephFS-Instanz ist eine zusätzliche Standby-Instanz erforderlich. Starten Sie entweder einen zusätzlichen Daemon oder verwenden Sie eine vorhandene Standby-Instanz.
   </para>
   <para>
    Durch das folgende Kommando wird die aktuelle Anzahl der aktiven und passiven MDS Daemons angezeigt.
   </para>
<screen><prompt>root # </prompt>ceph mds stat</screen>
   <para>
    Durch die folgenden Kommandos wird die Anzahl der aktiven MDSs auf zwei pro Dateisysteminstanz festgelegt.
   </para>
<screen><prompt>root # </prompt>ceph fs set <replaceable>fs_name</replaceable> max_mds 2</screen>
   <para>
    Sie müssen zwei Schritte ausführen, um den MDS Cluster vor einem Update zu verkleinern. Legen Sie zunächst <option>max_mds</option> fest, damit nur noch eine Instanz vorhanden ist:
   </para>
<screen><prompt>root # </prompt>ceph fs set <replaceable>fs_name</replaceable> max_mds 1</screen>
   <para>
    Deaktivieren Sie danach explizit die anderen MDS Daemons:
   </para>
<screen><prompt>root # </prompt>ceph mds deactivate <replaceable>fs_name</replaceable>:<replaceable>rank</replaceable></screen>
   <para>
    Dabei bezeichnet <replaceable>rank</replaceable> die Nummer eines aktiven MDS Daemons einer Dateisysteminstanz im Bereich von 0 bis <option>max_mds</option>-1. Weitere Informationen hierzu finden Sie unter <link xlink:href="http://docs.ceph.com/docs/luminous/cephfs/multimds/"/>.
   </para>
  </sect2>

  <sect2 xml:id="ceph-cephfs-multimds-updates">
   <title>MDS Cluster und Updates</title>
   <para>
    Bei Ceph Updates ändern sich möglicherweise die Feature-Einstellungen (normalerweise durch Hinzufügen neuer Features). Nicht kompatible Daemons (wie die älteren Versionen) funktionieren nicht bei einem nicht kompatiblen Feature-Satz und starten nicht. Dies bedeutet, dass durch das Update und den Neustart eines Daemons alle anderen noch nicht aktualisierten Daemons möglicherweise anhalten und nicht mehr starten. Aus diesem Grund empfehlen wir, vor einem Update von Ceph den aktiven MDS Cluster auf die Größe einer Instanz zu verkleinern und alle Standby Daemons anzuhalten. Die manuellen Schritte für diesen Update-Vorgang sind wie folgt:
   </para>
   <procedure>
    <step>
     <para>
      Aktualisieren Sie die auf Ceph bezogenen Pakete mithilfe von <command>zypper</command>.
     </para>
    </step>
    <step>
     <para>
      Verkleinern Sie den MDS Cluster wie oben beschrieben auf eine Instanz und halten Sie alle MDS Standby Daemons an. Verwenden Sie dazu deren <systemitem class="daemon">systemd</systemitem>-Einheiten in allen anderen Nodes:
     </para>
<screen><prompt>root # </prompt>systemctl stop ceph-mds\*.service ceph-mds.target</screen>
    </step>
    <step>
     <para>
      Starten Sie erst dann den einzig verbleibenden MDS Daemon und verursachen Sie seinen Neustart anhand der aktualisierten Binärdatei.
     </para>
<screen><prompt>root # </prompt>systemctl restart ceph-mds\*.service ceph-mds.target</screen>
    </step>
    <step>
     <para>
      Starten Sie alle anderen MDS Daemons neu und legen Sie die gewünschte Einstellung <option>max_mds</option> neu fest.
     </para>
<screen><prompt>root # </prompt>systemctl start ceph-mds.target</screen>
    </step>
   </procedure>
   <para>
    Wenn Sie DeepSea verwenden, führt es diesen Vorgang aus, falls das
    <package>ceph</package> -Paket in Phase 0 und 4 aktualisiert wurde. Es ist möglich, diesen Vorgang auszuführen, während bei Clients die CephFS-Instanz eingehängt ist und E/A weiterhin ausgeführt wird. Beachten Sie jedoch, dass es beim Neustart des aktiven MDS eine kurze E/A-Pause gibt. Die Clients werden automatisch wiederhergestellt.
   </para>
   <para>
    Es hat sich bewährt, vor dem Aktualisieren eines MDS Clusters die E/A-Last so weit wie möglich zu reduzieren. Ein inaktiver MDS Cluster durchläuft diesen Update-Vorgang schneller. Umgekehrt ist es auf einem sehr ausgelasteten Cluster mit mehreren MDS Daemons sehr wichtig, die Last vorher zu reduzieren, um zu verhindern, dass ein einzelner MDS Daemon durch fortlaufende E/A-Vorgänge überlastet wird.
   </para>
  </sect2>
 </sect1>
</chapter>
