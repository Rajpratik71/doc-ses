<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_erasure.xml" version="5.0" xml:id="cha-ceph-erasure">
 <title>Erasure Coded Pools</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>Bearbeiten</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>Ja</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  Ceph bietet eine Alternative zur normalen Reproduktion von Daten in Pools, die als <emphasis>Erasure</emphasis> oder <emphasis>Erasure Coded</emphasis> Pool bezeichnet wird. Erasure Pools stellen nicht alle Funktionen von <emphasis>reproduzierten</emphasis> Pools bereit, benötigen jedoch auch weniger Basisspeicherplatz. Ein standardmäßiger Erasure Pool, der 1 TB Daten speichern kann, benötigt 1,5 TB Basisspeicherplatz. Im Vergleich dazu benötigt ein reproduzierter Pool einen Basisspeicherplatz von 2 TB für dieselbe Datenmenge.
 </para>
 <para>
  Hintergrundinformationen zu dem Begriff Erasure Code finden Sie unter <link xlink:href="https://en.wikipedia.org/wiki/Erasure_code"/>.
 </para>
 <note>
  <para>
   Wenn Sie mit FileStore arbeiten, haben Sie erst Zugriff auf Erasure Coded Pools über die RBD-Schnittstelle, wenn Sie eine Cache-Schicht konfigurieren. Weitere Details hierzu finden Sie in <xref linkend="ceph-tier-erasure"/>. Alternativ können Sie auch mit BlueStore arbeiten.
  </para>
 </note>
 <note>
  <para>
   Stellen Sie sicher, dass die CRUSH-Regeln für <emphasis>Erasure Pools</emphasis> <literal>indep</literal> für <literal>step</literal> verwenden. Weitere Informationen finden Sie in <xref linkend="datamgm-rules-step-mode"/>.
  </para>
 </note>
 <sect1 xml:id="cha-ceph-erasure-default-profile">
  <title>Erstellen eines Erasure Coded Pools für Testzwecke</title>

  <para>
   Der einfachste Erasure Coded Pool entspricht RAID5 und benötigt mindestens drei Hosts. Dieser Vorgang beschreibt die Erstellung eines Pools für Testzwecke.
  </para>
  <procedure>
   <step>
    <para>
     Mit dem Kommando <command>ceph osd pool create</command> wird ein Pool vom Typ <emphasis>erasure</emphasis> erstellt. Die Zahl <literal>12</literal> steht für die Anzahl der Placement Groups. Mit den Standardparametern kann der Pool den Ausfall eines OSD verarbeiten.
    </para>
<screen><prompt>root # </prompt>ceph osd pool create ecpool 12 12 erasure
pool 'ecpool' created</screen>
   </step>
   <step>
    <para>
     Die Zeichenkette <literal>ABCDEFGHI</literal> wird in ein Objekt namens <literal>NYAN</literal> geschrieben.
    </para>
<screen><prompt>cephadm &gt; </prompt>echo ABCDEFGHI | rados --pool ecpool put NYAN -</screen>
   </step>
   <step>
    <para>
     Für Testzwecke können die OSDs nun deaktiviert werden, zum Beispiel durch Trennen vom Netzwerk.
    </para>
   </step>
   <step>
    <para>
     Auf den Inhalt der Datei wird mit dem Kommando <command>rados</command> zugegriffen, um zu testen, ob der Pool den Ausfall von Geräten verarbeiten kann.
    </para>
<screen><prompt>root # </prompt>rados --pool ecpool get NYAN -
ABCDEFGHI</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="cha-ceph-erasure-erasure-profiles">
  <title>Erasure Code Profile</title>
  <para>Wird das Kommando <command>ceph osd pool create</command> zum Erstellen eines <emphasis>erasure pool</emphasis> aufgerufen, dann wird das Standardprofil verwendet, es sei denn ein anderes Profil wird angegeben. Profile definieren die Redundanz von Daten. Dies erfolgt durch Festlegen von zwei Parametern, die willkürlich als <literal>k</literal> und <literal>m</literal> bezeichnet werden. k und m definieren, in wie viele <literal>Datenblöcke</literal> eine bestimmte Datenmenge aufgeteilt und wie viele Codierungs-Datenblöcke erstellt werden. Redundante Datenblöcke werden dann auf verschiedenen OSDs gespeichert.
  </para>
  <para>
   Für Erasure Pool-Profile erforderliche Definitionen:
  </para>

  <variablelist>
   <varlistentry>
    <term>chunk</term>
    <listitem>
     <para>
      Wenn die Verschlüsselungsfunktion aufgerufen wird, gibt sie Datenblöcke der selben Größe zurück: Datenblöcke, die verkettet werden können, um das ursprüngliche Objekt zu rekonstruieren sowie Codierungs-Datenblöcke, mit denen ein verlorener Datenblock neu aufgebaut werden kann.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>k</term>
    <listitem>
     <para>
      Die Anzahl der Datenblöcke, d. h. die Anzahl der Blöcke, in die das ursprüngliche Objekt aufgeteilt wurde. Bei einem Wert von beispielsweise <literal>k = 2</literal> ist, wird ein Objekt von 10 KB in <literal>k</literal> Objekte zu je 5 KB aufgeteilt.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>m</term>
    <listitem>
     <para>
      Die Anzahl der Codierungs-Datenblöcke, d. h. die Anzahl der zusätzlichen Blöcke, die durch die Verschlüsselungsfunktionen berechnet werden. Bei 2 Codierungs-Datenblöcken können 2 OSDs ausfallen, ohne dass Daten verloren gehen.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>crush-failure-domain</term>
    <listitem>
     <para>
      Definiert, an welche Geräte die Datenblöcke verteilt werden. Ein Bucket-Typ muss als Wert festgelegt werden. Alle Bucket-Typen finden Sie in <xref linkend="datamgm-buckets"/>. Wenn die Fehlerdomäne <literal>rack</literal> lautet, werden die Datenblöcke in verschiedenen Racks gespeichert, um die Stabilität im Fall von Rack-Fehlern zu erhöhen.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   Mit dem in <xref linkend="cha-ceph-erasure-default-profile"/> verwendeten standardmäßigen Erasure Code Profil verlieren Sie keine Cluster-Daten, wenn ein einzelner OSD ausfällt. Daher benötigt es zum Speichern von 1 TB Daten weitere 0,5 TB Basisspeicherplatz. Dies bedeutet, dass 1,5 TB Basisspeicherplatz für 1 TB Daten benötigt werden. Dies entspricht einer allgemeinen RAID5-Konfiguration. Zum Vergleich: Ein reproduzierter Pool benötigt 2 TB Basisspeicherplatz zum Speichern von 1 TB Daten.
  </para>
  <para>Die Einstellungen des Standardprofils werden angezeigt mit:
  </para>

<screen><prompt>root # </prompt>ceph osd erasure-code-profile get default
directory=.libs
k=2
m=1
plugin=jerasure
crush-failure-domain=host
technique=reed_sol_van</screen>

  <para>
   Es ist wichtig, das richtige Profil zu wählen, weil es nach Erstellung des Pools nicht mehr geändert werden kann. Ein neuer Pool mit einem anderen Profil muss erstellt und alle Objekte müssen vom vorigen Pool in den neuen Pool verschoben werden.
  </para>

  <para>
   Die wichtigsten Parameter des Profils sind <literal>k</literal>, <literal>m</literal> und <literal>crush-failure-domain</literal>, weil sie den Speicher-Overhead und die Datenhaltbarkeit definieren. Wenn beispielsweise die gewünschte Architektur den Verlust von zwei Racks mit einem Speicher-Overhead von 66 % kompensieren muss, kann das folgende Profil definiert werden:
  </para>

<screen><prompt>root # </prompt>ceph osd erasure-code-profile set <replaceable>myprofile</replaceable> \
   k=3 \
   m=2 \
   crush-failure-domain=rack</screen>

  <para>
   Das Beispiel in <xref linkend="cha-ceph-erasure-default-profile"/> kann mit diesem neuen Profil wiederholt werden:
  </para>

<screen><prompt>root # </prompt>ceph osd pool create ecpool 12 12 erasure <replaceable>myprofile</replaceable>
<prompt>cephadm &gt; </prompt>echo ABCDEFGHI | rados --pool ecpool put NYAN -
<prompt>root # </prompt>rados --pool ecpool get NYAN -
ABCDEFGHI</screen>

  <para>
   Das NYAN-Objekt wird in drei Datenblöcke aufgeteilt (<literal>k=3</literal>) und zwei weitere Datenblöcke werden erstellt (<literal>m=2</literal>). Der Wert von <literal>m</literal> definiert, wie viele OSDs gleichzeitig ausfallen können, ohne dass Daten verloren gehen. Mit <literal>crush-failure-domain=rack</literal> wird ein CRUSH-Regelsatz erstellt, der sicherstellt, dass keine zwei Datenblöcke im selben Rack gespeichert werden.
  </para>

  <informalfigure>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ceph_erasure_obj.png" width="80%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ceph_erasure_obj.png" width="60%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </informalfigure>

  <para>
   Weitere Informationen zu Erasure Coded-Profilen finden Sie unter <link xlink:href="http://docs.ceph.com/docs/master/rados/operations/erasure-code-profile"/>.
  </para>
 </sect1>
 <sect1 xml:id="ceph-tier-erasure">
  <title>Erasure Coded Pool und Cache Tiering</title>

  <para>
   Erasure Coded Pools benötigen mehr Ressourcen als reproduzierte Pools. Ihnen fehlen außerdem einige Funktionen wie eingeschränkter Schreibzugriff. Wir empfehlen, eine Cache-Schicht vor dem Erasure Coded Pool zu erstellen, um diese Beschränkungen auszugleichen.
  </para>

  <para>
   Wenn beispielsweise der Pool <quote>hot-storage</quote>zum Schnellspeichern angelegt ist, dann kann der <quote>ecpool</quote>, der in <xref linkend="cha-ceph-erasure-erasure-profiles"/> erstellt wurde, beschleunigt werden mit:
  </para>

<screen><prompt>root # </prompt>ceph osd tier add ecpool hot-storage
<prompt>root # </prompt>ceph osd tier cache-mode hot-storage writeback
<prompt>root # </prompt>ceph osd tier set-overlay ecpool hot-storage</screen>

  <para>
   Dadurch wird der Pool <quote>hot-storage</quote> als Schicht von "ecpool" in den Zurückschreiben-Modus versetzt, sodass jeder Schreib- und Lesevorgang zu "ecpool" tatsächlich den "hot-storage"-Pool verwendet und von dessen Flexibilität und Geschwindigkeit profitiert.
  </para>

  <para>
   Mit FileStore ist es nicht möglich, ein RBD-Image zu einem Erasure Coded Pool zu erstellen, weil dazu eingeschränkter Schreibzugriff erforderlich ist. Es ist jedoch möglich, ein RBD-Image zu einem Erasure Coded Pool zu erstellen, wenn eine reproduzierte Pool-Schicht eine Cache-Schicht festgelegt hat:
  </para>

<screen><prompt>root # </prompt>rbd --pool ecpool create --size 10 myvolume</screen>

  <para>
   Weitere Informationen zum Cache Tiering finden Sie in <xref linkend="cha-ceph-tiered"/>.
  </para>
 </sect1>
 <sect1 xml:id="ec-rbd">
  <title>Erasure Coded Pools mit RADOS Block Device</title>
  <para>
   Setzen Sie ein entsprechendes Tag, wenn Sie einen EC Pool als RBD-Pool kennzeichnen möchten:
  </para>
<screen>
<prompt>root # </prompt>ceph osd pool application enable rbd <replaceable>ec_pool_name</replaceable>
</screen>
  <para>
  RBD kann Image-<emphasis>Daten</emphasis> in EC Pools speichern. Der Image Header und die Metadaten müssen jedoch weiterhin in einem reproduzierten Pool gespeichert werden. Nehmen wir an, Sie verfügen zu diesem Zweck über einen Pool namens "rbd":
  </para>
<screen>
<prompt>root # </prompt>rbd create rbd/<replaceable>image_name</replaceable> --size 1T --data-pool <replaceable>ec_pool_name</replaceable>
</screen>
  <para>
   Sie können das Image normalerweise wie jedes andere Image verwenden, außer dass alle Daten im Pool <replaceable>ec_pool_name</replaceable> gespeichert werden statt im "rbd"-Pool.
  </para>
 </sect1>
</chapter>
