<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_cephfs.xml" version="5.0" xml:id="cha-ceph-as-cephfs">

 <title>Instalação do CephFS</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>editando</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes (sim)</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  O sistema de arquivos Ceph (CephFS) é um sistema de arquivos compatível com POSIX que usa um cluster de armazenamento do Ceph para armazenar seus dados. O CephFS usa o mesmo sistema de cluster que os dispositivos de blocos do Ceph, o armazenamento de objetos do Ceph com APIs S3 e Swift ou as vinculações nativas (<systemitem>librados</systemitem>).
 </para>
 <para>
  Para usar o CephFS, você precisa ter um cluster de armazenamento do Ceph em execução e, no mínimo, um <emphasis>servidor de metadados Ceph</emphasis>.
 </para>
 <sect1 xml:id="ceph-cephfs-limitations">
  <title>Cenários e diretrizes suportados do CephFS</title>

  <para>
   Com o SUSE Enterprise Storage, a SUSE inclui suporte oficial para diversos cenários em que o componente distribuído e de expansão horizontal CephFS é usado. Essa entrada descreve os limites físicos e apresenta orientações para os casos de uso sugeridos.
  </para>

  <para>
   Uma implantação suportada do CephFS deve atender a estes requisitos:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     No mínimo, um Servidor de Metadados. A SUSE recomenda implantar vários nós com a função MDS. Apenas um será “ativo”, e o restante será “passivo”. Lembre-se de mencionar todos os nós MDS no comando <command>mount</command> ao montar o CephFS de um cliente.
    </para>
   </listitem>
   <listitem>
    <para>
     Os instantâneos do CephFS estão desabilitados (padrão) e não são suportados nesta versão.
    </para>
   </listitem>
   <listitem>
    <para>
     Os clientes são baseados no SUSE Linux Enterprise Server 12 SP2 ou SP3 e usam o driver do módulo do kernel <literal>cephfs</literal>. O módulo FUSE não é suportado.
    </para>
   </listitem>
   <listitem>
    <para>
     As cotas do CephFS não são suportadas no SUSE Enterprise Storage, já que o suporte a cotas foi implementado apenas no cliente do FUSE.
    </para>
   </listitem>
   <listitem>
    <para>
     O CephFS suporta mudanças de layout de arquivo, conforme documentado em <link xlink:href="http://docs.ceph.com/docs/jewel/cephfs/file-layouts/"/>. No entanto, embora o sistema de arquivos seja montado por qualquer cliente, novos pools de dados não podem ser adicionados a um sistema de arquivos CephFS existente (<literal>ceph mds add_data_pool</literal>). Eles apenas podem ser adicionados enquanto o sistema de arquivos está desmontado.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="ceph-cephfs-mds">
  <title>Servidor de metadados Ceph</title>

  <para>
   O servidor de metadados Ceph (MDS) armazena metadados para o CephFS. Os dispositivos de blocos e o armazenamento de objetos do Ceph <emphasis>não</emphasis> usam o MDS. Os MDSs permitem que os usuários do sistema de arquivos POSIX executem comandos básicos, como <command>ls</command> ou <command>find</command>, sem a necessidade de uma carga enorme no cluster de armazenamento do Ceph.
  </para>

  <sect2 xml:id="ceph-cephfs-mdf-add">
   <title>Adicionando um servidor de metadados</title>
   <para>
    Você pode implantar o MDS durante o processo inicial de implantação do cluster, conforme descrito na <xref linkend="ceph-install-stack"/>, ou adicioná-lo a um cluster já implantado, conforme descrito no <xref linkend="salt-adding-nodes"/>.
   </para>
   <para>
    Após implantar o MDS, permita o serviço <literal>Ceph OSD/MDS</literal> na configuração do firewall do servidor no qual o MDS está implantado: Inicie o <literal>yast</literal>, navegue até <menuchoice> <guimenu>Security and Users (Segurança e Usuários)</guimenu> <guimenu>Firewall</guimenu> <guimenu>Allowed Services (Serviços Permitidos)</guimenu> </menuchoice> e, no menu suspenso <guimenu>Service to Allow</guimenu> (Serviço para Permitir), selecione <guimenu>Ceph OSD/MDS</guimenu>. Se o nó MDS do Ceph não permitir tráfego completo, haverá falha na montagem do sistema de arquivos, mesmo que outras operações continuem funcionando apropriadamente.
   </para>
  </sect2>

  <sect2 xml:id="ceph-cephfs-mds-config">
   <title>Configurando um servidor de metadados</title>
   <para>
    Você pode ajustar o comportamento do MDS inserindo as opções relevantes no arquivo de configuração <filename>ceph.conf</filename>.
   </para>
   <variablelist>
    <title>Tamanho do cache do MDS</title>
    <varlistentry>
     <term><option>mds cache memory limit</option>
     </term>
     <listitem>
      <para>
       O limite flexível de memória (em bytes) que o MDS impõe ao cache. Os administradores devem usá-lo no lugar da configuração antiga <option>mds cache size</option>. O padrão é 1 GB.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>mds cache reservation</option>
     </term>
     <listitem>
      <para>
       A reserva de cache (memória ou inodes) para manutenção do cache do MDS. Quando o MDS começar a usar a reserva, ele revogará o estado do cliente até o tamanho do cache reduzir para restaurar a reserva. O padrão é 0,05.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Para obter uma lista detalhada das opções de configuração relacionadas ao MDS, consulte <link xlink:href="http://docs.ceph.com/docs/master/cephfs/mds-config-ref/"/>.
   </para>
   <para>
    Para obter uma lista detalhada das opções de configuração do gravador de diário do MDS, consulte <link xlink:href="http://docs.ceph.com/docs/master/cephfs/journaler/"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs">
  <title>CephFS</title>

  <para>
   Quando você tem um cluster de armazenamento do Ceph saudável com pelo menos um servidor de metadados Ceph, pode criar e montar o sistema de arquivos Ceph. Verifique se o seu cliente tem conectividade de rede e um chaveiro de autenticação apropriado.
  </para>

  <sect2 xml:id="ceph-cephfs-cephfs-create">
   <title>Criando o CephFS</title>
   <para>
    Um CephFS requer pelo menos dois pools RADOS: um para <emphasis>dados</emphasis> e outro para <emphasis>metadados</emphasis>. Ao configurar esses pools, considere o seguinte:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Uso de um nível mais alto de replicação para o pool de metadados, pois qualquer perda de dados nesse pool pode tornar todo o sistema de arquivos inacessível.
     </para>
    </listitem>
    <listitem>
     <para>
      Uso de armazenamento de latência menor, como SSDs, para o pool de metadados, pois isso melhora a latência observada das operações do sistema de arquivos nos clientes.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Ao atribuir um <literal>role-mds</literal> em <filename>policy.cfg</filename>, os pools necessários são criados automaticamente. Você pode criar manualmente os pools <literal>cephfs_data</literal> e <literal>cephfs_metadata</literal> para ajuste do desempenho manual antes de configurar o Servidor de Metadados. O DeepSea não criará esses pools se eles já existirem.
   </para>
   <para>
    Para obter mais informações sobre gerenciamento de pools, consulte o <xref linkend="ceph-pools"/>.
   </para>
   <para>
    Para criar os dois pools necessários (por exemplo, “cephfs_data” e “cephfs_metadata”) com as configurações padrão para uso com o CephFS, execute os seguintes comandos:
   </para>
<screen><prompt>root # </prompt>ceph osd pool create cephfs_data <replaceable>pg_num</replaceable>
<prompt>root # </prompt>ceph osd pool create cephfs_metadata <replaceable>pg_num</replaceable></screen>
   <para>
    É possível usar os pools EC em vez dos pools replicados. É recomendável usar apenas os pools EC para requisitos de baixo desempenho e acesso aleatório com pouca frequência. Por exemplo, armazenamento frio, backups, arquivamento. O CephFS em pools EC requer a habilitação do BlueStore, e o pool deve ter a opção <literal>allow_ec_overwrite</literal> definida. É possível definir essa opção executando <command>ceph osd pool set ec_pool allow_ec_overwrites true</command>.
   </para>
   <para>
    Apagar a codificação aumenta significativamente o overhead das operações do sistema de arquivos, principalmente de pequenas atualizações. Esse overhead é inerente ao uso do recurso de apagar codificação como mecanismo de tolerância a falhas. Essa desvantagem é compensada pela redução substancial do overhead de espaço de armazenamento.
   </para>
   <para>
    Quando os pools são criados, você pode habilitar o sistema de arquivos com o comando <command>ceph fs new</command>:
   </para>
<screen><prompt>root # </prompt>ceph fs new <replaceable>fs_name</replaceable> <replaceable>metadata_pool_name</replaceable> <replaceable>data_pool_name</replaceable></screen>
   <para>
    Por exemplo:
   </para>
<screen><prompt>root # </prompt>ceph fs new cephfs cephfs_metadata cephfs_data</screen>
   <para>
    Você pode verificar se o sistema de arquivos foi criado listando todos os CephFSs disponíveis:
   </para>
<screen><prompt>root # </prompt><command>ceph</command> <option>fs ls</option>
 name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]</screen>
   <para>
    Quando o sistema de arquivos for criado, o MDS poderá inserir um estado <emphasis>ativo</emphasis>. Por exemplo, em um único sistema MDS:
   </para>
<screen><prompt>root # </prompt><command>ceph</command> <option>mds stat</option>
e5: 1/1/1 up</screen>
   <tip>
    <title>Mais tópicos</title>
    <para>
     Você pode encontrar mais informações sobre tarefas específicas, por exemplo, montar, desmontar e configuração avançada do CephFS, no <xref linkend="cha-ceph-cephfs"/>.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-cephfs-multimds">
   <title>Tamanho do cluster MDS</title>
   <para>
    Vários daemons MDS ativos podem atender a uma instância do CephFS. Todos os daemons MDS ativos atribuídos a uma instância do CephFS distribuirão a árvore de diretório do sistema de arquivos entre eles e, portanto, dividirão a carga de clientes simultâneos. Para adicionar um daemon MDS ativo a uma instância do CephFS, é necessário um standby separado. Inicie um daemon adicional ou use uma instância de standby existente.
   </para>
   <para>
    O comando a seguir exibirá o número atual de daemons MDS ativos e passivos.
   </para>
<screen><prompt>root # </prompt>ceph mds stat</screen>
   <para>
    O comando a seguir define o número de MDS ativos como dois em uma instância do sistema de arquivos.
   </para>
<screen><prompt>root # </prompt>ceph fs set <replaceable>fs_name</replaceable> max_mds 2</screen>
   <para>
    Para reduzir o cluster MDS antes de uma atualização, duas etapas são necessárias. Primeiramente, defina <option>max_mds</option> de forma que permaneça apenas uma instância:
   </para>
<screen><prompt>root # </prompt>ceph fs set <replaceable>fs_name</replaceable> max_mds 1</screen>
   <para>
    e, em seguida, desative explicitamente os outros daemons MDS ativos:
   </para>
<screen><prompt>root # </prompt>ceph mds deactivate <replaceable>fs_name</replaceable>:<replaceable>rank</replaceable></screen>
   <para>
    em que <replaceable>rank</replaceable> é o número de um daemon MDS ativo de uma instância do sistema de arquivos, variando de 0 a <option>max_mds</option>-1. Consulte <link xlink:href="http://docs.ceph.com/docs/luminous/cephfs/multimds/"/> para obter informações adicionais.
   </para>
  </sect2>

  <sect2 xml:id="ceph-cephfs-multimds-updates">
   <title>Cluster e atualizações do MDS</title>
   <para>
    Durante as atualizações do Ceph, os flags de recursos em uma instância do sistema de arquivos podem mudar (normalmente, adicionando novos recursos). Os daemons incompatíveis (como os de versões mais antigas) não funcionam com um conjunto de recursos incompatíveis e se recusarão a iniciar. Isso significa que a atualização e a reinicialização de um daemon podem fazer com que todos os outros daemons ainda não atualizados parem e se recusem a iniciar. Por esse motivo, recomendamos reduzir o cluster MDS ativo para o tamanho 1 e parar todos os daemons de standby antes de atualizar o Ceph. Veja a seguir as etapas manuais para este procedimento de atualização:
   </para>
   <procedure>
    <step>
     <para>
      Atualize os pacotes relacionados ao Ceph usando o <command>zypper</command>.
     </para>
    </step>
    <step>
     <para>
      Reduza o cluster MDS ativo, conforme descrito acima, para 1 instância e pare todos os daemons MDS de standby usando as unidades <systemitem class="daemon">systemd</systemitem> deles em todos os outros nós:
     </para>
<screen><prompt>root # </prompt>systemctl stop ceph-mds\*.service ceph-mds.target</screen>
    </step>
    <step>
     <para>
      Somente depois disso, reinicie o único daemon MDS restante, o que faz com que ele seja reiniciado usando o binário atualizado.
     </para>
<screen><prompt>root # </prompt>systemctl restart ceph-mds\*.service ceph-mds.target</screen>
    </step>
    <step>
     <para>
      Reinicie todos os outros daemons MDS e redefina a configuração <option>max_mds</option> desejada.
     </para>
<screen><prompt>root # </prompt>systemctl start ceph-mds.target</screen>
    </step>
   </procedure>
   <para>
    Se você usa o DeepSea, ele seguirá esse procedimento se o pacote
    <package>ceph</package> for atualizado durante as Fases 0 e 4. É possível executar esse procedimento enquanto os clientes estão com a instância do CephFS montada e a E/S está em andamento. No entanto, observe que haverá uma pausa de E/S muito breve durante a reinicialização do MDS ativo. Os clientes serão recuperados automaticamente.
   </para>
   <para>
    Convém reduzir a carga de E/S o máximo possível antes de atualizar um cluster MDS. Um cluster MDS ocioso passará por esse procedimento de atualização mais rapidamente. Por outro lado, em um cluster excessivamente carregado com vários daemons MDS, é essencial reduzir a carga de maneira antecipada para evitar que um único daemon MDS fique sobrecarregado com E/S contínua.
   </para>
  </sect2>
 </sect1>
</chapter>
