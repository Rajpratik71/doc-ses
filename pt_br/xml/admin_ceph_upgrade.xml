<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_upgrade.xml" version="5.0" xml:id="cha-ceph-upgrade">
 <title>Fazendo upgrade de versões anteriores</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>editando</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes (sim)</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  Este capítulo apresenta as etapas de upgrade do SUSE Enterprise Storage de versões anteriores para a atual.
 </para>
 <sect1 xml:id="ceph-upgrade-relnotes">
  <title>Ler os detalhes da versão</title>

  <para>
   Nos detalhes da versão, você encontra mais informações sobre o que mudou desde a versão anterior do SUSE Enterprise Storage. Consulte os detalhes da versão para verificar se:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     seu hardware precisa de considerações especiais.
    </para>
   </listitem>
   <listitem>
    <para>
     qualquer pacote de software usado foi significativamente modificado.
    </para>
   </listitem>
   <listitem>
    <para>
     são necessárias precauções especiais para a instalação.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Os detalhes da versão também apresentam informações que não puderam ser incluídas a tempo no manual. Eles também incluem notas sobre problemas conhecidos.
  </para>

  <para>
   Após instalar o pacote <package>release-notes-ses</package>, encontre os detalhes da versão no diretório local <filename>/usr/share/doc/release-notes</filename> ou no site <link xlink:href="https://www.suse.com/releasenotes/"/>.
  </para>
 </sect1>
 <sect1 xml:id="ceph-upgrade-general">
  <title>Procedimento geral de upgrade</title>

  <para>
   Considere os seguintes itens antes de iniciar o procedimento de upgrade:
  </para>

  <variablelist>
   <varlistentry>
    <term>Ordem do upgrade</term>
    <listitem>
     <para>
      Antes de fazer upgrade do cluster do Ceph, você precisa registrar corretamente o SUSE Linux Enterprise Server e o SUSE Enterprise Storage subjacentes no SCC ou na SMT. Você poderá fazer upgrade dos daemons no cluster enquanto ele estiver online e em execução. Alguns tipos de daemons dependem de outros. Por exemplo, os Ceph Object Gateways dependem dos daemons Ceph Monitors e Ceph OSD. Recomendamos o upgrade nesta ordem:
     </para>
     <orderedlist spacing="normal">
      <listitem>
       <para>
        Ceph Monitors
       </para>
      </listitem>
      <listitem>
       <para>
        Ceph Managers
       </para>
      </listitem>
      <listitem>
       <para>
        Ceph OSDs
       </para>
      </listitem>
      <listitem>
       <para>
        Servidores de Metadados
       </para>
      </listitem>
      <listitem>
       <para>
        Object Gateways
       </para>
      </listitem>
      <listitem>
       <para>
        iSCSI Gateways
       </para>
      </listitem>
      <listitem>
       <para>
        NFS Ganesha
       </para>
      </listitem>
     </orderedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Apagar instantâneos desnecessários do sistema operacional</term>
    <listitem>
     <para>
      Remova os instantâneos de sistema de arquivos que não são necessários das partições de nós do sistema operacional. Esse procedimento garante espaço livre suficiente no disco durante o upgrade.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Verificar a saúde do cluster</term>
    <listitem>
     <para>
      É recomendável verificar a saúde do cluster antes de iniciar o procedimento de upgrade.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Fazer upgrade individualmente</term>
    <listitem>
     <para>
      Recomendamos fazer upgrade de todos os daemons de determinado tipo (por exemplo, todos os daemons monitor ou OSD) um de cada vez para garantir que todos sejam da mesma versão. Recomendamos também fazer upgrade de todos os daemons no cluster antes de você tentar usar uma nova funcionalidade em uma versão.
     </para>
     <para>
      Após o upgrade de todos os daemons de um tipo específico, verifique o status deles.
     </para>
     <para>
      Verifique se cada monitor reingressou no quorum após o upgrade de todos os monitores:
     </para>
<screen><prompt>root # </prompt>ceph mon stat</screen>
     <para>
      Verifique se cada daemon Ceph OSD reingressou no cluster após o upgrade de todos os OSDs:
     </para>
<screen><prompt>root # </prompt>ceph osd stat</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     Definir o flag <option>require-osd-release luminous</option>
    </term>
    <listitem>
     <para>
      Quando é feito o upgrade do último OSD para o SUSE Enterprise Storage 5, os nós do monitor detectam que todos os OSDs estão executando a versão “luminous” do Ceph e podem reclamar que o flag <option>require-osd-release luminous</option> do osdmap não está definido. Nesse caso, você precisa definir esse flag manualmente para confirmar que, agora que o upgrade do cluster foi feito para “luminous”, não é possível revertê-lo para a versão menos eficiente “jewel” do Ceph. Defina o flag executando o seguinte comando:
     </para>
<screen><prompt>root@minion &gt; </prompt>sudo ceph osd require-osd-release luminous</screen>
     <para>
      Após a conclusão do comando, o aviso desaparecerá.
     </para>
     <para>
      Nas novas instalações do SUSE Enterprise Storage 5, esse flag é definido automaticamente quando os Ceph Monitors criam o osdmap inicial. Dessa forma, nenhuma ação do usuário final é necessária.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="ds-migrate-osd-encrypted">
  <title>Criptografando OSDs durante o upgrade</title>

  <para>
   Desde o SUSE Enterprise Storage 5, por padrão, os OSDs são implantados usando o BlueStore em vez do FileStore. Embora o BlueStore suporte criptografia, os Ceph OSDs são implantados sem criptografia, por padrão. O procedimento a seguir descreve as etapas para criptografar os OSDs durante o processo de upgrade. Vamos supor que ambos os discos de dados e WAL/BD que serão usados para implantação do OSD estejam limpos, sem partições. Se os discos já foram usados, limpe-os seguindo o procedimento descrito na <xref linkend="deploy-wiping-disk"/>.
  </para>

  <important>
   <title>Um OSD de cada vez</title>
   <para>
    Você precisa implantar os OSDs criptografados um por um, não ao mesmo tempo. O motivo disso é que os dados do OSD são esvaziados, e o cluster passa por várias iterações de redistribuição.
   </para>
  </important>

  <procedure>
   <step>
    <para>
     Determine os valores <option>bluestore block db size</option> e <option>bluestore block wal size</option> da sua implantação e adicione-os ao arquivo <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/global.conf</filename> no master Salt. Os valores precisam ser especificados em bytes.
    </para>
<screen>
[global]
bluestore block db size = 48318382080
bluestore block wal size = 2147483648
</screen>
    <para>
     Para obter mais informações sobre como personalizar o arquivo <filename>ceph.conf</filename>, consulte o <xref linkend="ds-custom-cephconf"/>.
    </para>
   </step>
   <step>
    <para>
     Execute a Fase 3 do DeepSea para distribuir as mudanças:
    </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
</screen>
   </step>
   <step>
    <para>
     Verifique se o arquivo <filename>ceph.conf</filename> está atualizado nos nós relevantes do OSD:
    </para>
<screen>
<prompt>root@minion &gt; </prompt>cat /etc/ceph/ceph.conf
</screen>
   </step>
   <step>
    <para>
     Edite os arquivos *.yml no diretório <filename>/srv/pillar/ceph/proposals/profile-default/stack/default/ceph/minions</filename> relevantes aos OSDs que você está criptografando. Compare o caminho deles com o que foi definido no arquivo <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> para garantir que você modifique os arquivos *.yml corretos.
    </para>
    <important>
     <title>Identificadores de disco longo</title>
     <para>
      Ao identificar discos OSD nos arquivos <filename>/srv/pillar/ceph/proposals/profile-default/stack/default/ceph/minions/*.yml</filename>, use os identificadores de disco longo.
     </para>
    </important>
    <para>
     Veja a seguir um exemplo de configuração de OSD. Como a criptografia é necessária, as opções <option>db_size</option> e <option>wal_size</option> foram removidas:
    </para>
<screen>
ceph:
 storage:
   osds:
     /dev/disk/by-id/scsi-SDELL_PERC_H730_Mini_007027b1065faa972100d34d7aa06d86:
       format: bluestore
       encryption: dmcrypt
       db: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN
       wal: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN
     /dev/disk/by-id/scsi-SDELL_PERC_H730_Mini_00d146b1065faa972100d34d7aa06d86:
       format: bluestore
       encryption: dmcrypt
       db: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN
       wal: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN
</screen>
   </step>
   <step>
    <para>
     Implante os novos OSDs de Armazenamento em Blocos com criptografia executando as Fases 2 e 3 do DeepSea:
    </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.2
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
</screen>
    <para>
     Você poderá observar o andamento com <command>ceph -s</command> ou <command>ceph osd tree</command>. É essencial permitir que o cluster seja redistribuído antes de repetir o processo no próximo nó do OSD.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph-upgrade-4to5">
  <title>Upgrade do SUSE Enterprise Storage 4 (implantação do DeepSea) para 5</title>

  <important xml:id="u4to5-softreq">
   <title>Requisitos de software</title>
   <para>
    Você precisa ter o seguinte software instalado e atualizado para as versões mais recentes dos pacotes em todos os nós do Ceph dos quais você deseja fazer upgrade antes de iniciar o procedimento de upgrade:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      SUSE Linux Enterprise Server 12 SP2
     </para>
    </listitem>
    <listitem>
     <para>
      SUSE Enterprise Storage 4
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Além disso, antes de iniciar o upgrade, você precisa fazer upgrade do nó master Salt para o SUSE Linux Enterprise Server 12 SP3 e o SUSE Enterprise Storage 5 executando o comando <command>zypper migration</command> (ou seu método de upgrade preferencial).
   </para>
  </important>

  <warning>
   <title>Pontos de consideração antes do upgrade</title>
   <itemizedlist>
    <listitem>
     <para>
      Verifique se o serviço AppArmor está em execução e desabilite-o em cada nó do cluster. Inicie o módulo AppArmor do YaST, selecione <guimenu>Settings</guimenu> (Configurações) e, em seguida, desative a caixa de seleção <guimenu>Enable Apparmor</guimenu> (Habilitar Apparmor). Para confirmar, clique em <guimenu>Done</guimenu> (Concluído).
     </para>
     <para>
      O SUSE Enterprise Storage <emphasis>não</emphasis> funcionará com o AppArmor habilitado.
     </para>
    </listitem>
    <listitem>
     <para>
      Embora o cluster permaneça totalmente funcional durante o upgrade, o DeepSea define o flag "noout", que impede o Ceph de redistribuir os dados durante o tempo de espera e, portanto, evita transferências de dados desnecessárias.
     </para>
    </listitem>
    <listitem>
     <para>
      Para otimizar o processo de upgrade, o DeepSea faz upgrade dos nós na ordem, com base nas funções atribuídas, conforme recomendado pelo upstream do Ceph: MONs, MGRs, OSDs, MDS, RGW, IGW e NFS Ganesha.
     </para>
     <para>
      O DeepSea não poderá evitar que a ordem recomendada seja violada se um nó executar vários serviços.
     </para>
    </listitem>
    <listitem>
     <para>
      Embora o cluster do Ceph permaneça operacional durante o upgrade, os nós podem ser reinicializados para aplicar novas versões de kernel, por exemplo. Para reduzir as operações de E/S em espera, é recomendável recusar solicitações recebidas durante o processo de upgrade.
     </para>
    </listitem>
    <listitem>
     <para>
      O upgrade do cluster pode levar muito tempo: aproximadamente o tempo necessário para fazer upgrade de uma máquina multiplicado pelo número de nós do cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      A partir do Ceph Luminous, a opção de configuração <option>osd crush location</option> não é mais suportada. Atualize os arquivos de configuração do DeepSea para usar <command>crush location</command> antes do upgrade.
     </para>
    </listitem>
   </itemizedlist>
  </warning>

  <para>
   Para fazer upgrade do cluster do SUSE Enterprise Storage 4 para a versão 5, siga estas etapas:
  </para>

  <procedure>
   <step>
    <para>
     Defina a nova ordem de classificação de objetos internos. Execute:
    </para>
<screen><prompt>root # </prompt>ceph osd set sortbitwise</screen>
    <tip>
     <para>
      Para verificar se o comando foi bem-sucedido, recomendamos executar
     </para>
<screen><prompt>root # </prompt>ceph osd dump --format json-pretty | grep sortbitwise
 "flags": "sortbitwise,recovery_deletes,purged_snapdirs",</screen>
    </tip>
   </step>
   <step>
    <para>
     Usando o <command>rpm -q deepsea</command>, verifique se a versão do pacote do DeepSea no nó master Salt começa com <literal>0.7</literal> no mínimo. Por exemplo:
    </para>
<screen><prompt>root # </prompt>rpm -q deepsea
deepsea-0.7.27+git.0.274c55d-5.1</screen>
    <para>
     Se o número da versão do pacote do DeepSea começa com 0.6, verifique se você migrou com êxito o nó master Salt para o SUSE Linux Enterprise Server 12 SP3 e o SUSE Enterprise Storage 5 (consulte <xref linkend="u4to5-softreq"/> no início desta seção). Isso se trata de um pré-requisito que deve ser concluído antes de iniciar o procedimento de upgrade.
    </para>
   </step>
   <step>
    <substeps>
     <step>
      <para>
       Se você registrou seus sistemas com o SUSEConnect e usa SCC/SMT, nenhuma outra ação é necessária. Continue na <xref linkend="step-updatepillar"/>.
      </para>
     </step>
     <step>
      <para>
       Se você <emphasis role="bold">não</emphasis> usa SCC/SMT, mas uma ISO de Mídia ou outra fonte de pacote, adicione os seguintes repositórios manualmente: SLE12-SP3 Base, SLE12-SP3 Update, SES5 Base e SES5 Update. Para fazer isso, você pode usar o comando <command>zypper</command>. Primeiramente, remova todos os repositórios de software existentes, adicione os novos necessários e, por fim, atualize as fontes dos repositórios:
      </para>
<screen>
<prompt>root # </prompt>zypper sd {0..99}
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/Storage/5/x86_64/product/ SES5-POOL
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/Storage/5/x86_64/update/ SES5-UPDATES
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/SLE-SERVER/12-SP3/x86_64/product/ SLES12-SP3-POOL
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/SLE-SERVER/12-SP3/x86_64/update/ SLES12-SP3-UPDATES
<prompt>root # </prompt>zypper ref
</screen>
      <para>
       Na sequência, mude os dados do Pillar para usar uma estratégia diferente. Edite
      </para>
<screen>/srv/pillar/ceph/stack/<replaceable>name_of_cluster</replaceable>/cluster.yml</screen>
      <para>
       e adicione a linha a seguir:
      </para>
<screen>upgrade_init: zypper-dup</screen>
      <tip>
       <para>
        A estratégia <literal>zypper-dup</literal> requer que você adicione manualmente os repositórios de software mais recentes, enquanto a <literal>zypper-migration</literal> padrão utiliza os repositórios fornecidos pelo SCC/SMT.
       </para>
      </tip>
     </step>
    </substeps>
   </step>
   <step xml:id="step-updatepillar">
    <para>
     Atualize o Pillar:
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> saltutil.sync_all</screen>
    <para>
     Consulte a <xref linkend="ds-minion-targeting"/> para obter detalhes sobre o destino dos minions Salt.
    </para>
   </step>
   <step>
    <para>
     Verifique se você fez a gravação no Pillar com êxito:
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> pillar.get upgrade_init</screen>
    <para>
     A saída do comando deve espelhar a entrada que você adicionou.
    </para>
   </step>
   <step>
    <para>
     Faça upgrade dos minions Salt:
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> state.apply ceph.updates.salt</screen>
   </step>
   <step>
    <para>
     Verifique se foi feito o upgrade de todos os minions Salt:
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> test.version</screen>
   </step>
   <step>
    <para>
     Inclua os minions Salt do cluster. Consulte a <xref linkend="ds-minion-targeting"/> do <xref linkend="ds-depl-stages"/> para obter mais detalhes.
    </para>
   </step>
   <step>
    <para>
     Inicie o upgrade do SUSE Linux Enterprise Server e do Ceph:
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.maintenance.upgrade</screen>
    <tip>
     <title>Nova execução na reinicialização</title>
     <para>
      Se o processo resultar em uma reinicialização do master Salt, execute novamente o comando para iniciar outra vez o processo de upgrade dos minions Salt.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Verifique se o AppArmor está desabilitado e parado em todos os nós após o upgrade:
    </para>
<screen><prompt>root # </prompt>systemctl disable apparmor.service
systemctl stop apparmor.service</screen>
   </step>
   <step>
    <para>
     Após o upgrade, os Ceph Managers ainda não estarão instalados. Para atingir um estado saudável do cluster, faça o seguinte:
    </para>
    <substeps>
     <step>
      <para>
       Execute a Fase 0 para habilitar a API REST do Salt:
      </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.0</screen>
     </step>
     <step>
      <para>
       Execute a Fase 1 para criar o subdiretório <filename>role-mgr/</filename>:
      </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.1</screen>
     </step>
     <step>
      <para>
       Edite o <guimenu>policy.cfg</guimenu> conforme descrito na <xref linkend="policy-configuration"/> e adicione uma função do Ceph Manager aos nós em que os Ceph Monitors estão implantados. Adicione também a função do openATTIC a um dos nós do cluster. Consulte o <xref linkend="ceph-oa"/> para obter mais detalhes.
      </para>
     </step>
     <step>
      <para>
       Execute a Fase 2 para atualizar o Pillar:
      </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.2</screen>
     </step>
     <step>
      <para>
       Agora, o DeepSea usa uma abordagem diferente para gerar o arquivo de configuração <filename>ceph.conf</filename>. Consulte o <xref linkend="ds-custom-cephconf"/> para obter mais detalhes.
      </para>
     </step>
     <step>
      <para>
       Execute a Fase 3 para implantar Ceph Managers:
      </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.3</screen>
     </step>
     <step>
      <para>
       Execute a Fase 4 para configurar o openATTIC apropriadamente:
      </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.4</screen>
     </step>
    </substeps>
    <note>
     <title>Incompatibilidade de recursos de chave do Ceph</title>
     <para>
      Em caso de falha no <literal>ceph.stage.3</literal> com o erro: "Error EINVAL: entity client.bootstrap-osd exists but caps do not match", significa que os recursos da chave <literal>client.bootstrap.osd</literal> existente do cluster não são compatíveis com aqueles que o DeepSea está tentando definir. Acima da mensagem de erro, em vermelho, você pode ver um dump do comando <command>ceph auth</command> que falhou. Analise esse comando para verificar o ID da chave e o arquivo que estão sendo usados. No caso do <literal>client.bootstrap-osd</literal>, o comando será
     </para>
<screen><prompt>root # </prompt>ceph auth add client.bootstrap-osd \
 -i /srv/salt/ceph/osd/cache/bootstrap.keyring</screen>
     <para>
      Para corrigir recursos de chave incompatíveis, verifique o conteúdo do arquivo de chaveiro que o DeepSea está tentando implantar. Por exemplo:
     </para>
<screen><prompt>cephadm &gt; </prompt>cat /srv/salt/ceph/osd/cache/bootstrap.keyring
[client.bootstrap-osd]
     key = AQD6BpVZgqVwHBAAQerW3atANeQhia8m5xaigw==
     caps mgr = "allow r"
     caps mon = "allow profile bootstrap-osd"</screen>
     <para>
      Compare-o com a saída de <command>ceph auth get client.bootstrap-osd</command>:
     </para>
<screen><prompt>root # </prompt>ceph auth get client.bootstrap-osd
exported keyring for client.bootstrap-osd
[client.bootstrap-osd]
     key = AQD6BpVZgqVwHBAAQerW3atANeQhia8m5xaigw==
     caps mon = "allow profile bootstrap-osd"</screen>
     <para>
      Observe que <literal>caps mgr = "allow r”</literal> está ausente na última chave. Para corrigir isso, execute:
     </para>
<screen><prompt>root # </prompt>ceph auth caps client.bootstrap-osd mgr \
 "allow r" mon "allow profile bootstrap-osd"</screen>
     <para>
      Agora, a execução de <literal>ceph.stage.3</literal> deve ser bem-sucedida.
     </para>
     <para>
      O mesmo problema pode ocorrer com os chaveiros do Servidor de Metadados e do Object Gateway durante a execução do <literal>ceph.stage.4</literal>. O mesmo procedimento acima deve ser aplicado: verificar o comando que falhou, o arquivo de chaveiro que está sendo implantado e os recursos da chave existente. Em seguida, execute <command>ceph auth caps</command> para atualizar os recursos da chave existente para corresponder aos que o DeepSea está implantando.
     </para>
    </note>
   </step>
  </procedure>

  <important>
   <title>Falha no upgrade</title>
   <para>
    Se o estado do cluster permanecer como “HEALTH_ERR” por mais do que 300 segundos, ou se um dos serviços para cada função atribuída ficar inativo por mais do que 900 segundos, haverá falha no upgrade. Nesse caso, tente localizar o problema, resolva-o e execute o procedimento de upgrade novamente. Em ambientes virtualizados, observe que os tempos de espera são mais curtos.
   </para>
  </important>

  <important>
   <title>Reinicializando os OSDs</title>
   <para>
    Após o upgrade do SUSE Enterprise Storage 5, os OSDs do FileStore precisarão de aproximadamente cinco minutos a mais para serem iniciados, já que o OSD fará uma conversão única de seus arquivos no disco.
   </para>
  </important>

  <tip>
   <title>Verificar a versão dos componentes/nós do cluster</title>
   <para>
    Quando você precisa saber as versões dos componentes e nós individuais do cluster (por exemplo, para saber se todos os seus nós estão realmente no mesmo nível de patch após o upgrade), você pode executar
   </para>
<screen><prompt>root@master # </prompt>salt-run status.report</screen>
   <para>
    O comando analisa os minions Salt conectados, verifica os números de versão do Ceph, Salt e SUSE Linux Enterprise Server e apresenta um relatório a você com a versão da maioria dos nós e os nós que têm uma versão diferente da maioria.
   </para>
  </tip>

  <sect2 xml:id="filestore2bluestore">
   <title>Migração do OSD para BlueStore</title>
   <para>
    O OSD do BlueStore é um novo back end para os daemons OSD. Ele é a opção padrão desde o SUSE Enterprise Storage 5. Comparado com o FileStore, que armazena objetos como arquivos em um sistema de arquivos XFS, o BlueStore pode proporcionar melhor desempenho, porque armazena os objetos diretamente no dispositivo de blocos subjacente. O BlueStore também disponibiliza outros recursos, como compactação incorporada e sobregravações EC, que não estão disponíveis no FileStore.
   </para>
   <para>
    Especificamente no BlueStore, um OSD tem um dispositivo “wal” (Registro Write-Ahead) e um dispositivo “db” (banco de dados RocksDB). O banco de dados RocksDB armazena os metadados para um OSD do BlueStore. Por padrão, esses dois dispositivos residirão no mesmo dispositivo como um OSD, mas qualquer um deles pode ser inserido em uma mídia mais rápida ou diferente.
   </para>
   <para>
    No SES5, tanto o FileStore quanto o BlueStore são suportados, e os OSDs do FileStore e do BlueStore podem coexistir em um único cluster. Durante o procedimento de upgrade do SUSE Enterprise Storage, os OSDs do FileStore não são convertidos automaticamente em BlueStore. Os recursos específicos do BlueStore não estarão disponíveis nos OSDs que não foram migrados para o BlueStore.
   </para>
   <para>
    Antes da conversão em BlueStore, os OSDs precisam executar o SUSE Enterprise Storage 5. A conversão é um processo lento, pois todos os dados são regravados duas vezes. O processo de migração pode levar muito tempo para ser concluído, mas não há nenhuma interrupção do cluster, e todos os clientes podem continuar acessando o cluster durante esse período. No entanto, é comum uma redução no desempenho durante a migração. Isso é causado pela redistribuição e preenchimento de dados dos cluster.
   </para>
   <para>
    Siga o procedimento abaixo para migrar os OSDs do FileStore para o BlueStore:
   </para>
   <tip>
    <title>Desativar medidas de segurança</title>
    <para>
     Os comandos do Salt necessários para executar a migração são bloqueados por medidas de segurança. Para desativar essa medida preventiva, execute o seguinte comando:
    </para>
<screen>
<prompt>root@master # </prompt>salt-run disengage.safety
</screen>
   </tip>
   <procedure>
    <step>
     <para>
      Migre os perfis de hardware:
     </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.migrate.policy</screen>
     <para>
      Esse executor migra quaisquer perfis de hardware que o arquivo <filename>policy.cfg</filename> usa atualmente. Ele processa o <filename>policy.cfg</filename>, encontra qualquer perfil de hardware que usa a estrutura de dados original e converte-a na nova estrutura de dados. O resultado é um novo perfil de hardware chamado “migrated-<replaceable>nome_original</replaceable>”. O <filename>policy.cfg</filename> também é atualizado.
     </para>
     <para>
      Se a configuração original tinha diários separados, a configuração do BlueStore usará o mesmo dispositivo do “wal” e do “db” para esse OSD.
     </para>
    </step>
    <step>
     <para>
      O DeepSea migra os OSDs definindo o peso deles como 0, o que “aspira” os dados até esvaziar o OSD. Você pode migrar os OSDs um por um ou todos ao mesmo tempo. Em qualquer um dos casos, quando o OSD estiver vazio, a orquestração o removerá e o recriará com a nova configuração.
     </para>
     <tip>
      <title>Método recomendado</title>
      <para>
       Use <command>ceph.migrate.nodes</command> se você tiver um grande número de nós de armazenamento físico ou poucos dados. Se um nó representa menos do que 10% da sua capacidade, o <command>ceph.migrate.nodes</command> pode ser um pouco mais rápido para mover todos os dados desses OSDs em paralelo.
      </para>
      <para>
       Se você não tem certeza sobre qual método usar, ou se o site tem poucos nós de armazenamento (por exemplo, cada nó tem mais do que 10% dos dados do cluster), selecione <command>ceph.migrate.osds</command>.
      </para>
     </tip>
     <substeps>
      <step>
       <para>
        Para migrar os OSDs um de cada vez, execute:
       </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.migrate.osds</screen>
      </step>
      <step>
       <para>
        Para migrar todos os OSDs em cada nó em paralelo, execute:
       </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.migrate.nodes</screen>
      </step>
     </substeps>
     <tip>
      <para>
       Como a orquestração não apresenta comentários sobre o andamento da migração, use
      </para>
<screen><prompt>root # </prompt>ceph osd tree</screen>
      <para>
       para ver quais OSDs têm um peso de zero periodicamente.
      </para>
     </tip>
    </step>
   </procedure>
   <para>
    Após a migração para o BlueStore, o total de objetos permanecerá o mesmo, e a utilização do disco será quase igual.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-upgrade-4to5cephdeloy">
  <title>Upgrade do SUSE Enterprise Storage 4 (implantação do <command>ceph-deploy</command>) para 5</title>

  <important>
   <title>Requisitos de software</title>
   <para>
    Você precisa ter o seguinte software instalado e atualizado para as versões mais recentes dos pacotes em todos os nós do Ceph dos quais você deseja fazer upgrade antes de iniciar o procedimento de upgrade:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      SUSE Linux Enterprise Server 12 SP2
     </para>
    </listitem>
    <listitem>
     <para>
      SUSE Enterprise Storage 4
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Escolha o master Salt para seu cluster. Se o Calamari está implantado no cluster, o nó Calamari já <emphasis>é</emphasis> o master Salt. Como alternativa, o nó de admin do qual você executou o comando <command>ceph-deploy</command> se tornará o master Salt.
   </para>
   <para>
    Antes de iniciar o procedimento a seguir, você precisa fazer upgrade do nó master Salt para o SUSE Linux Enterprise Server 12 SP3 e o SUSE Enterprise Storage 5 executando o comando <command>zypper migration</command> (ou seu método de upgrade preferencial).
   </para>
  </important>

  <para>
   Para fazer upgrade do cluster do SUSE Enterprise Storage 4 que foi implantado com <command>ceph-deploy</command> para a versão 5, siga estas etapas:
  </para>

  <procedure xml:id="upgrade4to5cephdeploy-all">
   <title>Etapas para aplicar a todos os nós do cluster (incluindo o nó Calamari)</title>
   <step>
    <para>
     Instale o pacote <systemitem>salt</systemitem> do SLE-12-SP2/SES4:
    </para>
<screen><prompt>root # </prompt>zypper install salt</screen>
   </step>
   <step>
    <para>
     Instale o pacote <systemitem>salt-minion</systemitem> do SLE-12-SP2/SES4, habilite e inicie o serviço relacionado:
    </para>
<screen><prompt>root # </prompt>zypper install salt-minion
<prompt>root # </prompt>systemctl enable salt-minion
<prompt>root # </prompt>systemctl start salt-minion</screen>
   </step>
   <step>
    <para>
     Verifique se o nome de host “salt” é resolvido para o endereço IP do nó master Salt. Se o master Salt não puder ser acessado pelo nome de host <literal>salt</literal>, edite o arquivo <filename>/etc/salt/minion</filename> ou crie um novo arquivo <filename>/etc/salt/minion.d/master.conf</filename> com o seguinte conteúdo:
    </para>
<screen>master: <replaceable>host_name_of_salt_master</replaceable></screen>
    <tip>
     <para>
      Os minions Salt existentes já têm a opção <option>master:</option> definida em <filename>/etc/salt/minion.d/calamari.conf</filename>. O nome do arquivo de configuração não importa, o diretório <filename>/etc/salt/minion.d/</filename> é importante.
     </para>
    </tip>
    <para>
     Se você efetuou quaisquer mudanças nos arquivos de configuração mencionados acima, reinicie o serviço Salt em todos os minions Salt:
    </para>
<screen><prompt>root@minion &gt; </prompt>systemctl restart salt-minion.service</screen>
   </step>
   <step>
    <substeps>
     <step>
      <para>
       Se você registrou seus sistemas com o SUSEConnect e usa SCC/SMT, nenhuma outra ação é necessária.
      </para>
     </step>
     <step>
      <para>
       Se você <emphasis role="bold">não</emphasis> usa SCC/SMT, mas uma ISO de Mídia ou outra fonte de pacote, adicione os seguintes repositórios manualmente: SLE12-SP3 Base, SLE12-SP3 Update, SES5 Base e SES5 Update. Para fazer isso, você pode usar o comando <command>zypper</command>. Primeiramente, remova todos os repositórios de software existentes, adicione os novos necessários e, por fim, atualize as fontes dos repositórios:
      </para>
<screen>
<prompt>root # </prompt>zypper sd {0..99}
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/Storage/5/x86_64/product/ SES5-POOL
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/Storage/5/x86_64/update/ SES5-UPDATES
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/SLE-SERVER/12-SP3/x86_64/product/ SLES12-SP3-POOL
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/SLE-SERVER/12-SP3/x86_64/update/ SLES12-SP3-UPDATES
<prompt>root # </prompt>zypper ref
</screen>
     </step>
    </substeps>
   </step>
  </procedure>

  <procedure xml:id="upgrade4to5cephdeploy-admin">
   <title>Etapas para aplicar ao nó master Salt</title>
   <step>
    <para>
     Defina a nova de ordem de classificação de objetos internos. Execute:
    </para>
<screen><prompt>root@master # </prompt>ceph osd set sortbitwise</screen>
    <tip>
     <para>
      Para verificar se o comando foi bem-sucedido, recomendamos executar
     </para>
<screen><prompt>root@master # </prompt>ceph osd dump --format json-pretty | grep sortbitwise
 "flags": "sortbitwise,recovery_deletes,purged_snapdirs",</screen>
    </tip>
   </step>
   <step>
    <para>
     Faça upgrade do nó master Salt para o SUSE Linux Enterprise Server 12 SP3 e o SUSE Enterprise Storage 5. Para os sistemas registrados pelo SCC, use <command>zypper migration</command>. Se você inserir manualmente os repositórios de software necessários, use <command>zypper dup</command>. Após o upgrade, verifique se apenas os repositórios do SUSE Linux Enterprise Server 12 SP3 e do SUSE Enterprise Storage 5 estão ativos (e atualizados) no nó master Salt antes de prosseguir.
    </para>
   </step>
   <step>
    <para>
     Se ele ainda não estiver presente, instale o pacote <systemitem>salt-master</systemitem>, habilite e inicie o serviço relacionado:
    </para>
<screen><prompt>root@master # </prompt>zypper install salt-master
<prompt>root@master # </prompt>systemctl enable salt-master
<prompt>root@master # </prompt>systemctl start salt-master</screen>
   </step>
   <step>
    <para>
     Verifique a presença de todos os minions Salt listando suas chaves:
    </para>
<screen><prompt>root@master # </prompt>salt-key -L</screen>
   </step>
   <step>
    <para>
     Adicione todas as chaves dos minions Salt ao master Salt incluindo o master minion:
    </para>
<screen><prompt>root@master # </prompt>salt-key -A -y</screen>
   </step>
   <step>
    <para>
     Verifique se as chaves de todos os minions Salt foram aceitas:
    </para>
<screen><prompt>root@master # </prompt>salt-key -L</screen>
   </step>
   <step>
    <para>
     Confirme se o software em seu nó master Salt está atualizado:
    </para>
<screen><prompt>root@master # </prompt>zypper migration</screen>
   </step>
   <step>
    <para>
     Instale o pacote <systemitem>deepsea</systemitem>:
    </para>
<screen><prompt>root@master # </prompt>zypper install deepsea</screen>
   </step>
   <step>
    <para>
     Inclua os minions Salt do cluster. Consulte a <xref linkend="ds-minion-targeting"/> do <xref linkend="ds-depl-stages"/> para obter mais detalhes.
    </para>
   </step>
   <step>
    <para>
     Importe o cluster instalado pelo <command>ceph-deploy</command> existente:
    </para>
<screen><prompt>root@master # </prompt>salt-run populate.engulf_existing_cluster</screen>
    <para>
     O comando fará o seguinte:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Distribuirá todos os módulos do Salt e DeepSea necessários para todos os minions Salt.
      </para>
     </listitem>
     <listitem>
      <para>
       Inspecionará o cluster do Ceph em execução e preencherá <filename>/srv/pillar/ceph/proposals</filename> com um layout do cluster.
      </para>
      <para>
       O <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> será criado com as funções correspondentes a todos os serviços do Ceph em execução detectados. Veja esse arquivo para verificar se cada um dos nós MON, OSD, RGW e MDS existentes tem as funções apropriadas. Os nós OSD serão importados para o subdiretório <filename>profile-import/</filename>, portanto, você pode examinar os arquivos em <filename>/srv/pillar/ceph/proposals/profile-import/cluster/</filename> e <filename>/srv/pillar/ceph/proposals/profile-import/stack/default/ceph/minions/</filename> para confirmar se os OSDs foram capturados corretamente.
      </para>
      <note>
       <para>
        O <filename>policy.cfg</filename> gerado apenas aplicará as funções dos serviços do Ceph detectados “role-mon”, “role-mgr”, “role-mds”, “role-rgw”, “role-admin” e “role-master” referentes ao nó master Salt. Quaisquer outras funções desejadas deverão ser adicionadas manualmente ao arquivo (consulte a <xref linkend="policy-role-assignment"/>).
       </para>
      </note>
     </listitem>
     <listitem>
      <para>
       O <filename>ceph.conf</filename> do cluster existente será gravado em <filename>/srv/salt/ceph/configuration/files/ceph.conf.import</filename>.
      </para>
     </listitem>
     <listitem>
      <para>
       O <filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename> incluirá o fsid do cluster, o cluster e as redes públicas, além de especificar a opção <option>configuration_init: default-import</option>, que faz o DeepSea usar o arquivo de configuração <filename>ceph.conf.import</filename> mencionado anteriormente, em vez de usar o gabarito <filename>/srv/salt/ceph/configuration/files/ceph.conf.j2</filename> padrão do DeepSea.
      </para>
      <note>
       <title><filename>Ceph.conf</filename> personalizado</title>
       <para>
        Se você precisa integrar o arquivo <filename>ceph.conf</filename> com mudanças personalizadas, aguarde a conclusão bem-sucedida do processo de importação/upgrade. Em seguida, edite o arquivo <filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename> e comente na seguinte linha:
       </para>
<screen>
configuration_init: default-import
</screen>
       <para>
        Grave o arquivo e siga as informações no <xref linkend="ds-custom-cephconf"/>.
       </para>
      </note>
     </listitem>
     <listitem>
      <para>
       Os vários chaveiros do cluster serão gravados nos seguintes diretórios:
      </para>
<screen>/srv/salt/ceph/admin/cache/
/srv/salt/ceph/mon/cache/
/srv/salt/ceph/osd/cache/
/srv/salt/ceph/mds/cache/
/srv/salt/ceph/rgw/cache/</screen>
      <para>
       Verifique se os arquivos de chaveiro existem e se <emphasis>não</emphasis> há um arquivo de chaveiro no seguinte diretório (o Ceph Manager não existia antes do SUSE Enterprise Storage 5):
      </para>
<screen>
/srv/salt/ceph/mgr/cache/
</screen>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     O comando <command>salt-run populate.engulf_existing_cluster</command> não consegue importar a configuração do openATTIC. Você precisa editar manualmente o arquivo <filename>policy.cfg</filename> e adicionar uma linha <literal>role-openattic</literal>. Consulte a <xref linkend="policy-configuration"/> para obter mais detalhes.
    </para>
   </step>

   <step>
    <para>
     O comando <command>salt-run populate.engulf_existing_cluster</command> não consegue importar as configurações de iSCSI Gateways. Se o cluster inclui iSCSI Gateways, importe as configurações manualmente:
    </para>
    <substeps>
     <step>
      <para>
       Em um dos nós do iSCSI Gateway, exporte o <filename>lrbd.conf</filename> atual e copie-o para o nó master Salt:
      </para>
<screen>
<prompt>root@minion &gt; </prompt>lrbd -o &gt;/tmp/lrbd.conf
<prompt>root@minion &gt; </prompt>scp /tmp/lrbd.conf admin:/srv/salt/ceph/igw/cache/lrbd.conf
</screen>
     </step>
     <step>
      <para>
       No nó master Salt, adicione a configuração padrão do iSCSI Gateway à configuração do DeepSea:
      </para>
<screen>
<prompt>root@master # </prompt>mkdir -p /srv/pillar/ceph/stack/ceph/
<prompt>root@master # </prompt>echo 'igw_config: default-ui' &gt;&gt; /srv/pillar/ceph/stack/ceph/cluster.yml
<prompt>root@master # </prompt>chown salt:salt /srv/pillar/ceph/stack/ceph/cluster.yml
</screen>
     </step>
     <step>
      <para>
       Adicione as funções do iSCSI Gateway ao <filename>policy.cfg</filename> e grave o arquivo:
      </para>
<screen>
role-igw/stack/default/ceph/minions/ses-1.ses.suse.yml
role-igw/cluster/ses-1.ses.suse.sls
[...]
</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Execute a Fase 1 para criar todas as funções possíveis:
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.1</screen>
   </step>
   <step>
    <para>
     Gere os subdiretórios necessários em <filename>/srv/pillar/ceph/stack</filename>:
    </para>
<screen><prompt>root@master # </prompt>salt-run push.proposal</screen>
   </step>
   <step>
    <para>
     Verifique se existe um cluster gerenciado pelo DeepSea em execução com as funções corretamente atribuídas:
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> pillar.get roles</screen>
    <para>
     Compare a saída com o layout real do cluster.
    </para>
   </step>
   <step>
    <para>
     O Calamari deixa uma tarefa programada do Salt em execução para verificar o status do cluster. Remova a tarefa:
    </para>
<screen>
<prompt>root@minion &gt; </prompt>salt <replaceable>target</replaceable> schedule.delete ceph.heartbeat
</screen>
   </step>
   <step>
    <para>
     A partir deste ponto, siga o procedimento descrito na <xref linkend="ceph-upgrade-4to5"/>.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph-upgrade-4to5crowbar">
  <title>Upgrade do SUSE Enterprise Storage 4 (implantação do Crowbar) para 5</title>

  <important>
   <title>Requisitos de software</title>
   <para>
    Você precisa ter o seguinte software instalado e atualizado para as versões mais recentes dos pacotes em todos os nós do Ceph dos quais você deseja fazer upgrade antes de iniciar o procedimento de upgrade:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      SUSE Linux Enterprise Server 12 SP2
     </para>
    </listitem>
    <listitem>
     <para>
      SUSE Enterprise Storage 4
     </para>
    </listitem>
   </itemizedlist>
  </important>

  <para>
   Para fazer upgrade do SUSE Enterprise Storage 4 implantado por meio do Crowbar para a versão 5, siga estas etapas:
  </para>

  <procedure>
   <step>
    <para>
     Para cada nó do Ceph (incluindo o nó Calamari), interrompa e desabilite todos os serviços relacionados ao Crowbar:
    </para>
<screen>
<prompt>root@minion &gt; </prompt>sudo systemctl stop chef-client
<prompt>root@minion &gt; </prompt>sudo systemctl disable chef-client
<prompt>root@minion &gt; </prompt>sudo systemctl disable crowbar_join
<prompt>root@minion &gt; </prompt>sudo systemctl disable crowbar_notify_shutdown
</screen>
   </step>
   <step>
    <para>
     Para cada nó do Ceph (incluindo o nó Calamari), verifique se os repositórios de software apontam para os produtos SUSE Enterprise Storage 5 e SUSE Linux Enterprise Server 12 SP3. Se ainda houver repositórios apontando para versões mais antigas do produto, desabilite-os.
    </para>
   </step>
   <step>
    <para>
     Para cada nó do Ceph (incluindo o nó Calamari), verifique se o
     <package>salt-minion</package> está instalado. Se não estiver, instale-o:
    </para>
<screen><prompt>root@minion &gt; </prompt>sudo zypper in salt salt-minion</screen>
   </step>
   <step>
    <para>
     Para os nós do Ceph sem o pacote <package>salt-minion</package>
     instalado, crie o arquivo <filename>/etc/salt/minion.d/master.conf</filename> com a opção <option>master</option> apontando para o nome completo do host do nó Calamari:
    </para>
<screen>master: <replaceable>full_calamari_hostname</replaceable></screen>
    <tip>
     <para>
      Os minions Salt existentes já têm a opção <option>master:</option> definida em <filename>/etc/salt/minion.d/calamari.conf</filename>. O nome do arquivo de configuração não importa, o diretório <filename>/etc/salt/minion.d/</filename> é importante.
     </para>
    </tip>
    <para>
     Habilite e inicie o serviço <systemitem class="daemon">salt-minion</systemitem>:
    </para>
<screen>
<prompt>root@minion &gt; </prompt>sudo systemctl enable salt-minion
<prompt>root@minion &gt; </prompt>sudo systemctl start salt-minion
</screen>
   </step>
   <step>
    <para>
     No nó Calamari, aceite quaisquer chaves de minion Salt restantes:
    </para>
<screen>
<prompt>root@master # </prompt>salt-key -L
[...]
Unaccepted Keys:
d52-54-00-16-45-0a.example.com
d52-54-00-70-ac-30.example.com
[...]

<prompt>root@master # </prompt>salt-key -A
The following keys are going to be accepted:
Unaccepted Keys:
d52-54-00-16-45-0a.example.com
d52-54-00-70-ac-30.example.com
Proceed? [n/Y] y
Key for minion d52-54-00-16-45-0a.example.com accepted.
Key for minion d52-54-00-70-ac-30.example.com accepted.
</screen>
   </step>
   <step>
    <para>
     Se o Ceph foi implantado na rede pública, e não há uma interface VLAN presente, adicione uma interface VLAN na rede pública do Crowbar ao nó Calamari.
    </para>
   </step>
   <step>
    <para>
     Faça upgrade do nó Calamari para o SUSE Linux Enterprise Server 12 SP3 e o SUSE Enterprise Storage 5, usando o comando <command>zypper migration</command> ou o método de sua preferência. A partir deste ponto, o nó Calamari torna-se o <emphasis>master Salt</emphasis>. Após o upgrade, reinicialize o master Salt.
    </para>
   </step>
   <step>
    <para>
     Instale o DeepSea no master Salt:
    </para>
<screen><prompt>root@master # </prompt>zypper in deepsea</screen>
   </step>
   <step>
    <para>
     Especifique a opção <option>deepsea_minions</option> para incluir o grupo correto de minions Salt nas fases de implantação. Consulte a <xref linkend="ds-minion-targeting-dsminions"/> para obter mais detalhes.
    </para>
   </step>
   <step>
    <para>
     O DeepSea espera que todos os nós do Ceph tenham um <filename>/etc/ceph/ceph.conf</filename> idêntico. O Crowbar implanta um <filename>ceph.conf</filename> levemente diferente em cada nó, portanto, você precisa consolidá-lo:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Remova a opção <option>osd crush location hook</option> que foi incluída pelo Calamari.
      </para>
     </listitem>
     <listitem>
      <para>
       Remova a opção <option>public addr</option> da seção <literal>[mon]</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       Remova os números de porta da opção <option>mon host</option>.
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Se você estava executando o Object Gateway, o Crowbar implantou um arquivo <filename>/etc/ceph/ceph.conf.radosgw</filename> à parte para manter os segredos do Keystone separados do arquivo <filename>ceph.conf</filename> regular. O Crowbar também adicionou um arquivo <filename>/etc/systemd/system/ceph-radosgw@.service</filename> personalizado. Como o DeepSea não oferece suporte a ele, é necessário removê-lo:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Anexe todas as seções <literal>[client.rgw....]</literal> do arquivo <filename>ceph.conf.radosgw</filename> ao <filename>/etc/ceph/ceph.conf</filename> em todos os nós.
      </para>
     </listitem>
     <listitem>
      <para>
       No nó do Object Gateway, execute o seguinte:
      </para>
<screen><prompt>root@minion &gt; </prompt>rm /etc/systemd/system/ceph-radosgw@.service
systemctl reenable ceph-radosgw@rgw.public.$<replaceable>hostname</replaceable></screen>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Verifique se o <command>ceph status</command> funciona quando executado do master Salt:
    </para>
<screen><prompt>root@master # </prompt>ceph status
cluster a705580c-a7ae-4fae-815c-5cb9c1ded6c2
health HEALTH_OK
[...]
</screen>
   </step>
   <step>
    <para>
     Importe o cluster existente:
    </para>
<screen>
<prompt>root@master # </prompt>salt-run populate.engulf_existing_cluster
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.1
<prompt>root@master # </prompt>salt-run push.proposal
</screen>
   </step>

   <step>
    <para>
     O comando <command>salt-run populate.engulf_existing_cluster</command> não consegue importar as configurações de iSCSI Gateways. Se o cluster inclui iSCSI Gateways, importe as configurações manualmente:
    </para>
    <substeps>
     <step>
      <para>
       Em um dos nós do iSCSI Gateway, exporte o <filename>lrbd.conf</filename> atual e copie-o para o nó master Salt:
      </para>
<screen>
<prompt>root@minion &gt; </prompt>lrbd -o &gt; /tmp/lrbd.conf
<prompt>root@minion &gt; </prompt>scp /tmp/lrbd.conf admin:/srv/salt/ceph/igw/cache/lrbd.conf
</screen>
     </step>
     <step>
      <para>
       No nó master Salt, adicione a configuração padrão do iSCSI Gateway à configuração do DeepSea:
      </para>
<screen>
<prompt>root@master # </prompt>mkdir -p /srv/pillar/ceph/stack/ceph/
<prompt>root@master # </prompt>echo 'igw_config: default-ui' &gt;&gt; /srv/pillar/ceph/stack/ceph/cluster.yml
<prompt>root@master # </prompt>chown salt:salt /srv/pillar/ceph/stack/ceph/cluster.yml
</screen>
     </step>
     <step>
      <para>
       Adicione as funções do iSCSI Gateway ao <filename>policy.cfg</filename> e grave o arquivo:
      </para>
<screen>
role-igw/stack/default/ceph/minions/ses-1.ses.suse.yml
role-igw/cluster/ses-1.ses.suse.sls
[...]
</screen>
     </step>
    </substeps>
   </step>
   <step>
    <substeps>
     <step>
      <para>
       Se você registrou seus sistemas com o SUSEConnect e usa SCC/SMT, nenhuma outra ação é necessária.
      </para>
     </step>
     <step>
      <para>
       Se você <emphasis role="bold">não</emphasis> usa SCC/SMT, mas uma ISO de Mídia ou outra fonte de pacote, adicione os seguintes repositórios manualmente: SLE12-SP3 Base, SLE12-SP3 Update, SES5 Base e SES5 Update. Para fazer isso, você pode usar o comando <command>zypper</command>. Primeiramente, remova todos os repositórios de software existentes, adicione os novos necessários e, por fim, atualize as fontes dos repositórios:
      </para>
<screen>
<prompt>root # </prompt>zypper sd {0..99}
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/Storage/5/x86_64/product/ SES5-POOL
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/Storage/5/x86_64/update/ SES5-UPDATES
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/SLE-SERVER/12-SP3/x86_64/product/ SLES12-SP3-POOL
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/SLE-SERVER/12-SP3/x86_64/update/ SLES12-SP3-UPDATES
<prompt>root # </prompt>zypper ref
</screen>
      <para>
       Na sequência, mude os dados do Pillar para usar uma estratégia diferente. Editar
      </para>
<screen>/srv/pillar/ceph/stack/<replaceable>name_of_cluster</replaceable>/cluster.yml</screen>
      <para>
       e adicione a linha a seguir:
      </para>
<screen>upgrade_init: zypper-dup</screen>
      <tip>
       <para>
        A estratégia <literal>zypper-dup</literal> requer que você adicione manualmente os repositórios de software mais recentes, enquanto a <literal>zypper-migration</literal> padrão utiliza os repositórios fornecidos pelo SCC/SMT.
       </para>
      </tip>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Corrija os grains do host para fazer com que o DeepSea use nomes de host curtos na rede pública para os IDs de instância do daemon Ceph. Para cada nó, você precisa executar <command>grains.set</command> com o novo nome de host (curto). Antes de executar <command>grains.set</command>, verifique as instância atuais do monitor usando o comando <command>ceph status</command>. Veja a seguir um exemplo de antes e depois:
    </para>
<screen>
<prompt>root@master # </prompt>salt <replaceable>target</replaceable> grains.get host
d52-54-00-16-45-0a.example.com:
    d52-54-00-16-45-0a
d52-54-00-49-17-2a.example.com:
    d52-54-00-49-17-2a
d52-54-00-76-21-bc.example.com:
    d52-54-00-76-21-bc
d52-54-00-70-ac-30.example.com:
    d52-54-00-70-ac-30
</screen>
<screen>
<prompt>root@master # </prompt>salt d52-54-00-16-45-0a.example.com grains.set \
 host public.d52-54-00-16-45-0a
<prompt>root@master # </prompt>salt d52-54-00-49-17-2a.example.com grains.set \
 host public.d52-54-00-49-17-2a
<prompt>root@master # </prompt>salt d52-54-00-76-21-bc.example.com grains.set \
 host public.d52-54-00-76-21-bc
<prompt>root@master # </prompt>salt d52-54-00-70-ac-30.example.com grains.set \
 host public.d52-54-00-70-ac-30
</screen>
<screen>
<prompt>root@master # </prompt>salt <replaceable>target</replaceable> grains.get host
d52-54-00-76-21-bc.example.com:
    public.d52-54-00-76-21-bc
d52-54-00-16-45-0a.example.com:
    public.d52-54-00-16-45-0a
d52-54-00-49-17-2a.example.com:
    public.d52-54-00-49-17-2a
d52-54-00-70-ac-30.example.com:
    public.d52-54-00-70-ac-30
</screen>
   </step>
   <step>
    <para>
     Execute o upgrade:
    </para>
<screen>
<prompt>root@master # </prompt>salt <replaceable>target</replaceable> state.apply ceph.updates
<prompt>root@master # </prompt>salt <replaceable>target</replaceable> test.version
<prompt>root@master # </prompt>salt-run state.orch ceph.maintenance.upgrade
</screen>
    <para>
     Todos os nós serão reinicializados. O cluster retornará com a reclamação de que não há uma instância ativa do Ceph Manager. Isso costuma acontecer. Neste ponto, o Calamari não deve mais ser instalado/estar em execução.
    </para>
   </step>
   <step>
    <para>
     Execute todas as fases de implantação necessárias para atingir um estado saudável do cluster:
    </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.0
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.1
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.2
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
</screen>
   </step>
   <step>
    <para>
     Para implantar o openATTIC (consulte o <xref linkend="ceph-oa"/>), adicione a linha <literal>role-openattic</literal> apropriada (consulte a <xref linkend="policy-role-assignment"/>) ao <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>. Em seguida, execute:
    </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.2
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.4
</screen>
   </step>
   <step>
    <para>
     Durante o upgrade, você pode receber erros do tipo: "Error EINVAL: entity [...] exists but caps do not match". Para resolvê-los, consulte a <xref linkend="ceph-upgrade-4to5"/>.
    </para>
   </step>
   <step>
    <para>
     Efetue a limpeza restante:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       O Crowbar cria entradas em <filename>/etc/fstab</filename> para cada OSD. Elas não são necessárias, portanto, apague-as.
      </para>
     </listitem>
     <listitem>
      <para>
       O Calamari deixa uma tarefa programada do Salt em execução para verificar o status do cluster. Remova a tarefa:
      </para>
<screen>
<prompt>root@master # </prompt>salt <replaceable>target</replaceable> schedule.delete ceph.heartbeat
</screen>
     </listitem>
     <listitem>
      <para>
       Ainda restam alguns pacotes desnecessários instalados, a maioria deles relacionada a ruby gems e chef. Não é necessário removê-los, mas convém apagá-los executando <command>zypper rm <replaceable>nome_pct</replaceable></command>.
      </para>
     </listitem>
    </itemizedlist>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph-upgrade-3to5">
  <title>Upgrade do SUSE Enterprise Storage 3 para 5</title>

  <important>
   <title>Requisitos de software</title>
   <para>
    Você precisa ter o seguinte software instalado e atualizado para as versões mais recentes dos pacotes em todos os nós do Ceph dos quais você deseja fazer upgrade antes de iniciar o procedimento de upgrade:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      SUSE Linux Enterprise Server 12 SP1
     </para>
    </listitem>
    <listitem>
     <para>
      SUSE Enterprise Storage 3
     </para>
    </listitem>
   </itemizedlist>
  </important>

  <para>
   Para fazer upgrade do cluster do SUSE Enterprise Storage 3 para a versão 5, siga as etapas descritas no <xref linkend="upgrade4to5cephdeploy-all"/> e no <xref linkend="upgrade4to5cephdeploy-admin"/>.
  </para>
 </sect1>
</chapter>
