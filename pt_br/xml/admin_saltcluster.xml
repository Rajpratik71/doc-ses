<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_saltcluster.xml" version="5.0" xml:id="storage-salt-cluster">
 <title>Administração do cluster do Salt</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:translation>sim</dm:translation>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  Após implantar o cluster do Ceph, provavelmente você precisará fazer várias modificações nele de vez em quando. Dentre elas, adicionar ou remover novos nós, discos ou serviços. Este capítulo descreve como você pode realizar essas tarefas de administração.
 </para>
 <sect1 xml:id="salt-adding-nodes">
  <title>Adicionando novos nós do cluster</title>

  <para>
   O procedimento de adição de novos nós ao cluster é quase idêntico à implantação de nó do cluster inicial descrita no <xref linkend="ceph-install-saltstack"/>:
  </para>

  <procedure>
   <step>
    <para>
     Instale o SUSE Linux Enterprise Server 12 SP3 no novo nó, defina a configuração de rede para que ela resolva o nome de host do master Salt corretamente e instale o pacote <systemitem>salt-minion</systemitem>:
    </para>
<screen><prompt>root@minion &gt; </prompt>zypper in salt-minion</screen>
    <para>
     Se o nome de host do master Salt não for <literal>salt</literal>, edite <filename>/etc/salt/minion</filename> e adicione o seguinte:
    </para>
<screen>master: <replaceable>DNS_name_of_your_salt_master</replaceable></screen>
    <para>
     Se você efetuou quaisquer mudanças nos arquivos de configuração mencionados acima, reinicie o serviço <systemitem>salt.minion</systemitem>:
    </para>
<screen><prompt>root@minion &gt; </prompt>systemctl restart salt-minion.service</screen>
   </step>
   <step>
    <para>
     Aceite todas as chaves do Salt no master Salt:
    </para>
<screen><prompt>root@master # </prompt>salt-key --accept-all</screen>
   </step>
   <step>
    <para>
     Verifique se o <filename>/srv/pillar/ceph/deepsea_minions.sls</filename> também está direcionado para o novo minion Salt. Consulte o <xref linkend="ds-minion-targeting-name"/> do <xref linkend="ds-depl-stages"/> para obter mais detalhes.
    </para>
   </step>
   <step>
    <para>
     Execute a fase de preparação. Ela sincroniza os módulos e grains para que o novo minion possa fornecer todas as informações esperadas pelo DeepSea:
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.0</screen>
   </step>
   <step>
    <para>
     Execute a fase de descoberta. Ela gravará novas entradas de arquivo no diretório <filename>/srv/pillar/ceph/proposals</filename>, em que você pode editar os arquivos .yml relevantes:
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.1</screen>
   </step>
   <step>
    <para>
     Se preferir, mude o <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> se o host recém-adicionado não corresponder ao esquema de nomeação existente. Para obter informações detalhadas, consulte o <xref linkend="policy-configuration"/>.
    </para>
   </step>
   <step>
    <para>
     Execute a fase de configuração. Ela lê tudo o que está em <filename>/srv/pillar/ceph</filename> e atualiza o pillar de acordo:
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.2</screen>
    <para>
     O pillar armazena dados que você pode acessar com o seguinte comando:
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> pillar.items</screen>
   </step>
   <step>
    <para>
     As fases de configuração e implantação incluem os nós recém-adicionados:
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.4</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt-adding-services">
  <title>Adicionando novas funções aos nós</title>

  <para>
   Você pode implantar com o DeepSea todos os tipos de funções suportadas. Consulte o <xref linkend="policy-role-assignment"/> para obter mais informações sobre os tipos de função suportados e ver exemplos de como correspondê-los.
  </para>

  <tip>
   <title>Fases e funções obrigatórias e opcionais</title>
   <para>
    Em geral, é recomendável executar a implantação completa de todas as Fases de 0 a 5 para adicionar uma nova função ao nó do cluster. Para economizar algum tempo, você pode ignorar as Fases 3 ou 4, dependendo do tipo de função que você pretende implantar. Enquanto as funções OSD e MON incluem os serviços básicos e são exigidas pelo Ceph, outras funções, como Object Gateway, são opcionais. As fases de implantação do DeepSea são hierárquicas: a Fase 3 implanta os serviços básicos e a Fase 4 implanta os opcionais.
   </para>
   <para>
    Portanto, você precisa executar a Fase 3 ao implantar as funções básicas, como MON, em um nó OSD existente e pode ignorar a Fase 4.
   </para>
   <para>
    Da mesma forma, você pode ignorar a Fase 3 ao implantar os serviços opcionais, como Object Gateway, mas precisa executar a Fase 4 neste caso.
   </para>
  </tip>

  <para>
   Para adicionar um novo serviço a um nó existente, siga estas etapas:
  </para>

  <procedure>
   <step>
    <para>
     Adapte o <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> para corresponder o host existente a uma nova função. Para obter mais detalhes, consulte o <xref linkend="policy-configuration"/>. Por exemplo, se você precisa executar um Object Gateway em um nó MON, a linha é semelhante a:
    </para>
<screen>role-rgw/xx/x/example.mon-1.sls</screen>
   </step>
   <step>
    <para>
     Execute a Fase 2 para atualizar o pilar:
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.2</screen>
   </step>
   <step>
    <para>
     Execute a Fase 3 para implantar os serviços básicos, ou a Fase 4 para implantar os serviços opcionais. Não há nenhum problema em executar as duas fases.
    </para>
   </step>
  </procedure>

  <tip>
   <para>
    Ao adicionar um OSD ao cluster existente, lembre-se de que o cluster será redistribuído após algum tempo. Para minimizar os períodos de redistribuição, recomendamos adicionar ao mesmo tempo todos os OSDs desejados.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="salt-node-removing">
  <title>Removendo e reinstalando nós do cluster</title>

  <para>
   Para remover uma função de um cluster, edite o <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> e remova a(s) linha(s) correspondente(s). Em seguida, execute as Fases 2 e 5 conforme descrito no <xref linkend="ceph-install-stack"/>.
  </para>

  <note>
   <title>Removendo OSDs do cluster</title>
   <para>
    Se for necessário remover determinado nó OSD do cluster, verifique se o cluster tem mais espaço livre em disco do que o disco que você pretende remover. A remoção de um OSD provoca a redistribuição do cluster inteiro.
   </para>
  </note>

  <para>
   Quando uma função é removida de um minion, o objetivo é desfazer todas as modificações relacionadas a essa função. Para a maioria das funções, a tarefa é simples, mas pode haver problemas com as dependências de pacotes. Se um pacote for desinstalado, suas dependências não serão.
  </para>

  <para>
   Os OSDs removidos aparecem como unidades em branco. As tarefas relacionadas sobregravam o início dos sistemas de arquivos, removem as partições de backup e limpam as tabelas de partição.
  </para>

  <note>
   <title>Preservando as partições criadas por outros métodos</title>
   <para>
    As unidades de disco já configuradas por outros métodos, como <command>ceph-deploy</command>, ainda podem conter partições. O DeepSea não as eliminará automaticamente. O administrador deve reaproveitar essas unidades.
   </para>
  </note>

  <example xml:id="ex-ds-rmnode">
   <title>Removendo um minion Salt do cluster</title>
   <para>
    Se os nomes dos minions de armazenamento forem, por exemplo, “data1.ceph”, “data2.ceph”, etc., o “data6.ceph” e as linhas relacionadas no <filename>policy.cfg</filename> serão semelhantes ao seguinte:
   </para>
<screen>[...]
# Hardware Profile
profile-default/cluster/data*.sls
profile-default/stack/default/ceph/minions/data*.yml
[...]</screen>
   <para>
    Portanto, para remover o minion Salt “data2.ceph”, mude as linhas para o seguinte:
   </para>
<screen>
[...]
# Hardware Profile
profile-default/cluster/data[1,3-6]*.sls
profile-default/stack/default/ceph/minions/data[1,3-6]*.yml
[...]</screen>
   <para>
    Em seguida, execute as Fases 2 e 5:
   </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.2
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.5</screen>
  </example>

  <example xml:id="ex-ds-mignode">
   <title>Migrando nós</title>
   <para>
    Considere a seguinte situação: durante a nova instalação do cluster, você (o administrador) alocou um dos nós de armazenamento como Object Gateway independente enquanto aguardava a chegada do hardware do gateway. Agora, o hardware permanente do gateway chegou e você pode finalmente atribuir a função desejada ao nó de armazenamento de backup e remover a função do gateway.
   </para>
   <para>
    Depois de executar as Fases 0 e 1 (consulte o <xref linkend="ds-depl-stages"/>) para o novo hardware, você denominou o novo gateway <literal>rgw1</literal>. Se o nó <literal>data8</literal> precisar que a função Object Gateway seja removida e que a função de armazenamento seja adicionada, o <filename>policy.cfg</filename> atual terá esta aparência:
   </para>
<screen># Hardware Profile
profile-default/cluster/data[1-7]*.sls
profile-default/stack/default/ceph/minions/data[1-7]*.sls

# Roles
role-rgw/cluster/data8*.sls</screen>
   <para>
    Portanto, faça uma mudança para:
   </para>
<screen># Hardware Profile
profile-default/cluster/data[1-8]*.sls
profile-default/stack/default/ceph/minions/data[1-8]*.sls

# Roles
role-rgw/cluster/rgw1*.sls</screen>
   <para>
    Execute as Fases de 2 a 5. A Fase 3 adicionará o <literal>data8</literal> como nó de armazenamento. Por algum tempo, o <literal>data8</literal> terá as duas funções. A Fase 4 adicionará a função Object Gateway ao <literal>rgw1</literal>, e a Fase 5 removerá a função Object Gateway do <literal>data8</literal>.
   </para>
  </example>
 </sect1>
 <sect1 xml:id="ds-mon">
  <title>Reimplantando nós do monitor</title>

  <para>
   Quando há falha em um ou mais nós do monitor e eles não respondem, você precisa remover os monitores com falha do cluster e, possivelmente, readicioná-los ao cluster.
  </para>

  <important>
   <title>O mínimo são três nós do monitor</title>
   <para>
    O número de nós do monitor não deve ser menor do que três. Se houver falha em um nó do monitor e, por esse motivo, o cluster ficar apenas com um ou dois nós, você precisará atribuir temporariamente a função do monitor a outros nós do cluster antes de reimplantar os nós com falha do monitor. Após reimplantar os nós com falha do monitor, você poderá desinstalar as funções temporárias do monitor.
   </para>
   <para>
    Para obter mais informações sobre como adicionar novos nós/funções ao cluster do Ceph, consulte a <xref linkend="salt-adding-nodes"/> e a <xref linkend="salt-adding-services"/>.
   </para>
   <para>
    Para obter mais informações sobre como remover nós do cluster, consulte a <xref linkend="salt-node-removing"/>.
   </para>
  </important>

  <para>
   Há dois graus básicos de falha no nó do Ceph:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     O host do minion Salt está danificado fisicamente ou no nível do OS e não responde à chamada <command>salt “<replaceable>nome_do_minion</replaceable>” test.ping</command>. Nesse caso, você precisa reimplantar o servidor por completo seguindo as instruções relevantes no <xref linkend="ceph-install-stack"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Houve falha nos serviços relacionados ao monitor e eles se recusam a se recuperar, mas o host responde à chamada <command>salt “<replaceable>nome_do_minion</replaceable>” test.ping</command>. Nesse caso, siga estas etapas:
    </para>
   </listitem>
  </itemizedlist>

  <procedure>
   <step>
    <para>
     Edite o <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> no master Salt e remova ou atualize as linhas correspondentes aos nós com falha do monitor para que agora eles apontem para os nós ativos do monitor.
    </para>
   </step>
   <step>
    <para>
     Execute as Fases de 2 a 5 do DeepSea para aplicar as mudanças:
    </para>
<screen>
<prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.2
<prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.3
<prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.4
<prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.5
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt-node-add-disk">
  <title>Adicionar um OSD a um nó</title>

  <para>
   Para adicionar um disco a um nó OSD existente, verifique se qualquer partição no disco foi removida e limpa. Consulte a <xref linkend="deploy-wiping-disk"/> no <xref linkend="ceph-install-stack"/> para obter mais detalhes. Depois que o disco estiver vazio, adicione-o ao arquivo YAML do nó. O caminho para o arquivo é <filename>/srv/pillar/ceph/proposals/profile-default/stack/default/ceph/minions/<replaceable>nome_do_nó</replaceable>.yml</filename>. Após gravar o arquivo, execute as fases 2 e 3 do DeepSea:
  </para>

<screen><prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.2
<prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.3</screen>

  <tip>
   <title>Perfis atualizados automaticamente</title>
   <para>
    Em vez de editar manualmente o arquivo YAML, o DeepSea pode criar novos perfis. Para permitir que o DeepSea crie novos perfis, os perfis existentes precisam ser movidos:
   </para>
<screen><prompt>root@master # </prompt><command>old</command> /srv/pillar/ceph/proposals/profile-default/
<prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.1
<prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.2
<prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.3</screen>
  </tip>
 </sect1>
 <sect1 xml:id="salt-removing-osd">
  <title>Removendo um OSD</title>

  <para>
   Você pode remover um Ceph OSD do cluster executando o seguinte comando:
  </para>

<screen><prompt>root@master # </prompt><command>salt-run</command> disengage.safety
<prompt>root@master # </prompt><command>salt-run</command> remove.osd <replaceable>OSD_ID</replaceable></screen>

  <para>
   <replaceable>OSD_ID</replaceable> precisa ser o número do OSD sem o termo <literal>osd</literal>. Por exemplo, use apenas o dígito <literal>3</literal> de <literal>osd.3</literal>.
  </para>

  <tip>
   <title>Removendo vários OSDs</title>
   <para>
    Não é possível remover vários OSDs em paralelo com o comando <command>salt-run remove.osd</command>. Para automatizar a remoção de vários OSDs, você pode usar o seguinte loop (5, 21, 33, 19 são os números de ID dos OSDs que serão removidos):
   </para>
<screen>
for i in 5 21 33 19
do
 echo $i
 salt-run disengage.safety
 salt-run remove.osd $i
done
</screen>
  </tip>

  <sect2 xml:id="osd-forced-removal">
   <title>Removendo à força os OSDs danificados</title>
   <para>
    Há casos em que há falha na remoção normal de um OSD (consulte a <xref linkend="salt-removing-osd"/>). Por exemplo, isso poderá acontecer se o OSD ou o cache dele estiver danificado, quando ele é afetado por operações de E/S travadas ou quando há falha na desmontagem do disco OSD. Nesse caso, você precisa forçar a remoção do OSD:
   </para>
<screen><prompt>root@master # </prompt><replaceable>target</replaceable> osd.remove <replaceable>OSD_ID</replaceable> force=True</screen>
   <para>
    Esse comando remove tanto a partição de dados quanto as partições de diário ou WAL/BD.
   </para>
   <para>
    Para identificar dispositivos de diário/WAL/BD órfãos, siga estas etapas:
   </para>
   <procedure>
    <step>
     <para>
      Selecione o dispositivo que pode ter partições órfãs e grave a lista de suas partições em um arquivo:
     </para>
<screen>
<prompt>root@minion &gt; </prompt>ls /dev/sdd?* &gt; /tmp/partitions
</screen>
    </step>
    <step>
     <para>
      Execute <command>readlink</command> em todos os dispositivos block.wal, block.db e de diário e compare a saída com a lista de partições que foi gravada:
     </para>
<screen>
<prompt>root@minion &gt; </prompt>readlink -f /var/lib/ceph/osd/ceph-*/{block.wal,block.db,journal} \
 | sort | comm -23 /tmp/partitions -
</screen>
     <para>
      A saída é a lista de partições que <emphasis>não</emphasis> são usadas pelo Ceph.
     </para>
    </step>
    <step>
     <para>
      Remova as partições órfãs que não pertencem ao Ceph usando o comando de sua preferência (por exemplo, <command>fdisk</command>, <command>parted</command> ou <command>sgdisk</command>).
     </para>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="ds-osd-recover">
  <title>Recuperando um nó OSD reinstalado</title>

  <para>
   Se houver falha no sistema operacional que não puder ser recuperada em um dos nós OSD, siga estas etapas para recuperá-lo e reimplantar a função OSD com os dados do cluster inalterados:
  </para>

  <procedure>
   <step>
    <para>
     Reinstale o sistema operacional no nó.
    </para>
   </step>
   <step>
    <para>
     Instale o pacote <package>salt-minion</package> no nó OSD, apague a chave antiga do minion Salt no master Salt e registre a nova chave no master Salt. Para obter mais informações sobre a implantação do minion Salt, consulte o <xref linkend="ceph-install-stack"/>.
    </para>
   </step>
   <step>
    <para>
     Em vez de executar toda a Fase 0, execute as seguintes partes:
    </para>
<screen>
<prompt>root@master # </prompt>salt '<replaceable>osd_node</replaceable>' state.apply ceph.sync
<prompt>root@master # </prompt>salt '<replaceable>osd_node</replaceable>' state.apply ceph.packages.common
<prompt>root@master # </prompt>salt '<replaceable>osd_node</replaceable>' state.apply ceph.mines
<prompt>root@master # </prompt>salt '<replaceable>osd_node</replaceable>' state.apply ceph.updates
</screen>
   </step>
   <step>
    <para>
     Execute as Fases de 1 a 5 do DeepSea:
    </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.1
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.2
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.4
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.5
</screen>
   </step>
   <step>
    <para>
     Execute a Fase 0 do DeepSea:
    </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.0
</screen>
   </step>
   <step>
    <para>
     Reinicialize o nó OSD relevante. Todos os discos OSD serão redescobertos e reutilizados.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt-automated-installation">
  <title>Instalação automatizada pelo Salt</title>

  <para>
   É possível automatizar a instalação por meio do reator Salt. Para ambientes virtuais ou de hardware consistentes, esta configuração permitirá a criação de um cluster do Ceph com o comportamento especificado.
  </para>

  <warning>
   <para>
    O Salt não pode executar verificações de dependência com base nos eventos do reator. Existe um risco real de tornar seu master Salt sobrecarregado e sem resposta.
   </para>
  </warning>

  <para>
   A instalação automatizada requer o seguinte:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Um <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> criado apropriadamente.
    </para>
   </listitem>
   <listitem>
    <para>
     Configuração personalizada preparada incluída no diretório <filename>/srv/pillar/ceph/stack</filename>.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   A configuração do reator padrão executará apenas as Fases 0 e 1. Isso permite testar o reator sem esperar a conclusão das fases seguintes.
  </para>

  <para>
   Quando o primeiro comando salt-minion for iniciado, a Fase 0 começará. Um bloqueio impede várias instâncias. Quando todos os minions concluírem a Fase 0, a Fase 1 será iniciada.
  </para>

  <para>
   Se a operação for executada apropriadamente, mude a última linha no <filename>/etc/salt/master.d/reactor.conf</filename>:
  </para>

<screen>- /srv/salt/ceph/reactor/discovery.sls</screen>

  <para>
   para
  </para>

<screen>- /srv/salt/ceph/reactor/all_stages.sls</screen>
 </sect1>
 <sect1 xml:id="deepsea-rolling-updates">
  <title>Atualizando os nós do cluster</title>

  <para>
   Convém aplicar atualizações sequenciais aos nós do cluster regularmente. Para aplicar as atualizações, execute a Fase 0:
  </para>

<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.0</screen>

  <para>
   Se o DeepSea detectar um cluster do Ceph em execução, ele aplicará as atualizações e reiniciará os nós sequencialmente. O DeepSea segue a recomendação oficial do Ceph de atualizar primeiro os monitores, depois os OSDs e, por último, os serviços adicionais, como MDS, Object Gateway, iSCSI Gateway ou NFS Ganesha. O DeepSea interromperá o processo de atualização se ele detectar algum problema no cluster. Veja a seguir o que pode provocar isso:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     O Ceph relata “HEALTH_ERR” por mais do que 300 segundos.
    </para>
   </listitem>
   <listitem>
    <para>
     Os minions Salt são consultados para verificar se os serviços atribuídos deles ainda estão ativos depois de uma atualização. Haverá falha na atualização se os serviços ficarem inativos por mais do que 900 segundos.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Faça esses ajustes para garantir que o cluster do Ceph continue funcionando mesmo com as atualizações corrompidas ou com falha.
  </para>

  <para>
   A Fase 0 do DeepSea atualizará o sistema por meio do <command>zypper update</command> e o reinicializará se o kernel for atualizado. Para eliminar a possibilidade de uma reinicialização forçada de todos os nós em potencial, verifique se o kernel mais recente está instalado e em execução antes de iniciar a Fase 0 do DeepSea.
  </para>

  <tip>
   <title><command>zypper patch</command></title>
   <para>
    Se você preferir atualizar o sistema usando o comando <command>zypper patch</command>, edite o <filename>/srv/pillar/ceph/stack/global.yml</filename> e adicione a seguinte linha:
   </para>
<screen>update_method_init: zypper-patch</screen>
  </tip>

  <para>
   Você pode mudar o comportamento de reinicialização padrão da Fase 0 do DeepSea adicionando as seguintes linhas ao <filename>/srv/pillar/ceph/stack/global.yml</filename>:
  </para>

<screen>stage_prep_master: default-update-no-reboot
stage_prep_minion: default-update-no-reboot</screen>

  <para>
   <literal>stage_prep_master</literal> define o comportamento da Fase 0 do master Salt, e <literal>stage_prep_minion</literal> define o comportamento de todos os minions. Todos os parâmetros disponíveis são:
  </para>

  <variablelist>
   <varlistentry>
    <term>default</term>
    <listitem>
     <para>
      Instalar as atualizações e reinicializar após a atualização.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>default-update-no-reboot</term>
    <listitem>
     <para>
      Instalar as atualizações sem reinicializar.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>default-no-update-reboot</term>
    <listitem>
     <para>
      Reinicializar sem instalar as atualizações.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>default-no-update-no-reboot</term>
    <listitem>
     <para>
      Não instalar as atualizações nem reinicializar.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="sec-salt-cluster-reboot">
  <title>Parando ou reinicializando o cluster</title>

  <para>
   Em alguns casos, talvez seja necessário parar ou reinicializar o cluster inteiro. Recomendamos verificar com cuidado as dependências dos serviços em execução. As seguintes etapas apresentam uma descrição de como parar e iniciar o cluster:
  </para>

  <procedure>
   <step>
    <para>
     Especifique para o cluster do Ceph não marcar os OSDs com o flag “out”:
    </para>
<screen><prompt>root # </prompt><command>ceph</command> osd set noout</screen>
   </step>
   <step>
    <para>
     Pare os daemons e os nós na seguinte ordem:
    </para>
    <orderedlist>
     <listitem>
      <para>
       Clientes de armazenamento
      </para>
     </listitem>
     <listitem>
      <para>
       Gateways. Por exemplo, NFS Ganesha ou Object Gateway
      </para>
     </listitem>
     <listitem>
      <para>
       Servidor de Metadados
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph OSD
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Manager
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Monitor
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     Se necessário, execute as tarefas de manutenção.
    </para>
   </step>
   <step>
    <para>
     Inicie os nós e os servidores na ordem inversa do processo de encerramento:
    </para>
    <orderedlist>
     <listitem>
      <para>
       Ceph Monitor
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Manager
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph OSD
      </para>
     </listitem>
     <listitem>
      <para>
       Servidor de Metadados
      </para>
     </listitem>
     <listitem>
      <para>
       Gateways. Por exemplo, NFS Ganesha ou Object Gateway
      </para>
     </listitem>
     <listitem>
      <para>
       Clientes de armazenamento
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     Remova o flag “noout”:
    </para>
<screen><prompt>root # </prompt><command>ceph</command> osd unset noout</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ds-custom-cephconf">
  <title>Arquivo <filename>ceph.conf</filename> personalizado</title>

  <para>
   Se você precisar inserir configurações personalizadas no arquivo <filename>ceph.conf</filename>, poderá modificar os arquivos de configuração no diretório <filename>/srv/salt/ceph/configuration/files/ceph.conf.d</filename>:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     global.conf
    </para>
   </listitem>
   <listitem>
    <para>
     mon.conf
    </para>
   </listitem>
   <listitem>
    <para>
     mgr.conf
    </para>
   </listitem>
   <listitem>
    <para>
     mds.conf
    </para>
   </listitem>
   <listitem>
    <para>
     osd.conf
    </para>
   </listitem>
   <listitem>
    <para>
     client.conf
    </para>
   </listitem>
   <listitem>
    <para>
     rgw.conf
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <title><filename>Rgw.conf</filename> exclusivo</title>
   <para>
    O Object Gateway oferece muita flexibilidade e é exclusivo em comparação com outras seções do <filename>ceph.conf</filename>. Todos os outros componentes do Ceph têm cabeçalhos estáticos, como <literal>[mon]</literal> ou <literal>[osd]</literal>. O Object Gateway tem cabeçalhos exclusivos, como <literal>[client.rgw.rgw1]</literal>. Isso significa que o arquivo <filename>rgw.conf</filename> precisa de uma entrada de cabeçalho. Consulte <filename>/srv/salt/ceph/configuration/files/rgw.conf</filename> para ver um exemplo.
   </para>
  </note>

  <important>
   <title>Executar a fase 3</title>
   <para>
    Após fazer mudanças personalizadas nos arquivos de configuração mencionados acima, execute a Fase 3 para aplicá-las aos nós do cluster:
   </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.3</screen>
  </important>

  <para>
   Esses arquivos são incluídos do arquivo de gabarito <filename>/srv/salt/ceph/configuration/files/ceph.conf.j2</filename> e correspondem às diferentes seções aceitas pelo arquivo de configuração do Ceph. Ao inserir um trecho da configuração no arquivo correto, o DeepSea pode incluí-lo na seção correta. Você não precisa adicionar nenhum dos cabeçalhos de seção.
  </para>

  <tip>
   <para>
    Para aplicar quaisquer opções de configuração apenas às instâncias específicas de um daemon, adicione um cabeçalho como <literal>[osd.1]</literal>. As seguintes opções de configuração serão aplicadas apenas ao daemon OSD com o ID 1.
   </para>
  </tip>

  <sect2>
   <title>Anulando os padrões</title>
   <para>
    As últimas declarações em uma seção anulam as anteriores. Portanto, é possível anular a configuração padrão conforme especificado no gabarito <filename>/srv/salt/ceph/configuration/files/ceph.conf.j2</filename>. Por exemplo, para desativar a autenticação do cephx, adicione as três linhas a seguir ao arquivo <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/global.conf</filename>:
   </para>
<screen>auth cluster required = none
auth service required = none
auth client required = none</screen>
  </sect2>

  <sect2>
   <title>Incluindo arquivos de configuração</title>
   <para>
    Se você precisar aplicar muitas configurações personalizadas, use as seguintes declarações de inclusão nos arquivos de configuração personalizados para facilitar o gerenciamento de arquivos. Veja a seguir um exemplo de arquivo <filename>osd.conf</filename>:
   </para>
<screen>[osd.1]
{% include "ceph/configuration/files/ceph.conf.d/osd1.conf" ignore missing %}
[osd.2]
{% include "ceph/configuration/files/ceph.conf.d/osd2.conf" ignore missing %}
[osd.3]
{% include "ceph/configuration/files/ceph.conf.d/osd3.conf" ignore missing %}
[osd.4]
{% include "ceph/configuration/files/ceph.conf.d/osd4.conf" ignore missing %}</screen>
   <para>
    No exemplo anterior, os arquivos <filename>osd1.conf</filename>, <filename>osd2.conf</filename>, <filename>osd3.conf</filename> e <filename>osd4.conf</filename> contêm opções de configuração específicas ao OSD relacionado.
   </para>
   <tip>
    <title>Configuração de tempo de execução</title>
    <para>
     As mudanças feitas nos arquivos de configuração do Ceph entrarão em vigor após a reinicialização dos daemons Ceph relacionados. Consulte a <xref linkend="ceph-config-runtime"/> para obter mais informações sobre como mudar a configuração de tempo de execução do Ceph.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-config-runtime">
  <title>Configuração de tempo de execução do Ceph</title>

  <para>
   A <xref linkend="ds-custom-cephconf"/> descreve como fazer mudanças no arquivo de configuração do Ceph <filename>ceph.conf</filename>. Entretanto, o comportamento real do cluster não é determinado pelo estado atual do arquivo <filename>ceph.conf</filename>, mas sim pela configuração dos daemons Ceph em execução, que são armazenados na memória.
  </para>

  <para>
   É possível consultar determinada configuração em um daemon Ceph usando o <emphasis>soquete de admin</emphasis> no nó em que o daemon está em execução. Por exemplo, o seguinte comando obtém o valor do parâmetro de configuração <option>osd_max_write_size</option> do daemon denominado <literal>osd.0</literal>:
  </para>

<screen><prompt>root # </prompt>ceph --admin-daemon /var/run/ceph/ceph-osd.0.asok \
config get osd_max_write_size
{
  "osd_max_write_size": "90"
}</screen>

  <para>
   Você também pode <emphasis>mudar</emphasis> as configurações dos daemons em tempo de execução. Lembre-se de que essa mudança é temporária e será perdida após a reinicialização do próximo daemon. Por exemplo, o seguinte comando muda o parâmetro <option>osd_max_write_size</option> para “50” em todos os OSDs no cluster:
  </para>

<screen><prompt>root # </prompt>ceph tell osd.* injectargs --osd_max_write_size 50</screen>

  <warning>
   <title><command>injectargs</command> não é confiável</title>
   <para>
    Mudar as configurações do cluster com o comando <command>injectargs</command> não é 100% confiável. Se você precisa ter certeza de que o parâmetro mudado está ativo, mude-o no arquivo de configuração e reinicie todos os daemons no cluster.
   </para>
  </warning>
 </sect1>
</chapter>
