<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_operating_pools.xml" version="5.0" xml:id="ceph-pools">
 <title>Gerenciando pools de armazenamento</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:translation>sim</dm:translation>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  O Ceph armazena dados em pools. Pools são grupos lógicos para armazenamento de objetos. Quando você implanta um cluster pela primeira vez sem criar um pool, o Ceph usa os pools padrão para armazenar os dados. Um pool oferece a você:
 </para>
 <itemizedlist mark="bullet" spacing="normal">
  <listitem>
   <para>
    <emphasis>Resiliência</emphasis>: Agora você pode definir quantos OSDs podem falhar sem perda de dados. Para os pools replicados, esse é o número desejado de cópias/réplicas de um objeto. Novos pools são criados com um total padrão de réplicas definido como 3. Como a configuração típica armazena um objeto e uma cópia adicional, você precisa definir o total de réplicas como 2. Para os pools com codificação de eliminação, esse é o número de pacotes de codificação (que é <emphasis>m=2</emphasis> no perfil de codificação de eliminação).
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Grupos de Posicionamento</emphasis>: São estruturas internas de dados para armazenar dados em um pool em vários OSDs. O modo como o Ceph armazena os dados nos PGs é definido em um Mapa CRUSH. Você pode definir o número de grupos de posicionamento para o pool. Uma configuração típica usa aproximadamente 100 grupos de posicionamento por OSD para possibilitar o equilíbrio ideal sem usar muitos recursos de computação. Ao configurar vários pools, tenha cuidado para garantir que você defina um número adequado de grupos de posicionamento para o pool e o cluster como um todo.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Regras CRUSH</emphasis>: Quando você armazena dados em um pool, um conjunto de regras CRUSH mapeado para o pool permite que o CRUSH identifique uma regra para o posicionamento do objeto e suas réplicas (ou pacotes para os pools com codificação de eliminação) no cluster. Você pode criar uma regra CRUSH personalizada para o pool.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Instantâneos</emphasis>: Ao criar instantâneos com <command>ceph osd pool mksnap</command>, você efetivamente captura um instantâneo de determinado pool.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Definir Propriedade</emphasis>: Você pode definir um ID de usuário como proprietário de um pool.
   </para>
  </listitem>
 </itemizedlist>
 <para>
  Para organizar dados em pools, você pode listar, criar e remover pools. Você também pode ver as estatísticas de uso para cada pool.
 </para>
 <sect1 xml:id="ceph-pools-associate">
  <title>Associar pools a um aplicativo</title>

  <para>
   Antes de usar os pools, você precisa associá-los a um aplicativo. Os pools que serão usados com o CephFS ou os pools criados automaticamente pelo Object Gateway são associados de forma automática. Os pools planejados para uso com o RBD precisam ser inicializados usando a ferramenta <command>rbd</command> (consulte a <xref linkend="ceph-rbd-commands"/> para obter mais informações).
  </para>

  <para>
   Nos outros casos, você pode associar manualmente um nome de aplicativo de formato livre a um pool:
  </para>

<screen><prompt>root # </prompt>ceph osd pool application enable <replaceable>pool_name</replaceable> <replaceable>application_name</replaceable></screen>

  <tip>
   <title>Nomes de aplicativos padrão</title>
   <para>
    O CephFS usa o nome do aplicativo <literal>cephfs</literal>, o Dispositivo de Blocos RADOS usa o <literal>rbd</literal> e o Object Gateway usa o <literal>rgw</literal>.
   </para>
  </tip>

  <para>
   É possível associar um pool a vários aplicativos, e cada aplicativo tem seus próprios metadados. Você pode exibir os metadados do aplicativo para determinado pool usando o seguinte comando:
  </para>

<screen><prompt>root # </prompt>ceph osd pool application get <replaceable>pool_name</replaceable></screen>
 </sect1>
 <sect1 xml:id="ceph-pools-operate">
  <title>Pools operacionais</title>

  <para>
   Esta seção apresenta informações práticas para realizar tarefas básicas com pools. Você aprenderá como listar, criar e apagar pools, bem como mostrar as estatísticas ou gerenciar instantâneos de um pool.
  </para>

  <sect2>
   <title>Listar pools</title>
   <para>
    Para listar os pools do cluster, execute:
   </para>
<screen><prompt>root # </prompt>ceph osd lspools
0 rbd, 1 photo_collection, 2 foo_pool,</screen>
  </sect2>

  <sect2 xml:id="ceph-pools-operate-add-pool">
   <title>Criar um pool</title>
   <para>
    Para criar um pool replicado, execute:
   </para>
<screen><prompt>root # </prompt>ceph osd pool create <replaceable>pool_name</replaceable> <replaceable>pg_num</replaceable> <replaceable>pgp_num</replaceable> replicated <replaceable>crush_ruleset_name</replaceable> \
<replaceable>expected_num_objects</replaceable></screen>
   <para>
    Para criar um pool com codificação de eliminação, execute:
   </para>
<screen><prompt>root # </prompt>ceph osd pool create <replaceable>pool_name</replaceable> <replaceable>pg_num</replaceable> <replaceable>pgp_num</replaceable> erasure <replaceable>erasure_code_profile</replaceable> \
 <replaceable>crush_ruleset_name</replaceable> <replaceable>expected_num_objects</replaceable></screen>
   <para>
    O <command>ceph osd pool create</command> poderá falhar se você exceder o limite de grupos de posicionamento por OSD. O limite é definido com a opção <option>mon_max_pg_per_osd</option>.
   </para>
   <variablelist>
    <varlistentry>
     <term>pool_name</term>
     <listitem>
      <para>
       O nome do pool. Ele deve ser exclusivo. Essa opção é obrigatória.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       O número total de grupos de posicionamento para o pool. Essa opção é obrigatória. O valor padrão é 8.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       O número total de grupos de posicionamento para fins de posicionamento. Ele deve ser igual ao número total de grupos de posicionamento, exceto para cenários de divisão de grupo de posicionamento. Essa opção é obrigatória. O valor padrão é 8.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_type</term>
     <listitem>
      <para>
       O tipo de pool, que pode ser <emphasis>replicated</emphasis> para recuperação de OSDs perdidos mantendo várias cópias dos objetos, ou <emphasis>erasure</emphasis> para aplicar um tipo de recurso RAID5 generalizado. Os pools replicados exigem mais armazenamento bruto, porém implementam todas as operações do Ceph. Os pools com codificação de eliminação exigem menos armazenamento bruto, porém implementam apenas um subconjunto de operações disponíveis. O padrão é “replicated”.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crush_ruleset_name</term>
     <listitem>
      <para>
       O nome do conjunto de regras CRUSH para este pool. Se o conjunto de regras especificado não existir, haverá falha na criação do pool replicado com -ENOENT. No entanto, o pool replicado criará um novo conjunto de regras de eliminação com o nome especificado. O valor padrão é “erasure-code” para um pool com codificação de eliminação. Coleta a variável de configuração do Ceph <option>osd_pool_default_crush_replicated_ruleset</option> para o pool replicado.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>erasure_code_profile=profile</term>
     <listitem>
      <para>
       Apenas para pools com codificação de eliminação. Use o perfil de codificação de eliminação. Ele deve ser um perfil existente, conforme definido por <command>osd erasure-code-profile set</command>.
      </para>
      <para>
       Ao criar um pool, defina o número de grupos de posicionamento como um valor adequado (por exemplo, 100). Considere também o número total de grupos de posicionamento por OSD. Os grupos de posicionamento são onerosos em termos de computação, portanto, o desempenho será prejudicado se você tiver muitos pools com vários grupos de posicionamento (por exemplo, 50 pools com 100 grupos de posicionamento cada). O ponto do rendimento regressivo depende da capacidade do host OSD.
      </para>
      <para>
       Consulte <link xlink:href="http://docs.ceph.com/docs/master/rados/operations/placement-groups/">Grupos de Posicionamento</link> para obter detalhes sobre como calcular um número apropriado de grupos de posicionamento para seu pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>expected_num_objects</term>
     <listitem>
      <para>
       O número esperado de objetos para este pool. Ao definir esse valor, a divisão da pasta do PG ocorre no momento da criação do pool. Isso evita o impacto da latência com uma divisão de pasta em tempo de execução.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2>
   <title>Definir cotas do pool</title>
   <para>
    Você pode definir cotas do pool para o número máximo de bytes e/ou para o número máximo de objetos por pool.
   </para>
<screen><prompt>root # </prompt>ceph osd pool set-quota <replaceable>pool-name</replaceable> <replaceable>max_objects</replaceable> <replaceable>obj-count</replaceable> <replaceable>max_bytes</replaceable> <replaceable>bytes</replaceable></screen>
   <para>
    Por exemplo:
   </para>
<screen><prompt>root # </prompt>ceph osd pool set-quota data max_objects 10000</screen>
   <para>
    Para remover uma cota, defina o valor como 0.
   </para>
  </sect2>

  <sect2 xml:id="ceph-pools-operate-del-pool">
   <title>Apagar um pool</title>
   <warning>
    <title>A exclusão do pool não é reversível</title>
    <para>
     Os pools podem conter dados importantes. Apagar um pool faz com que todos os dados nele desapareçam, e não é possível recuperá-los.
    </para>
   </warning>
   <para>
    Como a exclusão acidental do pool é um perigo real, o Ceph implementa dois mecanismos que impedem que os pools sejam apagados. Os dois mecanismos devem ser desabilitados antes que um pool possa ser apagado.
   </para>
   <para>
    O primeiro mecanismo é o flag <literal>NODELETE</literal>. Cada pool tem esse flag, e seu valor padrão é “false”. Para saber o valor desse flag em um pool, execute o seguinte comando:
   </para>
<screen><prompt>root # </prompt>ceph osd pool get <replaceable>pool_name</replaceable> nodelete</screen>
   <para>
    Se a saída for <literal>nodelete: true</literal>, não será possível apagar o pool até você mudar o flag usando o seguinte comando:
   </para>
<screen>ceph osd pool set <replaceable>pool_name</replaceable> nodelete false</screen>
   <para>
    O segundo mecanismo é o parâmetro de configuração de todo o cluster <option>mon allow pool delete</option>, que assume como padrão “false”. Por padrão, isso significa que não é possível apagar um pool. A mensagem de erro exibida é:
   </para>
<screen>Error EPERM: pool deletion is disabled; you must first set the
mon_allow_pool_delete config option to true before you can destroy a pool</screen>
   <para>
    Para apagar o pool mesmo com essa configuração de segurança, você pode definir <option>mon allow pool delete</option> temporariamente como “true”, apagar o pool e, em seguida, reverter o parâmetro para “false”:
   </para>
<screen><prompt>root # </prompt>ceph tell mon.* injectargs --mon-allow-pool-delete=true
<prompt>root # </prompt>ceph osd pool delete pool_name pool_name --yes-i-really-really-mean-it
<prompt>root # </prompt>ceph tell mon.* injectargs --mon-allow-pool-delete=false</screen>
   <para>
    O comando <command>injectargs</command> exibe a seguinte mensagem:
   </para>
<screen>injectargs:mon_allow_pool_delete = 'true' (not observed, change may require restart)</screen>
   <para>
    Trata-se apenas de uma confirmação de que o comando foi executado com êxito. Isso não é um erro.
   </para>
   <para>
    Se você criou seus próprios conjuntos de regras e suas próprias regras para um pool, convém removê-los quando ele não for mais necessário. Se você criou usuários com permissões estritamente para um pool que não existe mais, convém apagá-los também.
   </para>
  </sect2>

  <sect2>
   <title>Renomear um pool</title>
   <para>
    Para renomear um pool, execute:
   </para>
<screen><prompt>root # </prompt>ceph osd pool rename <replaceable>current-pool-name</replaceable> <replaceable>new-pool-name</replaceable></screen>
   <para>
    Se você renomear um pool e tiver recursos por pool para um usuário autenticado, deverá atualizar os recursos do usuário com o novo nome do pool.
   </para>
  </sect2>

  <sect2>
   <title>Mostrar as estatísticas do pool</title>
   <para>
    Para mostrar as estatísticas de uso de um pool, execute:
   </para>
<screen><prompt>root # </prompt>rados df
pool name  category  KB  objects   lones  degraded  unfound  rd  rd KB  wr  wr KB
cold-storage    -   228   1         0      0          0       0   0      1   228
data            -    1    4         0      0          0       0   0      4    4
hot-storage     -    1    2         0      0          0       15  10     5   231
metadata        -    0    0         0      0          0       0   0      0    0
pool1           -    0    0         0      0          0       0   0      0    0
rbd             -    0    0         0      0          0       0   0      0    0
total used          266268          7
total avail       27966296
total space       28232564</screen>
  </sect2>

  <sect2 xml:id="ceph-pools-values">
   <title>Definir os valores do pool</title>
   <para>
    Para definir um valor para um pool, execute:
   </para>
<screen><prompt>root # </prompt>ceph osd pool set <replaceable>pool-name</replaceable> <replaceable>key</replaceable> <replaceable>value</replaceable></screen>
   <para>
    Você pode definir valores para as seguintes chaves:
   </para>
   <variablelist>
    <varlistentry>
     <term>size</term>
     <listitem>
      <para>
       Define o número de réplicas para os objetos no pool. Consulte a <xref linkend="ceph-pools-options-num-of-replicas"/> para obter mais detalhes. Apenas pools replicados.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>min_size</term>
     <listitem>
      <para>
       Define o número mínimo de réplicas necessárias para E/S. Consulte a <xref linkend="ceph-pools-options-num-of-replicas"/> para obter mais detalhes. Apenas pools replicados.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crash_replay_interval</term>
     <listitem>
      <para>
       O número de segundos para permitir que os clientes reproduzam solicitações confirmadas, mas não comprometidas.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       O número de grupos de posicionamento para o pool. Se você adicionar OSDs ao cluster, deverá aumentar o valor de grupos de posicionamento. Para obter detalhes, consulte a <xref linkend="storage-bp-cluster-mntc-add-pgnum"/>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       O número efetivo de grupos de posicionamento a ser usado ao calcular o posicionamento dos dados.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crush_ruleset</term>
     <listitem>
      <para>
       O conjunto de regras a ser usado para mapear o posicionamento de objetos no cluster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hashpspool</term>
     <listitem>
      <para>
       Defina (1) ou não defina (0) o flag HASHPSPOOL em um pool específico. A habilitação desse flag muda o algoritmo para distribuir melhor os PGs pelos OSDs. Após a habilitação desse flag em um pool com o flag HASHPSPOOL definido como 0, o cluster iniciará o preenchimento para reposicionar todos os PGs corretamente. Saiba que isso pode gerar uma carga considerável de E/S em um cluster, portanto, é necessário realizar um bom planejamento em clusters de produção altamente carregados.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nodelete</term>
     <listitem>
      <para>
       Impede que o pool seja removido.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nopgchange</term>
     <listitem>
      <para>
       Impede que <option>pg_num</option> e <option>pgp_num</option> do pool sejam modificados.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nosizechange</term>
     <listitem>
      <para>
       Impede que o tamanho do pool seja modificado.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>write_fadvise_dontneed</term>
     <listitem>
      <para>
       Defina/Não defina o flag <literal>WRITE_FADVISE_DONTNEED</literal> em um pool específico.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>noscrub,nodeep-scrub</term>
     <listitem>
      <para>
       Desabilita a depuração (em detalhes) dos dados para o pool específico a fim de resolver uma alta carga de E/S temporária.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_type</term>
     <listitem>
      <para>
       Habilita o monitoramento de conjunto de acertos para pools de cache. Consulte <link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">Filtro de Bloom</link> para obter informações adicionais. Essa opção pode ter os seguintes valores: <literal>bloom</literal>, <literal>explicit_hash</literal>, <literal>explicit_object</literal>. O padrão é <literal>bloom</literal>, os outros valores são apenas para teste.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_count</term>
     <listitem>
      <para>
       O número de conjuntos de acertos para armazenar nos pools de cache. Quanto maior o número, mais RAM é consumida pelo daemon <systemitem>ceph-osd</systemitem>. O padrão é <literal>0</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_period</term>
     <listitem>
      <para>
       A duração em segundos de um período do conjunto de acertos para os pools de cache. Quanto maior o número, mais RAM é consumida pelo daemon <systemitem>ceph-osd</systemitem>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_fpp</term>
     <listitem>
      <para>
       A probabilidade de falsos positivos para o tipo de conjunto de acertos bloom. Consulte <link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">Filtro de Bloom</link> para obter informações adicionais. A faixa válida é de 0,0 a 1,0. O padrão é <literal>0,05</literal>
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>use_gmt_hitset</term>
     <listitem>
      <para>
       Force os OSDs a usar marcações de horário em GMT (Horário de Greenwich) ao criar um conjunto de acertos para camadas de cache. Isso garante que os nós em fusos horários diferentes retornem o mesmo resultado. O padrão é <literal>1</literal>. Esse valor não deve ser mudado.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_dirty_ratio</term>
     <listitem>
      <para>
       A porcentagem do pool de cache que contém os objetos modificados antes que o agente de camadas de cache os descarregue para o pool de armazenamento de suporte. O padrão é <literal>0,4</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_dirty_high_ratio</term>
     <listitem>
      <para>
       A porcentagem do pool de cache que contém os objetos modificados antes que o agente de camadas de cache os descarregue para o pool de armazenamento de suporte com uma velocidade maior. O padrão é <literal>0,6</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_full_ratio</term>
     <listitem>
      <para>
       A porcentagem do pool de cache que contém os objetos não modificados (limpos) antes que o agente de camadas de cache os elimine do pool de cache. O padrão é <literal>0,8</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>target_max_bytes</term>
     <listitem>
      <para>
       O Ceph iniciará o descarregamento ou a eliminação de objetos quando o limite <option>max_bytes</option> for acionado.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>target_max_objects</term>
     <listitem>
      <para>
       O Ceph iniciará o descarregamento ou a eliminação de objetos quando o limite <option>max_objects</option> for acionado.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_grade_decay_rate</term>
     <listitem>
      <para>
       Taxa de redução de temperatura entre dois <literal>hit_set</literal>s sucessivos. O padrão é <literal>20</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_search_last_n</term>
     <listitem>
      <para>
       Considera no máximo <literal>N</literal> aparições nos <literal>hit_set</literal>s para o cálculo da temperatura. O padrão é <literal>1</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_min_flush_age</term>
     <listitem>
      <para>
       O tempo (em segundos) antes que o agente de camadas de cache descarregue um objeto do pool de cache para o pool de armazenamento.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_min_evict_age</term>
     <listitem>
      <para>
       O tempo (em segundos) antes que o agente de camadas de cache elimine um objeto do pool de cache.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>fast_read</term>
     <listitem>
      <para>
       Se esse flag estiver habilitado nos pools com codificação de eliminação, a solicitação de leitura emitirá subleituras para todos os fragmentos e aguardará até receber fragmentos suficientes para decodificar e atender ao cliente. No caso dos plug-ins de eliminação <emphasis>jerasure</emphasis> e <emphasis>isa</emphasis>, quando as primeiras <literal>K</literal> respostas são retornadas, a solicitação do cliente é atendida imediatamente, usando os dados decodificados dessas respostas. Isso ajuda a receber alguns recursos para melhorar o desempenho. No momento, esse flag é suportado apenas para pools com codificação de eliminação. O padrão é <literal>0</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>scrub_min_interval</term>
     <listitem>
      <para>
       O intervalo mínimo em segundos para depuração do pool quando a carga do cluster está baixa. O padrão <literal>0</literal> significa que o valor <option>osd_scrub_min_interval</option> do arquivo de configuração do Ceph foi usado.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>scrub_max_interval</term>
     <listitem>
      <para>
       O intervalo máximo em segundos para depuração do pool, independentemente da carga do cluster. O padrão <literal>0</literal> significa que o valor <option>osd_scrub_max_interval</option> do arquivo de configuração do Ceph foi usado.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>deep_scrub_interval</term>
     <listitem>
      <para>
       O intervalo em segundos para depuração do pool <emphasis>em detalhes</emphasis>. O padrão <literal>0</literal> significa que o valor <option>osd_deep_scrub</option> do arquivo de configuração do Ceph foi usado.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2>
   <title>Obter os valores do pool</title>
   <para>
    Para obter um valor de um pool, execute:
   </para>
<screen><prompt>root # </prompt>ceph osd pool get <replaceable>pool-name</replaceable> <replaceable>key</replaceable></screen>
   <para>
    Você pode obter valores para as chaves listadas na <xref linkend="ceph-pools-values"/> e as chaves a seguir:
   </para>
   <variablelist>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       O número de grupos de posicionamento para o pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       O número efetivo de grupos de posicionamento a ser usado ao calcular o posicionamento dos dados. A faixa válida é igual a ou menor do que <literal>pg_num</literal>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph-pools-options-num-of-replicas">
   <title>Definir o número de réplicas do objeto</title>
   <para>
    Para definir o número de réplicas do objeto em um pool replicado, execute o seguinte:
   </para>
<screen><prompt>root # </prompt>ceph osd pool set <replaceable>poolname</replaceable> size <replaceable>num-replicas</replaceable></screen>
   <para>
    O <replaceable>num-replicas</replaceable> inclui o próprio objeto. Por exemplo, se você deseja o objeto e duas cópias dele para um total de três instâncias do objeto, especifique 3.
   </para>
   <para>
    Se você definir <replaceable>num-replicas</replaceable> como 2, haverá apenas <emphasis>uma</emphasis> cópia dos dados. Se você perder uma instância do objeto, precisará confiar que a outra cópia não foi corrompida desde a última <link xlink:href="http://ceph.com/docs/master/rados/configuration/osd-config-ref/#scrubbing">depuração</link> durante a recuperação, por exemplo.
   </para>
   <para>
    A definição de um pool para uma réplica significa que existe exatamente <emphasis>uma</emphasis> instância do objeto de dados no pool. Se houver falha no OSD, você perderá os dados. Um uso possível para um pool com uma réplica é armazenar dados temporários por um curto período.
   </para>
   <para>
    A definição de mais de três réplicas para um pool significa apenas um pequeno aumento na confiabilidade, mas pode ser adequada em casos raros. Lembre-se de que, quanto mais réplicas, mais espaço em disco é necessário para armazenar as cópias do objeto. Se você precisar de segurança máxima de dados, recomendamos usar pools com codificação de eliminação. Para obter mais informações, consulte o <xref linkend="cha-ceph-erasure"/>. 
   </para>
   <warning>
    <title>Recomendação de mais do que duas réplicas</title>
    <para>
     Desaconselhamos fortemente o uso de apenas 2 réplicas. Em caso de falha em um OSD, a falha do segundo OSD devido a uma alta carga de trabalho durante a recuperação é extremamente provável.
    </para>
   </warning>
   <para>
    Por exemplo:
   </para>
<screen><prompt>root # </prompt>ceph osd pool set data size 3</screen>
   <para>
    Você pode executar esse comando para cada pool.
   </para>
   <note>
    <para>
     Um objeto pode aceitar E/S no modo degradado com menos do que <literal>pool size</literal> réplicas. Para definir um número mínimo de réplicas necessárias para E/S, você deve usar a configuração <literal>min_size</literal>. Por exemplo:
    </para>
<screen><prompt>root # </prompt>ceph osd pool set data min_size 2</screen>
    <para>
     Isso garante que nenhum objeto no pool de dados receba E/S com menos do que <literal>min_size</literal> réplicas.
    </para>
   </note>
  </sect2>

  <sect2>
   <title>Obter o número de réplicas do objeto</title>
   <para>
    Para obter o número de réplicas do objeto, execute o seguinte:
   </para>
<screen><prompt>root # </prompt>ceph osd dump | grep 'replicated size'</screen>
   <para>
    O Ceph listará os pools, com o atributo <literal>replicated size</literal> realçado. Por padrão, o Ceph cria duas réplicas de um objeto (um total de três cópias, ou um tamanho de 3).
   </para>
  </sect2>

  <sect2 xml:id="storage-bp-cluster-mntc-add-pgnum">
   <title>Aumentando o número de grupos de posicionamento</title>
   <para>
    Ao criar um novo pool, você especifica o número de grupos de posicionamento para ele (consulte a <xref linkend="ceph-pools-operate-add-pool"/>). Após adicionar mais OSDs ao cluster, normalmente você precisará aumentar o número de grupos de posicionamento também por motivos de desempenho e durabilidade dos dados. Para cada grupo de posicionamento, os nós OSD e do monitor precisam de memória, rede e CPU o tempo todo, e ainda mais durante a recuperação. Em virtude disso, a redução no número de grupos de posicionamento economiza uma quantidade significativa de recursos.
   </para>
   <warning>
    <title>Um valor muito alto de <option>pg_num</option></title>
    <para>
     Ao mudar o valor de <option>pg_num</option> de um pool, talvez o novo número de grupos de posicionamento possa exceder o limite permitido. Por exemplo
    </para>
<screen><prompt>root # </prompt>ceph osd pool set rbd pg_num 4096
 Error E2BIG: specified pg_num 3500 is too large (creating 4096 new PGs \
 on ~64 OSDs exceeds per-OSD max of 32)</screen>
    <para>
     O limite impede a divisão extrema do grupo de posicionamento e é derivado do valor <option>mon_osd_max_split_count</option>.
    </para>
   </warning>
   <para>
    Determinar o novo número correto de grupos de posicionamento para um cluster redimensionado é uma tarefa complexa. Uma abordagem é aumentar continuamente o número de grupos de posicionamento até o estado de desempenho ideal do cluster. Para determinar o novo número incrementado de grupos de posicionamento, você precisa obter o valor do parâmetro <option>mon_osd_max_split_count</option> e adicioná-lo ao número atual de grupos de posicionamento. Para ter uma ideia básica, observe o seguinte script:
   </para>
<screen><prompt>cephadm &gt; </prompt>max_inc=`ceph daemon mon.a config get mon_osd_max_split_count 2&gt;&amp;1 \
  | tr -d '\n ' | sed 's/.*"\([[:digit:]]\+\)".*/\1/'`
<prompt>cephadm &gt; </prompt>pg_num=`ceph osd pool get rbd pg_num | cut -f2 -d: | tr -d ' '`
<prompt>cephadm &gt; </prompt>echo "current pg_num value: $pg_num, max increment: $max_inc"
<prompt>cephadm &gt; </prompt>next_pg_num="$(($pg_num+$max_inc))"
<prompt>cephadm &gt; </prompt>echo "allowed increment of pg_num: $next_pg_num"</screen>
   <para>
    Depois de descobrir o próximo número de grupos de posicionamento, aumente-o com
   </para>
<screen><prompt>root # </prompt>ceph osd pool set <replaceable>pool_name</replaceable> pg_num <replaceable>next_pg_num</replaceable></screen>
  </sect2>

  <sect2 xml:id="storage-bp-cluster-mntc-add-pool">
   <title>Adicionando um pool</title>
   <para>
    Depois que você implantar um cluster pela primeira vez, o Ceph usará os pools padrão para armazenar os dados. Depois disso, você poderá criar um novo pool com
   </para>
<screen><prompt>root # </prompt>ceph osd pool create</screen>
   <para>
    Para obter mais informações sobre a criação de pools de cluster, consulte a <xref linkend="ceph-pools-operate-add-pool"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="pools-migration">
  <title>Migração de pool</title>

  <para>
   Ao criar um pool (consulte a <xref linkend="ceph-pools-operate-add-pool"/>), você precisa especificar os parâmetros iniciais, como o tipo de pool ou o número de grupos de posicionamento. Mais tarde, se você decidir mudar qualquer um desses parâmetros após inserir dados no pool, será necessário migrar os dados do pool para outro cujos parâmetros sejam adequados à sua implantação.
  </para>

  <para>
   Há vários métodos de migração de pool. É recomendável usar a <emphasis>camada de cache</emphasis>, pois esse método é transparente, reduz o tempo de espera do cluster e evita a duplicação de todos os dados do pool.
  </para>

  <sect2 xml:id="pool-migrate-cache-tier">
   <title>Migrar usando a camada de cache</title>
   <para>
    O princípio é simples: incluir o pool que você precisa migrar para a camada de cache na ordem inversa. Obtenha mais detalhes sobre as camadas de cache no <xref linkend="cha-ceph-tiered"/>. Por exemplo, para migrar um pool replicado denominado “testpool” para um pool com codificação de eliminação, siga estas etapas:
   </para>
   <procedure>
    <title>Migrando o pool replicado para o pool com codificação de eliminação</title>
    <step>
     <para>
      Crie um novo pool com codificação de eliminação chamado “newpool”:
     </para>
<screen>
<prompt>root@minion &gt; </prompt>ceph osd pool create newpool 4096 4096 erasure default
</screen>
     <para>
      Agora você tem dois pools: o “testpool” replicado original preenchido com dados e o novo “newpool” com codificação de eliminação vazio:
     </para>
     <figure>
      <title>Pools antes da migração</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate1.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate1.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Configure a camada de cache e defina o pool replicado “testpool” como o pool de cache:
     </para>
<screen>
<prompt>root@minion &gt; </prompt>ceph osd tier add newpool testpool --force-nonempty
<prompt>root@minion &gt; </prompt>ceph osd cache-mode testpool forward
</screen>
     <para>
      A partir deste momento, todos os objetos novos serão criados no novo pool:
     </para>
     <figure>
      <title>Configuração da camada de cache</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate2.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate2.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Force o pool de cache a mover todos os objetos para o novo pool:
     </para>
<screen>
<prompt>root@minion &gt; </prompt>rados -p testpool cache-flush-evict-all
</screen>
     <figure>
      <title>Descarregamento de dados</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate3.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate3.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Alterne todos os clientes para o novo pool. Até todos os dados serem descarregados para o novo pool com codificação de eliminação, você precisa especificar uma sobreposição para que esses objetos sejam pesquisados no pool antigo:
     </para>
<screen>
<prompt>root@minion &gt; </prompt>ceph osd tier set-overlay newpool testpool
</screen>
     <para>
      Com a sobreposição, todas as operações são encaminhadas para o “testpool” replicado antigo:
     </para>
     <figure>
      <title>Definindo a sobreposição</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate4.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate4.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      Agora você pode alternar todos os clientes para acessar objetos no novo pool.
     </para>
    </step>
    <step>
     <para>
      Após a migração de todos os dados para o “newpool” com codificação de eliminação, remova a sobreposição e o pool de cache antigo “testpool”:
     </para>
<screen>
<prompt>root@minion &gt; </prompt>ceph osd tier remove-overlay newpool
<prompt>root@minion &gt; </prompt>ceph osd tier remove newpool testpool
</screen>
     <figure>
      <title>Migração concluída</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate5.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate5.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="cha-ceph-snapshots-pool">
  <title>Instantâneos de pool</title>

  <para>
   Os instantâneos de pool são capturados com base no estado do pool inteiro do Ceph. Com os instantâneos de pool, você pode manter o histórico de estado do pool. Dependendo do tamanho do pool, a criação de instantâneos de pool pode exigir bastante espaço de armazenamento. Confira sempre se há espaço em disco suficiente no armazenamento relacionado antes de criar um instantâneo de um pool.
  </para>

  <sect2>
   <title>Criar um instantâneo de um pool</title>
   <para>
    Para criar um instantâneo de um pool, execute:
   </para>
<screen><prompt>root # </prompt>ceph osd pool mksnap <replaceable>pool-name</replaceable> <replaceable>snap-name</replaceable></screen>
   <para>
    Por exemplo:
   </para>
<screen><prompt>root # </prompt>ceph osd pool mksnap pool1 snapshot1
created pool pool1 snap snapshot1</screen>
  </sect2>

  <sect2>
   <title>Remover um instantâneo de um pool</title>
   <para>
    Para remover um instantâneo de um pool, execute:
   </para>
<screen><prompt>root # </prompt>ceph osd pool rmsnap <replaceable>pool-name</replaceable> <replaceable>snap-name</replaceable></screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-ceph-pool-compression">
  <title>Compactação de dados</title>

  <para>
   A partir do SUSE Enterprise Storage 5, o BlueStore fornece compactação de dados sob demanda para economizar espaço em disco.
  </para>

  <sect2 xml:id="sec-ceph-pool-compression-enable">
   <title>Habilitar compactação</title>
   <para>
    É possível habilitar a compactação de dados para um pool com:
   </para>
<screen><prompt>root # </prompt><command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> ompression_algorithm snappy
<prompt>root # </prompt><command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> compression_mode aggressive</screen>
   <para>
    Substitua <replaceable>POOL_NAME</replaceable> pelo pool no qual a compactação será habilitada.
   </para>
  </sect2>

  <sect2 xml:id="sec-ceph-pool-compression-options">
   <title>Opções de compactação de pool</title>
   <para>
    Uma lista completa de configurações de compactação:
   </para>
   <variablelist>
    <varlistentry>
     <term>compression_algorithm</term>
     <listitem>
      <para>
       Valores: <literal>none</literal>, <literal>zstd</literal>, <literal>snappy</literal>. Padrão: <literal>snappy</literal>.
      </para>
      <para>
       O algoritmo de compactação a ser usado depende do caso de uso específico. Veja a seguir várias recomendações:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Não use <literal>zlib</literal>: os outros algoritmos são melhores.
        </para>
       </listitem>
       <listitem>
        <para>
         Se você precisa de uma boa taxa de compactação, use <literal>zstd</literal>. Observe que <literal>zstd</literal> não é recomendado para o BlueStore por causa do alto overhead de CPU ao comprimir pequenas quantidades de dados.
        </para>
       </listitem>
       <listitem>
        <para>
         Se você precisa de uso menor da CPU, use <literal>lz4</literal> ou <literal>snappy</literal>.
        </para>
       </listitem>
       <listitem>
        <para>
         Realize um benchmark desses algoritmos em uma amostra dos dados reais e observe o uso de CPU e memória do cluster.
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_mode</term>
     <listitem>
      <para>
       Valor: {<literal>none</literal>, <literal>aggressive</literal>, <literal>passive</literal>, <literal>force</literal>}. Padrão: <literal>none</literal>.
      </para>
      <itemizedlist>
       <listitem>
        <para>
         <literal>none</literal>: nunca comprimir
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>passive</literal>: comprimir se houver a dica <literal>COMPRESSIBLE</literal>
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>aggressive</literal>: comprimir, exceto se houver a dica <literal>INCOMPRESSIBLE</literal>
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>force</literal>: sempre comprimir
        </para>
       </listitem>
      </itemizedlist>
      <para>
       Para obter informações sobre como definir o flag <literal>COMPRESSIBLE</literal> ou <literal>INCOMPRESSIBLE</literal>, consulte <link xlink:href="http://docs.ceph.com/docs/doc-12.2.0-major-changes/rados/api/librados/#rados_set_alloc_hint"/>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_required_ratio</term>
     <listitem>
      <para>
       Valor: Duplo, Taxa = SIZE_COMPRESSED / SIZE_ORIGINAL. Padrão: <literal>.875</literal>
      </para>
      <para>
       Os objetos acima dessa taxa não serão comprimidos por causa do baixo ganho líquido.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_max_blob_size</term>
     <listitem>
      <para>
       Valor: Número Inteiro Não Assinado, tamanho em bytes. Padrão: <literal>0</literal>
      </para>
      <para>
       Tamanho mínimo dos objetos que serão comprimidos.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_min_blob_size</term>
     <listitem>
      <para>
       Valor: Número Inteiro Não Assinado, tamanho em bytes. Padrão: <literal>0</literal>
      </para>
      <para>
       Tamanho máximo dos objetos que serão comprimidos.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="sec-ceph-pool-bluestore-compression-options">
   <title>Opções globais de compactação</title>
   <para>
    As seguintes opções de configuração podem ser definidas na configuração do Ceph e aplicam-se a todos os OSDs, não apenas a um único pool. A configuração específica do pool listada na <xref linkend="sec-ceph-pool-compression-options"/> tem prioridade.
   </para>
   <variablelist>
    <varlistentry>
     <term>bluestore_compression_algorithm</term>
     <listitem>
      <para>
       Valores: <literal>none</literal>, <literal>zstd</literal>, <literal>snappy</literal>, <literal>zlib</literal>. Padrão: <literal>snappy</literal>.
      </para>
      <para>
       O algoritmo de compactação a ser usado depende do caso de uso específico. Veja a seguir várias recomendações:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Não use <literal>zlib</literal>, os outros algoritmos são melhores.
        </para>
       </listitem>
       <listitem>
        <para>
         Se você precisa de uma boa taxa de compactação, use <literal>zstd</literal>. Observe que <literal>zstd</literal> não é recomendado para o BlueStore por causa do alto overhead de CPU ao comprimir pequenas quantidades de dados.
        </para>
       </listitem>
       <listitem>
        <para>
         Se você precisa de uso menor da CPU, use <literal>lz4</literal> ou <literal>snappy</literal>.
        </para>
       </listitem>
       <listitem>
        <para>
         Realize um benchmark desses algoritmos em uma amostra dos dados reais e observe o uso de CPU e memória do cluster.
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_mode</term>
     <listitem>
      <para>
       Valor: {<literal>none</literal>, <literal>aggressive</literal>, <literal>passive</literal>, <literal>force</literal>}. Padrão: <literal>none</literal>.
      </para>
      <itemizedlist>
       <listitem>
        <para>
         <literal>none</literal>: nunca comprimir
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>passive</literal>: comprimir se houver a dica <literal>COMPRESSIBLE</literal>.
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>aggressive</literal>: comprimir, exceto se houver a dica <literal>INCOMPRESSIBLE</literal>
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>force</literal>: sempre comprimir
        </para>
       </listitem>
      </itemizedlist>
      <para>
       Para obter informações sobre como definir o flag <literal>COMPRESSIBLE</literal> ou <literal>INCOMPRESSIBLE</literal>, consulte <link xlink:href="http://docs.ceph.com/docs/doc-12.2.0-major-changes/rados/api/librados/#rados_set_alloc_hint"/>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_required_ratio</term>
     <listitem>
      <para>
       Valor: Duplo, Taxa = SIZE_COMPRESSED / SIZE_ORIGINAL. Padrão: <literal>.875</literal>
      </para>
      <para>
       Os objetos acima dessa taxa não serão comprimidos por causa do baixo ganho líquido.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size</term>
     <listitem>
      <para>
       Valor: Número Inteiro Não Assinado, tamanho em bytes. Padrão: <literal>0</literal>
      </para>
      <para>
       Tamanho mínimo dos objetos que serão comprimidos.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size</term>
     <listitem>
      <para>
       Valor: Número Inteiro Não Assinado, tamanho em bytes. Padrão: <literal>0</literal>
      </para>
      <para>
       Tamanho máximo dos objetos que serão comprimidos.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size_ssd</term>
     <listitem>
      <para>
       Valor: Número Inteiro Não Assinado, tamanho em bytes. Padrão: <literal>8K</literal>
      </para>
      <para>
       Tamanho mínimo dos objetos que serão comprimidos e armazenados na unidade de estado sólido.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size_ssd</term>
     <listitem>
      <para>
       Valor: Número Inteiro Não Assinado, tamanho em bytes. Padrão: <literal>64K</literal>
      </para>
      <para>
       Tamanho máximo dos objetos que serão comprimidos e armazenados na unidade de estado sólido.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size_hdd</term>
     <listitem>
      <para>
       Valor: Número Inteiro Não Assinado, tamanho em bytes. Padrão: <literal>128K</literal>
      </para>
      <para>
       Tamanho mínimo dos objetos que serão comprimidos e armazenados em discos rígidos.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size_hdd</term>
     <listitem>
      <para>
       Valor: Número Inteiro Não Assinado, tamanho em bytes. Padrão: <literal>512K</literal>
      </para>
      <para>
       Tamanho máximo dos objetos que serão comprimidos e armazenados em discos rígidos.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
</chapter>
