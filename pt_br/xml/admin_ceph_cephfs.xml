<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_cephfs.xml" version="5.0" xml:id="cha-ceph-cephfs">


 <title>Sistema de arquivos em cluster</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>editando</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes (sim)</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  Este capítulo descreve as tarefas de administração que normalmente são executadas depois que o cluster é configurado e o CephFS é exportado. Se você precisar de mais informações sobre como configurar o CephFS, consulte o <xref linkend="cha-ceph-as-cephfs"/>.
 </para>
 <sect1 xml:id="ceph-cephfs-cephfs-mount">
  <title>Montando o CephFS</title>

  <para>
   Quando o sistema de arquivos é criado e o MDS está ativo, você está pronto para montar o sistema de arquivos de um host de cliente.
  </para>

  <sect2 xml:id="cephfs-client-preparation">
   <title>Preparação do cliente</title>
   <para>
    Se o host de cliente executa o SUSE Linux Enterprise 12 SP2 ou SP3, você pode ignorar esta seção, já que o sistema está pronto para montar o CephFS ''out-of-the-box”.
   </para>
   <para>
    Se o host de cliente executa o SUSE Linux Enterprise 12 SP1, você precisa aplicar todos os patches mais recentes antes de montar o CephFS.
   </para>
   <para>
    Em qualquer caso, tudo o que é preciso para montar o CephFS está incluído no SUSE Linux Enterprise. O produto SUSE Enterprise Storage não é necessário.
   </para>
   <para>
    Para suportar a sintaxe completa de <command>mount</command>, o pacote
    <package>ceph-common</package> (que é fornecido com o SUSE Linux Enterprise) deve ser instalado antes de tentar montar o CephFS.
   </para>
  </sect2>

  <sect2 xml:id="Creating-Secret-File">
   <title>Criar um arquivo de segredo</title>
   <para>
    Por padrão, o cluster do Ceph é executado com a autenticação ativada. Você deve criar um arquivo que armazena sua chave secreta (não o chaveiro propriamente dito). Para obter a chave secreta para determinado usuário e, em seguida, criar o arquivo, faça o seguinte:
   </para>
   <procedure>
    <title>Criando uma chave secreta</title>
    <step>
     <para>
      Veja a chave do usuário específico em um arquivo de chaveiro:
     </para>
<screen>cat /etc/ceph/ceph.client.admin.keyring</screen>
    </step>
    <step>
     <para>
      Copie a chave do usuário que utilizará o sistema de arquivos Ceph FS montado. Normalmente, a chave tem a seguinte aparência:
     </para>
<screen>AQCj2YpRiAe6CxAA7/ETt7Hcl9IyxyYciVs47w==</screen>
    </step>
    <step>
     <para>
      Crie um arquivo com o nome de usuário como parte do nome de arquivo. Por exemplo, <filename>/etc/ceph/admin.secret</filename> para o usuário <emphasis>admin</emphasis>.
     </para>
    </step>
    <step>
     <para>
      Cole o valor da chave no arquivo criado na etapa anterior.
     </para>
    </step>
    <step>
     <para>
      Defina os direitos de acesso apropriados para o arquivo. O usuário deve ser a única pessoa que pode ler o arquivo. Outras pessoas não devem ter nenhum direito de acesso.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ceph-cephfs-krnldrv">
   <title>Montar o CephFS</title>
   <para>
    Você pode montar o CephFS com o comando <command>mount</command>. Você precisa especificar o nome de host ou endereço IP do monitor. Como a autenticação <systemitem>cephx</systemitem> está habilitada por padrão no SUSE Enterprise Storage, você precisa especificar um nome de usuário e também o segredo relacionado:
   </para>
<screen>sudo mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secret=AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</screen>
   <para>
    Como o comando anterior permanece no histórico do shell, uma abordagem mais segura é ler o segredo de um arquivo:
   </para>
<screen>sudo mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    Observe que o arquivo de segredo deve conter apenas o segredo do chaveiro real. Em nosso exemplo, o arquivo incluirá apenas a seguinte linha:
   </para>
<screen>AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</screen>
   <tip>
    <title>Especificar vários monitores</title>
    <para>
     Convém especificar vários monitores separados por vírgulas na linha de comando <command>mount</command> para o caso de um monitor ficar inativo no momento da montagem. Cada endereço de monitor adota o formato <literal>host[:porta]</literal>. Se a porta não for especificada, será usado o padrão 6789.
    </para>
   </tip>
   <para>
    Crie o ponto de montagem no host local:
   </para>
<screen>sudo mkdir /mnt/cephfs</screen>
   <para>
    Monte o CephFS:
   </para>
<screen>sudo mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    Um subdiretório <filename>subdir</filename> poderá ser especificado se um subconjunto do sistema de arquivos tiver que ser montado:
   </para>
<screen>sudo mount -t ceph ceph_mon1:6789:/subdir /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    Você pode especificar mais de um host de monitor no comando <command>mount</command>:
   </para>
<screen>sudo mount -t ceph ceph_mon1,ceph_mon2,ceph_mon3:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <important>
    <title>Acesso de leitura ao diretório raiz</title>
    <para>
     Se forem usados clientes com restrição de caminho, os recursos do MDS precisarão incluir o acesso de leitura ao diretório raiz. Por exemplo, um chaveiro pode ter a seguinte aparência:
    </para>
<screen>client.bar
 key: supersecretkey
 caps: [mds] allow rw path=/barjail, allow r path=/
 caps: [mon] allow r
 caps: [osd] allow rwx</screen>
    <para>
     A parte <literal>allow r path=/</literal> indica que os clientes com restrição de caminho podem ver o volume raiz, mas não podem gravar nele. Isso pode ser um problema para casos de uso em que o isolamento completo é um requisito.
    </para>
   </important>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs-unmount">
  <title>Desmontando o CephFS</title>

  <para>
   Para desmontar o CephFS, use o comando <command>umount</command>:
  </para>

<screen>sudo umount /mnt/cephfs</screen>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs-fstab">
  <title>CephFS no <filename>/etc/fstab</filename></title>

  <para>
   Para montar o CephFS automaticamente na inicialização do cliente, insira a linha correspondente na respectiva tabela de sistemas de arquivos <filename>/etc/fstab</filename>:
  </para>

<screen>mon1:6790,mon2:/subdir /mnt/cephfs ceph name=admin,secretfile=/etc/ceph/secret.key,noatime,_netdev 0 2</screen>
 </sect1>
 <sect1 xml:id="ceph-cephfs-activeactive">
  <title>Vários daemons MDS ativos (MDS ativo-ativo)</title>

  <para>
   Por padrão, o CephFS é configurado para um único daemon MDS ativo. Para aumentar o desempenho dos metadados em sistemas de grande escala, é possível habilitar vários daemons MDS ativos, que compartilharão a carga de trabalho dos metadados entre eles.
  </para>

  <sect2>
   <title>Quando usar MDS ativo-ativo</title>
   <para>
    Considere o uso de vários daemons MDS ativos em caso de gargalo no desempenho dos metadados no MDS único padrão.
   </para>
   <para>
    A adição de mais daemons não aumenta o desempenho em todos os tipos de carga de trabalho. Por exemplo, um único aplicativo em execução em um só cliente não se beneficiará de um número maior de daemons MDS, a menos que o aplicativo esteja efetuando muitas operações de metadados em paralelo.
   </para>
   <para>
    As cargas de trabalho que costumam se beneficiar de um número maior de daemons MDS ativos são aquelas com vários clientes, que podem atuar em muitos diretórios separados.
   </para>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-increase">
   <title>Aumentando o tamanho do cluster MDS ativo</title>
   <para>
    Cada sistema de arquivos CephFS tem uma configuração <option>max_mds</option> que controla quantas classificações serão criadas. O número real de classificações no sistema de arquivos apenas será aumentado se um daemon sobressalente estiver disponível para assumir a nova classificação. Por exemplo, se houver apenas um daemon MDS em execução, e <option>max_mds</option> estiver definido como dois, não será criada uma segunda classificação.
   </para>
   <para>
    No exemplo a seguir, definimos a opção <option>max_mds</option> como 2 para criar uma nova classificação separadamente do padrão. Para ver as mudanças, execute <command>ceph status</command> antes e depois que você definir <option>max_mds</option> e observe a linha que contém <literal>fsmap</literal>:
   </para>
<screen><prompt>root@master # </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby
    [...]
<prompt>root@master # </prompt><command>ceph</command> mds set max_mds 2
<prompt>root@master # </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/2 up  {0=node2=up:active,1=node1=up:active}
    [...]</screen>
   <para>
    A classificação recém-criada (1) passa pelo estado de “criação” e depois entra no estado “ativo”.
   </para>
   <important>
    <title>Daemons de standby</title>
    <para>
     Mesmo com vários daemons MDS ativos, um sistema altamente disponível ainda requer daemons de standby para assumir o controle em caso de falha em qualquer um dos servidores que executam um daemon ativo.
    </para>
    <para>
     Consequentemente, o limite máximo ideal de <option>max_mds</option> para sistemas de alta disponibilidade é menor do que o número total de servidores MDS no sistema. Para se manter disponível em caso de várias falhas do servidor, aumente o número de daemons de standby no sistema para corresponder ao número de falhas do servidor que você precisa superar.
    </para>
   </important>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-decrease">
   <title>Diminuindo o número de classificações</title>
   <para>
    Todas as classificações, incluindo as que devem ser removidas, devem primeiro estar ativas. Isso significa que é necessário ter pelo menos <option>max_mds</option> daemons MDS disponíveis.
   </para>
   <para>
    Primeiramente, defina <option>max_mds</option> como um número mais baixo. Por exemplo, volte a ter um único MDS ativo:
   </para>
<screen><prompt>root@master # </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/2 up  {0=node2=up:active,1=node1=up:active}
    [...]
<prompt>root@master # </prompt><command>ceph</command> mds set max_mds 1
<prompt>root@master # </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby
    [...]</screen>
   <para>
    Observe que ainda temos dois MDS ativos. As classificações ainda existem, apesar de termos reduzido <option>max_mds</option>, pois <option>max_mds</option> apenas restringe a criação de novas classificações.
   </para>
   <para>
    Em seguida, use o comando <command>ceph mds deactivate <replaceable>rank</replaceable></command> para remover a classificação desnecessária:
   </para>
<screen><prompt>root@master # </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/1 up  {0=node2=up:active,1=node1=up:active}
<prompt>root@master # </prompt><command>ceph</command> mds deactivate 1
telling mds.1:1 192.168.58.101:6805/2799214375 to deactivate

<prompt>root@master # </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/1 up  {0=node2=up:active,1=node1=up:stopping}

<prompt>root@master # </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby</screen>
   <para>
    A classificação desativada entrará primeiro no estado de interrupção, por um período enquanto ela transmite sua cota de metadados para os daemons ativos restantes. Essa fase pode levar de segundos a minutos. Se o MDS parecer travado no estado de interrupção, isso deverá ser investigado como um possível bug.
   </para>
   <para>
    Se um daemon MDS falhar ou for terminado durante o estado de “interrupção”, um standby assumirá o controle, e a classificação voltará para o estado “ativo”. Você poderá tentar desativá-la novamente quando ela voltar a ser ativa.
   </para>
   <para>
    Quando um daemon sair do estado de interrupção, ele será iniciado novamente e voltará a ser standby.
   </para>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-pinning">
   <title>Fixando manualmente árvores do diretório em uma classificação</title>
   <para>
    Em várias configurações de servidor de metadados ativas, um balanceador é executado, que funciona para distribuir a carga de metadados igualmente no cluster. Em geral, isso funciona bem o suficiente para a maioria dos usuários; mas, às vezes, convém anular o balanceador dinâmico com mapeamentos explícitos de metadados para classificações específicas. Isso pode permitir que o administrador ou os usuários distribuam a carga do aplicativo igualmente ou limitem o impacto das solicitações de metadados dos usuários sobre o cluster inteiro.
   </para>
   <para>
    O mecanismo fornecido para essa finalidade é chamado “export pin”. Ele é um atributo estendido de diretórios. O nome desse atributo estendido é <literal>ceph.dir.pin</literal>. Os usuários podem definir esse atributo usando os comandos padrão:
   </para>
<screen>setfattr -n ceph.dir.pin -v 2 <replaceable>/path/to/dir</replaceable></screen>
   <para>
    O valor (<option>-v</option>) do atributo estendido é a classificação à qual atribuir a subárvore do diretório. O valor padrão -1 indica que o diretório não foi fixado.
   </para>
   <para>
    O export pin de um diretório é herdado do seu pai mais próximo com um export pin definido. Portanto, a definição do export pin em um diretório afeta todos os seus filhos. No entanto, a fixação do pai pode ser anulada pela definição do export pin do diretório filho. Por exemplo:
   </para>
<screen>mkdir -p a/b                      # "a" and "a/b" start with no export pin set.
setfattr -n ceph.dir.pin -v 1 a/  # "a" and "b" are now pinned to rank 1.
setfattr -n ceph.dir.pin -v 0 a/b # "a/b" is now pinned to rank 0
                                  # and "a/" and the rest of its children
                                  # are still pinned to rank 1.</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-failover">
  <title>Failover de Gerenciamento</title>

  <para>
   Se um daemon MDS parar de se comunicar com o monitor, o monitor aguardará <option>mds_beacon_grace</option> segundos (o padrão é 15 segundos) antes de marcar o daemon como <emphasis>lento</emphasis>. Você pode configurar um ou mais daemons de “standby” para assumir o controle durante o failover do daemon MDS.
  </para>

  <sect2 xml:id="ceph-cephfs-failover-standby">
   <title>Configurando daemons de standby</title>
   <para>
    Há várias configurações que controlam o comportamento de um daemon durante o standby. Você pode especificá-las no <filename>ceph.conf</filename> no host em que o daemon MDS é executado. O daemon carrega essas configurações quando é iniciado e as envia para o monitor.
   </para>
   <para>
    Por padrão, se nenhuma dessas configurações for usada, todos os daemons MDS que não tiverem uma classificação serão usados como “standby” para qualquer classificação.
   </para>
   <para>
    As configurações que associam um daemon de standby a determinado nome ou classificação não garantem que o daemon apenas será usado para essa classificação. Quando há vários standbys disponíveis, elas indicam que o daemon de standby associado será usado. Em caso de falha em uma classificação, e se um standby estiver disponível, ele será usado mesmo se estiver associado a uma diferente classificação ou daemon nomeado.
   </para>
   <variablelist>
    <varlistentry>
     <term>mds_standby_replay</term>
     <listitem>
      <para>
       Se definido como verdadeiro, o daemon de standby lerá continuamente o diário de metadados de uma classificação ativa. Isso resulta em um cache de metadados a quente e acelera o processo de failover em caso de falha no daemon que está assumindo a classificação.
      </para>
      <para>
       Uma classificação ativa pode ter apenas um daemon de reprodução de standby atribuído a ela. Se dois daemons forem definidos como reprodução de standby, um deles vencerá arbitrariamente, e o outro se tornará um standby normal que não é de reprodução.
      </para>
      <para>
       Quando um daemon entrar no estado de reprodução de standby, ele apenas será usado como standby para a classificação que está seguindo. Se houver falha em outra classificação, esse daemon de reprodução de standby não será usado como substituição, mesmo que não haja outros standbys disponíveis.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds_standby_for_name</term>
     <listitem>
      <para>
       Defina essa opção para fazer com que o daemon de standby apenas assuma uma classificação com falha se o último daemon a mantê-la corresponder a este nome.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds_standby_for_rank</term>
     <listitem>
      <para>
       Defina essa opção para fazer com que o daemon de standby apenas assuma a classificação especificada. Se houver falha em outra classificação, esse daemon não será usado para substituí-la.
      </para>
      <para>
       Use em conjunto com <option>mds_standby_for_fscid</option> para ser específico quanto à classificação do sistema de arquivos que você pretende usar em caso de vários sistemas de arquivos.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds_standby_for_fscid</term>
     <listitem>
      <para>
       Se <option>mds_standby_for_rank</option> for definido, trata-se simplesmente de um qualificador que indica qual classificação do sistema de arquivos está sendo referenciada.
      </para>
      <para>
       Se <option>mds_standby_for_rank</option> não for definido, a definição de FSCID fará com que este daemon seja direcionado para qualquer classificação no FSCID especificado. Use essa opção se você tem um daemon que deseja usar para qualquer classificação, mas apenas em um sistema de arquivos específico.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mon_force_standby_active</term>
     <listitem>
      <para>
       Essa configuração é usada nos hosts de monitor. O padrão é verdadeiro.
      </para>
      <para>
       Se for falso, os daemons configurados com <option>standby_replay=true</option> apenas se tornarão ativos se houver falha na classificação/nome que foram configurados para seguir. Por outro lado, se essa configuração for verdadeira, um daemon configurado com <option>standby_replay=true</option> poderá ser atribuído a alguma outra classificação.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph-cephfs-failover-examples">
   <title>Exemplos</title>
   <para>
    Veja a seguir várias configurações do <filename>ceph.conf</filename> de exemplo. Você pode copiar um <filename>ceph.conf</filename> com a configuração de todos os daemons para todos os servidores ou pode ter um arquivo diferente em cada servidor contendo a configuração do daemon desse servidor.
   </para>
   <sect3>
    <title>Par simples</title>
    <para>
     Dois daemons MDS “a” e “b” atuando como par. Aquele que não estiver atribuído a uma classificação no momento será o seguidor de reprodução de standby do outro.
    </para>
<screen>[mds.a]
mds standby replay = true
mds standby for rank = 0

[mds.b]
mds standby replay = true
mds standby for rank = 0</screen>
   </sect3>

  </sect2>
 </sect1>
</chapter>
