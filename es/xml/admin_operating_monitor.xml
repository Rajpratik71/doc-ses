<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_operating_monitor.xml" version="5.0" xml:id="ceph-monitor">
 <title>Determinación del estado del clúster</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:translation>sí</dm:translation>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  Si dispone de un clúster en ejecución, puede utilizar la herramienta <command>ceph</command> para supervisarlo. Normalmente, determinar el estado del clúster implica comprobar su estado de OSD, monitor, grupo de colocación y servidor de metadatos.<remark role="fixme">Maybe revert to old version of sentence: Determining the cluster state typically involves
  checking OSD status, monitor status, placement group status and metadata
  server status.</remark>
 </para>
 <tip>
  <title>modo interactivo</title>
  <para>
   Para ejecutar la herramienta <command>ceph</command> en el modo interactivo, escriba <command>ceph</command> en la línea de comandos sin ningún argumento. El modo interactivo es más cómodo si se van a introducir más comandos <command>ceph</command> en una fila. Por ejemplo:
  </para>
<screen><prompt>cephadm &gt; </prompt>ceph
ceph&gt; health
ceph&gt; status
ceph&gt; quorum_status
ceph&gt; mon_status</screen>
 </tip>
 <sect1 xml:id="monitor-health">
  <title>Comprobación del estado del clúster</title>

  <para>
   Una vez iniciado el clúster y antes de empezar a leer o escribir datos, compruebe su estado:
  </para>

<screen><prompt>root # </prompt>ceph health
HEALTH_WARN 10 pgs degraded; 100 pgs stuck unclean; 1 mons down, quorum 0,2 \
node-1,node-2,node-3</screen>

  <para>
   El clúster de Ceph devolverá uno de los siguientes códigos de estado:
  </para>

  <variablelist>
   <varlistentry>
    <term>OSD_DOWN</term>
    <listitem>
     <para>
      Uno o varios OSD están señalados como inactivos. Se ha detenido el daemon OSD o los pares OSD no pueden acceder al OSD a través de la red. Algunas de las causas pueden ser un daemon detenido o bloqueado, un host caído o una interrupción de la red.
     </para>
     <para>
      Compruebe que el host está en buen estado, el daemon se inicia y la red está funcionando. Si el daemon se ha bloqueado, el archivo de registro de daemon (<filename>/var/log/ceph/ceph-osd.*</filename>) puede contener información de depuración.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_<replaceable>tipo de bloqueo</replaceable>_DOWN; por ejemplo: OSD_HOST_DOWN</term>
    <listitem>
     <para>
      Todos los OSD con un determinado subárbol CRUSH se marcan como inactivos, por ejemplo, todos los OSD de un host.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_ORPHAN</term>
    <listitem>
     <para>
      Se hace referencia a un OSD en la jerarquía de asignaciones CRUSH, pero no existe. El OSD se puede quitar de la jerarquía CRUSH con:
     </para>
<screen><prompt>root # </prompt>ceph osd crush rm osd.<replaceable>ID</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_OUT_OF_ORDER_FULL</term>
    <listitem>
     <para>
      Los umbrales de uso para <emphasis>backfillfull</emphasis>, <emphasis>nearfull</emphasis>, <emphasis>full</emphasis> o <emphasis>failsafe_full</emphasis> no son ascendentes. En concreto, esperamos <emphasis>backfillfull</emphasis> &lt; <emphasis>nearfull</emphasis>, <emphasis>nearfull</emphasis> &lt; <emphasis>full</emphasis> y <emphasis>full</emphasis> &lt; <emphasis>failsafe_full</emphasis>. Los umbrales se pueden ajustar con:
     </para>
<screen><prompt>root # </prompt>ceph osd set-backfillfull-ratio <replaceable>ratio</replaceable>
<prompt>root # </prompt>ceph osd set-nearfull-ratio <replaceable>ratio</replaceable>
<prompt>root # </prompt>ceph osd set-full-ratio <replaceable>ratio</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_FULL</term>
    <listitem>
     <para>
      Uno o varios OSD han excedido el umbral <emphasis>full</emphasis>, lo que impide que el clúster de servicio a las operaciones de escritura. El uso del repositorio se puede comprobar con:
     </para>
<screen><prompt>root # </prompt>ceph df</screen>
     <para>
      La proporción de <emphasis>full</emphasis> definida actualmente se puede consultar con:
     </para>
<screen><prompt>root # </prompt>ceph osd dump | grep full_ratio</screen>
     <para>
      Una solución a corto plazo para restaurar la disponibilidad de las operaciones de escritura es aumentar el umbral "full" en una pequeña cantidad:
     </para>
<screen><prompt>root # </prompt>ceph osd set-full-ratio <replaceable>ratio</replaceable></screen>
     <para>
      Añada nuevo espacio de almacenamiento al clúster mediante la implantación de más OSD o suprima datos existentes para liberar espacio.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_BACKFILLFULL</term>
    <listitem>
     <para>
      Uno o varios OSD han excedido el umbral <emphasis>backfillfull</emphasis>, lo que impide que los datos se puedan reequilibrar en este dispositivo. Se trata de una advertencia previa de que podría resultar imposible completar el reequilibrio y de que el clúster está casi lleno. El uso del repositorio se puede comprobar con:
     </para>
<screen><prompt>root # </prompt>ceph df</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_NEARFULL</term>
    <listitem>
     <para>
      Uno o varios OSD han excedido el umbral <emphasis>nearfull</emphasis>. Se trata de una advertencia previa de que el clúster está casi lleno. El uso del repositorio se puede comprobar con:
     </para>
<screen><prompt>root # </prompt>ceph df</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSDMAP_FLAGS</term>
    <listitem>
     <para>
      Se han definido uno o varios indicadores de interés en el clúster. Con la excepción de <emphasis>full</emphasis>, estos indicadores se pueden establecer o borrar con:
     </para>
<screen><prompt>root # </prompt>ceph osd set <replaceable>flag</replaceable>
<prompt>root # </prompt>ceph osd unset <replaceable>flag</replaceable></screen>
     <para>
      Los indicadores son los siguientes:
     </para>
     <variablelist>
      <varlistentry>
       <term>full</term>
       <listitem>
        <para>
         El clúster se marca como lleno y no permite realizar operaciones de escritura.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>pauserd, pausewr </term>
       <listitem>
        <para>
         Operaciones de lectura o de escritura en pausa.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noup</term>
       <listitem>
        <para>
         No se pueden iniciar los OSD.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nodown</term>
       <listitem>
        <para>
         Se omiten los informes de fallo de los OSD, de modo que los monitores no marquen los OSD como caídos (<emphasis>down</emphasis>).
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noin</term>
       <listitem>
        <para>
         Los OSD que se hayan marcado como <emphasis>out</emphasis> (fuera) no se volverán a marcar como <emphasis>in</emphasis> (dentro) cuando se inicien.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noout</term>
       <listitem>
        <para>
         Los OSD caídos (<emphasis>down</emphasis>) no se marcarán automáticamente como <emphasis>out</emphasis> (fuera) después del intervalo configurado.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nobackfill, norecover, norebalance</term>
       <listitem>
        <para>
         Las operaciones de recuperación o de reequilibrio de datos están suspendidas.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noscrub, nodeep_scrub</term>
       <listitem>
        <para>
         Las operaciones de borrado seguro (consulte la <xref linkend="scrubbing"/>) están inhabilitadas.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>notieragent</term>
       <listitem>
        <para>
         Las actividades de niveles de caché están suspendidas.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_FLAGS</term>
    <listitem>
     <para>
      Uno o varios OSD tienen establecido un indicador de interés por OSD. Los indicadores son los siguientes:
     </para>
     <variablelist>
      <varlistentry>
       <term>noup</term>
       <listitem>
        <para>
         El OSD no se puede iniciar.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nodown</term>
       <listitem>
        <para>
         Se omitirán los informes de errores para este OSD.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noin</term>
       <listitem>
        <para>
         Si este OSD se ha marcado anteriormente como <emphasis>out</emphasis> (fuera) tras un fallo, no se marcará como <emphasis>in</emphasis> (dentro) cuando se inicie.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noout</term>
       <listitem>
        <para>
         Si este OSD está apagado, no se marcará automáticamente como <emphasis>out</emphasis> (fuera) después del intervalo configurado.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <para>
      Los indicadores por OSD se pueden definir y desactivar con:
     </para>
<screen><prompt>root # </prompt>ceph osd add-<replaceable>flag</replaceable> <replaceable>osd-ID</replaceable>
<prompt>root # </prompt>ceph osd rm-<replaceable>flag</replaceable> <replaceable>osd-ID</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OLD_CRUSH_TUNABLES</term>
    <listitem>
     <para>
      La asignación CRUSH utiliza una configuración muy antigua y se debe actualizar. Los tunables más antiguos que se pueden utilizar (es decir, es la versión más antigua del cliente que se puede conectar al clúster) sin que se active esta advertencia de estado está determinada por la opción de configuración <option>mon_crush_min_required_version</option>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OLD_CRUSH_STRAW_CALC_VERSION</term>
    <listitem>
     <para>
      La asignación CRUSH utiliza un método más antiguo y no óptimo para calcular los valores de peso intermedio para el ordenamiento por casilleros. La asignación CRUSH debe actualizarse para utilizar el método más reciente (<option>straw_calc_version</option>=1).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>CACHE_POOL_NO_HIT_SET</term>
    <listitem>
     <para>
      Uno o varios repositorios de caché no están configurados con un conjunto de resultados para realizar el seguimiento del uso, lo que impide que el agente de niveles de caché identifique los objetos fríos que debe limpiar y expulsar del caché. Los conjuntos de resultados se pueden configurar en el repositorio de caché con:
     </para>
<screen><prompt>root # </prompt>ceph osd pool set <replaceable>poolname</replaceable> hit_set_type <replaceable>type</replaceable>
<prompt>root # </prompt>ceph osd pool set <replaceable>poolname</replaceable> hit_set_period <replaceable>period-in-seconds</replaceable>
<prompt>root # </prompt>ceph osd pool set <replaceable>poolname</replaceable> hit_set_count <replaceable>number-of-hitsets</replaceable>
<prompt>root # </prompt>ceph osd pool set <replaceable>poolname</replaceable> hit_set_fpp <replaceable>target-false-positive-rate</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_NO_SORTBITWISE</term>
    <listitem>
     <para>
      No hay ningún OSD anterior a luminous v12 en ejecución, pero no se ha definido el indicador <option>sortbitwise</option>. Debe establecer el indicador <option>sortbitwise</option> para que se puedan iniciar los OSD luminous v12 o versiones más recientes:
     </para>
<screen><prompt>root # </prompt>ceph osd set sortbitwise</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_FULL</term>
    <listitem>
     <para>
      Uno o varios repositorios han alcanzado su cuota y ya no permiten más operaciones de escritura. Es posible definir cuotas de repositorio y límites de uso con:
     </para>
<screen><prompt>root # </prompt>ceph df detail</screen>
     <para>
      Puede aumentar la cuota de repositorio con
     </para>
<screen><prompt>root # </prompt>ceph osd pool set-quota <replaceable>poolname</replaceable> max_objects <replaceable>num-objects</replaceable>
<prompt>root # </prompt>ceph osd pool set-quota <replaceable>poolname</replaceable> max_bytes <replaceable>num-bytes</replaceable></screen>
     <para>
      o suprimir algunos datos para reducir el uso.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_AVAILABILITY</term>
    <listitem>
     <para>
      Hay una disponibilidad reducida de los datos, lo que significa que el clúster no puede responder a posibles peticiones de lectura o escritura de algunos de los datos del clúster. En particular, uno o varios grupos de colocación se encuentran en un estado que no permite atender las peticiones de E/S. Los estados de los grupos de colocación afectados son <emphasis>peering</emphasis> (emparejando), <emphasis>stale</emphasis> (detenido), <emphasis>incomplete</emphasis> (incompleto) y la ausencia de <emphasis>active</emphasis> (activo) (si dichos estados no desaparecen rápidamente). Encontrará información detallada sobre los grupos de colocación afectados en:
     </para>
<screen><prompt>root # </prompt>ceph health detail</screen>
     <para>
      En la mayoría de los casos, la causa raíz es que uno o varios OSD están caídos. Se puede consultar el estado de los grupos de colocación afectados específicos con:
     </para>
<screen><prompt>root # </prompt>ceph tell <replaceable>pgid</replaceable> query</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_DEGRADED</term>
    <listitem>
     <para>
      Se reduce la redundancia de algunos datos, lo que significa que el clúster no tiene el número deseado de réplicas de todos los datos (para los repositorios replicados) o fragmentos de código de borrado (para los repositorios codificados de borrado). En concreto, uno o varios grupos de colocación tienen establecido el indicador <emphasis>degraded</emphasis> (degradado) o <emphasis>undersized</emphasis> (tamaño insuficiente) (no hay suficientes instancias de ese grupo de colocación en el clúster), o bien hace tiempo que no cuenta con el indicador <emphasis>clean</emphasis> (limpio). Encontrará información detallada sobre los grupos de colocación afectados en:
     </para>
<screen><prompt>root # </prompt>ceph health detail</screen>
     <para>
      En la mayoría de los casos, la causa raíz es que uno o varios OSD están caídos. Se puede consultar el estado de los grupos de colocación afectados específicos con:
     </para>
<screen><prompt>root # </prompt>ceph tell <replaceable>pgid</replaceable> query</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_DEGRADED_FULL</term>
    <listitem>
     <para>
      La redundancia de datos se puede reducir o estar en peligro para algunos datos debido a la falta de espacio disponible en el clúster. En concreto, uno o varios grupos de colocación tienen establecidos los indicadores <emphasis>backfill_toofull</emphasis> o <emphasis>recovery_toofull</emphasis>, lo que significa que el clúster no puede migrar o recuperar datos debido a que uno o varios OSD superan el umbral <emphasis>backfillfull</emphasis>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_DAMAGED</term>
    <listitem>
     <para>
      El proceso de borrado seguro de datos (consulte la <xref linkend="scrubbing"/>) ha detectado problemas con la coherencia de los datos en el clúster. Específicamente, hay uno o varios grupos de colocación con el indicador <emphasis>inconsistent</emphasis> o <emphasis>snaptrim_error</emphasis>, que indican que una operación anterior de borrado seguro ha detectado un problema, o bien se ha establecido el indicador <emphasis>repair</emphasis>, lo que significa que hay una reparación de dicha incoherencia en curso.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_SCRUB_ERRORS</term>
    <listitem>
     <para>
      Los procesos de borrado seguro de OSD recientes han revelado incoherencias.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>CACHE_POOL_NEAR_FULL</term>
    <listitem>
     <para>
      Un repositorio de nivel de caché está casi lleno. La capacidad en este contexto está determinada por las propiedades <emphasis>target_max_bytes</emphasis> y <emphasis>target_max_objects</emphasis> que aparecen en el repositorio de caché. Cuando el repositorio alcanza el umbral objetivo, las peticiones de escritura para el repositorio podrían bloquearse hasta que los datos se limpien y se expulsen del caché, un estado que suele producir latencias muy altas y un rendimiento deficiente. El objetivo de tamaño del repositorio de caché se puede ajustar con:
     </para>
<screen><prompt>root # </prompt>ceph osd pool set <replaceable>cache-pool-name</replaceable> target_max_bytes <replaceable>bytes</replaceable>
<prompt>root # </prompt>ceph osd pool set <replaceable>cache-pool-name</replaceable> target_max_objects <replaceable>objects</replaceable></screen>
     <para>
      Las actividades normales de limpieza y expulsión del caché también se pueden atascar por una reducción de la disponibilidad o el rendimiento del nivel base o por la carga global del clúster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>TOO_FEW_PGS</term>
    <listitem>
     <para>
      El número de grupos de colocación en uso está por debajo del umbral configurable de grupos de colocación por OSD (<option>mon_pg_warn_min_per_osd</option>). Esto puede producir una distribución y reequilibrio de datos subóptimo entre los OSD del clúster y reducir el rendimiento general.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>TOO_MANY_PGS</term>
    <listitem>
     <para>
      El número de grupos de colocación en uso está por encima del umbral configurable de grupos de colocación por OSD (<option>mon_pg_warn_max_per_osd</option>). Esto puede dar lugar a un uso más elevado de memoria para los daemons de OSD, un emparejamiento más lento después de cambios de estado del clúster (por ejemplo, reinicios, incorporaciones o eliminaciones de OSD) y una mayor carga mayor en los gestores y monitores de Ceph.
     </para>
     <para>
      No es posible reducir el valor <option>pg_num</option> para los repositorios existentes. Sin embargo, sí se puede reducir el valor <option>pgp_num</option>. A efectos prácticos, esto coloca algunos grupos de colocación en los mismos conjuntos de OSD, mitigando algunos de los impactos negativos descritos anteriormente. El valor <option>pgp_num</option> se puede ajustar con:
     </para>
<screen><prompt>root # </prompt>ceph osd pool set <replaceable>pool</replaceable> pgp_num <replaceable>value</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SMALLER_PGP_NUM</term>
    <listitem>
     <para>
      Uno o varios repositorios tienen un valor <option>pgp_num</option> inferior a <option>pg_num</option>. Por lo general, esto suele indicar que se ha aumentado el número de grupos de colocación sin aumentar al mismo tiempo el comportamiento de colocación. Normalmente, el problema se resuelve definiendo <option>pgp_num</option> para que coincida con <option>pg_num</option>, lo que activa la migración de datos:
     </para>
<screen>ceph osd pool set <replaceable>pool</replaceable> pgp_num <replaceable>pg_num_value</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>MANY_OBJECTS_PER_PG</term>
    <listitem>
     <para>
      Uno o varios repositorios tienen un número medio de objetos por grupo de colocación significativamente superior al promedio general del clúster. El umbral específico se controla mediante el valor de configuración <option>mon_pg_warn_max_object_skew</option>. Esto suele indicar que los repositorios que contienen la mayoría de los datos del clúster disponen de muy pocos grupos de colocación o que los demás repositorios, que no contienen tantos datos, tienen demasiados grupos de colocación. Se puede elevar el umbral para silenciar la advertencia de estado; para ello, ajuste la opción de configuración <option>mon_pg_warn_max_object_skew</option> en los monitores.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_APP_NOT_ENABLED</term>
    <listitem>
     <para>
      Existe un repositorio que contiene uno o varios objetos, pero no se ha etiquetado para su uso por parte de una aplicación determinada. Para resolver esta advertencia, etiquete el repositorio para su uso por parte de una aplicación. Por ejemplo, si el repositorio lo utiliza RBD:
     </para>
<screen><prompt>root # </prompt>rbd pool init <replaceable>pool_name</replaceable></screen>
     <para>
      Si el repositorio está siendo utilizado por una aplicación personalizada "foo", también se puede etiquetar mediante el comando de bajo nivel:
     </para>
<screen><prompt>root # </prompt>ceph osd pool application enable foo</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_FULL</term>
    <listitem>
     <para>
      Uno o varios repositorios han alcanzado su cuota (o van a alcanzarla muy pronto). El umbral para que se active esta situación de error depende de la opción de configuración <option>mon_pool_quota_crit_threshold</option>. Las cuotas del repositorio se pueden aumentar, reducir o eliminar con:
     </para>
<screen><prompt>root # </prompt>ceph osd pool set-quota <replaceable>pool</replaceable> max_bytes <replaceable>bytes</replaceable>
<prompt>root # </prompt>ceph osd pool set-quota <replaceable>pool</replaceable> max_objects <replaceable>objects</replaceable></screen>
     <para>
      Si define el valor de la cuota como 0, la cuota se inhabilitará.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_NEAR_FULL</term>
    <listitem>
     <para>
      Uno o varios repositorios se están aproximando a su cuota. El umbral para que se active esta situación de advertencia depende de la opción de configuración <option>mon_pool_quota_warn_threshold</option>. Las cuotas del repositorio se pueden aumentar, reducir o eliminar con:
     </para>
<screen><prompt>root # </prompt>ceph osd osd pool set-quota <replaceable>pool</replaceable> max_bytes <replaceable>bytes</replaceable>
<prompt>root # </prompt>ceph osd osd pool set-quota <replaceable>pool</replaceable> max_objects <replaceable>objects</replaceable></screen>
     <para>
      Si define el valor de la cuota como 0, la cuota se inhabilitará.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OBJECT_MISPLACED</term>
    <listitem>
     <para>
      Uno o varios objetos en el clúster no se almacenan en el nodo en el que clúster pretende almacenarlos. Esto indica que una migración de datos debida a algunos cambios recientes del clúster aún no se ha completado. El almacenamiento de datos en el lugar equivocado no es una situación peligrosa por sí misma. La coherencia de los datos nunca corre peligro y las copias antiguas de los objetos no se eliminan hasta que se alcanza el número de copias nuevas deseadas (en las ubicaciones correctas).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OBJECT_UNFOUND</term>
    <listitem>
     <para>
      No es posible encontrar uno o más objetos en el clúster. En concreto, los OSD saben que debe existir una copia nueva o actualizada de un objeto, pero no se ha encontrado ninguna copia de esa versión del objeto en los OSD que están actualmente en línea. Las peticiones de lectura o escritura a los objetos "unfound" (no encontrados) se bloquean. En condiciones idóneas, se podrá volver a conectar un OSD caído que contenga la copia más reciente del objeto no encontrado. Los OSD candidatos se pueden identificar a partir del estado de emparejamiento de los grupos de colocación responsables del objeto no encontrado:
     </para>
<screen><prompt>root # </prompt>ceph tell <replaceable>pgid</replaceable> query</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>REQUEST_SLOW</term>
    <listitem>
     <para>
      Una o varias peticiones de OSD tardan mucho tiempo en procesarse. Esto puede indicar un nivel de carga extremo, un dispositivo de almacenamiento lento o un fallo de software. Puede consultar la cola de peticiones de los OSD en cuestión ejecutando el siguiente comando desde el host OSD:
     </para>
<screen><prompt>root # </prompt>ceph daemon osd.<replaceable>id</replaceable> ops</screen>
     <para>
      Puede consultar un resumen de las peticiones más lentas realizadas recientemente:
     </para>
<screen><prompt>root # </prompt>ceph daemon osd.<replaceable>id</replaceable> dump_historic_ops</screen>
     <para>
      Puede consultar la ubicación de un OSD con:
     </para>
<screen><prompt>root # </prompt>ceph osd find osd.<replaceable>id</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>REQUEST_STUCK</term>
    <listitem>
     <para>
      Una o varias peticiones de OSD llevan bloqueadas demasiado tiempo. Esto indica que el clúster lleva un periodo prolongado de tiempo en un estado incorrecto (por ejemplo, no hay suficientes OSD en ejecución) o que hay algún problema interno con el OSD.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_NOT_SCRUBBED</term>
    <listitem>
     <para>
      A uno o varios de los grupos de colocación no se les ha aplicado el borrado seguro (consulte la <xref linkend="scrubbing"/>) recientemente. Normalmente, a los grupos de colocación se les aplica el borrado seguro cuando transcurre el número de segundos indicado en <option>mon_scrub_interval</option>; esta advertencia se activa cuando transcurren los intervalos indicados en <option>mon_warn_not_scrubbed</option> sin que se realice un borrado seguro. A los grupos de colocación no se les aplicará el borrado seguro si no están marcados como limpios; esto puede ocurrir si se extravían o se degradan (consulte PG_AVAILABILITY y PG_DEGRADED más arriba). Puede iniciar manualmente el borrado seguro de un grupo de colocación limpio con:
     </para>
<screen><prompt>root # </prompt>ceph pg scrub <replaceable>pgid</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_NOT_DEEP_SCRUBBED</term>
    <listitem>
     <para>
      Uno o varios grupos de colocación no se han sometido a un borrado seguro profundo (consulte la <xref linkend="scrubbing"/>) recientemente. Normalmente, a los grupos de colocación se les aplica un borrado seguro profundo cuando transcurre el número de segundos indicado en <option>osd_deep_mon_scrub_interval</option>; esta advertencia se activa cuando transcurren los intervalos indicados en <option>mon_warn_not_deep_scrubbed</option> sin que se realice un borrado seguro profundo. A los grupos de colocación no se les aplicará el borrado seguro profundo si no están marcados como limpios; esto puede ocurrir si se extravían o se degradan (consulte PG_AVAILABILITY y PG_DEGRADED más arriba). Puede iniciar manualmente el borrado seguro de un grupo de colocación limpio con:
     </para>
<screen><prompt>root # </prompt>ceph pg deep-scrub <replaceable>pgid</replaceable></screen>
    </listitem>
   </varlistentry>
  </variablelist>

  <tip>
   <para>
    Si ha especificado ubicaciones distintas de las establecidas por defecto para la configuración o el anillo de claves, puede indicarlas:
   </para>
<screen><prompt>root # </prompt>ceph -c <replaceable>/path/to/conf</replaceable> -k <replaceable>/path/to/keyring</replaceable> health</screen>
  </tip>
 </sect1>
 <sect1 xml:id="monitor-watch">
  <title>Visualización de un clúster</title>

  <para>
   Puede averiguar el estado inmediato de un clúster mediante <command>ceph -s</command>. Por ejemplo, un pequeño clúster de Ceph que conste de un monitor y dos OSD puede mostrar lo siguiente cuando se esté ejecutando una carga de trabajo:
  </para>

<screen>cluster:
  id:     6586341d-4565-3755-a4fd-b50f51bee248
  health: HEALTH_OK

services:
  mon: 3 daemons, quorum blueshark1,blueshark2,blueshark3
  mgr: blueshark3(active), standbys: blueshark2, blueshark1
  osd: 15 osds: 15 up, 15 in

data:
  pools:   8 pools, 340 pgs
  objects: 537 objects, 1985 MB
  usage:   23881 MB used, 5571 GB / 5595 GB avail
  pgs:     340 active+clean

io:
  client:   100 MB/s rd, 26256 op/s rd, 0 op/s wr</screen>

  <para>
   La salida proporciona la siguiente información:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     ID del clúster
    </para>
   </listitem>
   <listitem>
    <para>
     Estado del clúster
    </para>
   </listitem>
   <listitem>
    <para>
     Valor epoch de la asignación de monitores y estado del quórum de monitores
    </para>
   </listitem>
   <listitem>
    <para>
     Valor epoch de asignación de OSD y estado de los OSD
    </para>
   </listitem>
   <listitem>
    <para>
     La versión de asignación del grupo de colocación
    </para>
   </listitem>
   <listitem>
    <para>
     El número de grupos de colocación y de repositorios
    </para>
   </listitem>
   <listitem>
    <para>
     La cantidad <emphasis>teórica</emphasis> de datos almacenados y el número de objetos almacenados
    </para>
   </listitem>
   <listitem>
    <para>
     La cantidad total de datos almacenados
    </para>
   </listitem>
  </itemizedlist>

  <tip>
   <title>cómo calcula Ceph el uso de datos</title>
   <para>
    El valor <literal>used</literal> (utilizado) refleja la cantidad real de almacenamiento en bruto utilizado. El número menor del valor <literal>xxx GB / xxx GB</literal> indica la cantidad disponible de la capacidad de almacenamiento global del clúster. El número teórico refleja el tamaño de los datos almacenados antes de que se repliquen, se clonen o se capturen en una instantánea. Por lo tanto, la cantidad de datos que se almacena realmente suele superar el almacenamiento teórico, dado que Ceph crea réplicas de los datos y también puede utilizar la capacidad de almacenamiento para tareas de clonación y de captura de instantáneas.
   </para>
  </tip>

  <para>
   Otros comandos que muestran la información de estado inmediato son los siguientes:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <command>ceph pg stat</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>ceph osd pool stats</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>ceph df</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>ceph df detail</command>
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Para obtener la información actualizada en tiempo real, añada cualquiera de estos comandos (incluidos <command>ceph -s</command>) a un bucle de espera, por ejemplo:
  </para>

<screen><systemitem class="username">root</systemitem>while true ; do ceph -s ; sleep 10 ; done</screen>

  <para>
   Pulse <keycombo><keycap function="control"/><keycap>C</keycap></keycombo> cuando quiera detener la visualización.
  </para>
 </sect1>
 <sect1 xml:id="monitor-stats">
  <title>Comprobación de las estadísticas de uso de un clúster</title>

  <para>
   Para comprobar el uso de datos de un clúster y la distribución de datos entre los repositorios, puede utilizar la opción <command>df</command>. Es similar al parámetro <command>df</command> de Linux. Realice lo siguiente:
  </para>

<screen><prompt>root # </prompt>ceph df
GLOBAL:
    SIZE       AVAIL      RAW USED     %RAW USED
    55886G     55826G       61731M          0.11
POOLS:
    NAME         ID     USED      %USED     MAX AVAIL     OBJECTS
    testpool     1          0         0        17676G           0
    ecpool       2      4077M      0.01        35352G        2102
    test1        3          0         0        17676G           0
    rbd          4         16         0        17676G           3
    rbd1         5         16         0        17676G           3
    ecpool1      6      5708M      0.02        35352G        2871</screen>

  <para>
   La sección <literal>GLOBAL</literal> de la salida proporciona una descripción general de la cantidad de almacenamiento que utiliza el clúster para los datos.
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <literal>SIZE</literal>: capacidad de almacenamiento global del clúster.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>AVAIL</literal>: cantidad de espacio disponible en el clúster.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>RAW USED</literal>: cantidad de almacenamiento en bruto utilizado.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>% RAW USED</literal>: porcentaje de almacenamiento en bruto utilizado. Utilice este número junto con <literal>full ratio</literal> (lleno) y <literal>near full ratio</literal> (casi lleno) para asegurarse de que no se alcanza la capacidad de su clúster. Consulte <link xlink:href="http://docs.ceph.com/docs/master/rados/configuration/mon-config-ref#storage-capacit">Storage Capacity</link> (Capacidad de almacenamiento) para obtener información adicional.
    </para>
    <note>
     <title>nivel de llenado de clúster</title>
     <para>
      Un nivel de llenado en bruto del almacenamiento de un 70 % a un 80 % indica que es necesario añadir espacio de almacenamiento al clúster. Un nivel de uso más elevado puede provocar que algunos OSD se llenen y se produzcan problemas de estado del clúster.
     </para>
     <para>
      Utilice el comando <command>ceph osd df tree</command> para que se muestre el nivel de llenado de todos los OSD.
     </para>
    </note>
   </listitem>
  </itemizedlist>

  <para>
   La sección <literal>POOLS</literal> de la salida proporciona una lista de los repositorios y el uso teórico de cada uno de ellos. La salida de esta sección <emphasis>no</emphasis> refleja las réplicas, las clonaciones ni las instantáneas. Por ejemplo, si almacena un objeto con 1 MB de datos, el uso teórico es de 1 MB, pero el uso real puede ser de 2 MB o más, según el número de réplicas, clonaciones e instantáneas.
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <literal>NAME</literal>: nombre del repositorio.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>ID</literal>: ID del repositorio.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>USED</literal>: la cantidad teórica de los datos almacenados expresada en kilobytes, a menos que se añada M (megabytes) o G (gigabytes) a la cifra.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>%USED</literal>: el porcentaje teórico del almacenamiento utilizado por repositorio.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>MAX AVAIL</literal>: el espacio máximo disponible en el repositorio especificado.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>OBJECTS</literal>: el número teórico de objetos almacenados por repositorio.
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <para>
    Las cifras de la sección POOLS son teóricas. No incluyen el número de réplicas, clonaciones o instantáneas. Por lo tanto, la suma de las cantidades de USED y %USED no será equivalente a las cantidades de RAW USED y %RAW USED de la sección %GLOBAL de la salida.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="monitor-status">
  <title>Comprobación del estado de un clúster</title>

  <para>
   Para comprobar el estado de un clúster, ejecute lo siguiente:
  </para>

<screen><prompt>root # </prompt>ceph status</screen>

  <para>
   O bien
  </para>

<screen><prompt>root # </prompt>ceph -s</screen>

  <para>
   En el modo interactivo, escriba <command>status</command> (estado) y pulse <keycap function="enter"/>.
  </para>

<screen>ceph&gt; status</screen>

  <para>
   Ceph mostrará el estado del clúster. Por ejemplo, un pequeño clúster de Ceph que conste de un monitor y dos OSD puede mostrar lo siguiente:
  </para>

<screen>cluster b370a29d-9287-4ca3-ab57-3d824f65e339
 health HEALTH_OK
 monmap e1: 1 mons at {ceph1=10.0.0.8:6789/0}, election epoch 2, quorum 0 ceph1
 osdmap e63: 2 osds: 2 up, 2 in
  pgmap v41332: 952 pgs, 20 pools, 17130 MB data, 2199 objects
        115 GB used, 167 GB / 297 GB avail
               1 active+clean+scrubbing+deep
             951 active+clean</screen>
 </sect1>
 <sect1 xml:id="monitor-osdstatus">
  <title>Comprobación del estado de los OSD</title>

  <para>
   Para comprobar los OSD y asegurarse de que estén activos y conectados, ejecute:
  </para>

<screen><prompt>root # </prompt>ceph osd stat</screen>

  <para>
   O bien
  </para>

<screen><prompt>root # </prompt>ceph osd dump</screen>

  <para>
   También puede ver los OSD según su posición en la asignación de CRUSH.
  </para>

<screen><prompt>root # </prompt>ceph osd tree</screen>

  <para>
   Ceph imprimirá un árbol CRUSH con un host, sus OSD, su estado de funcionamiento y su peso.
  </para>

<screen># id    weight  type name       up/down reweight
-1      3       pool default
-3      3               rack mainrack
-2      3                       host osd-host
0       1                               osd.0   up      1
1       1                               osd.1   up      1
2       1                               osd.2   up      1</screen>
 </sect1>
 <sect1 xml:id="storage-bp-monitoring-fullosd">
  <title>Comprobación de OSD llenos</title>

  <para>
   Ceph impide escribir en un OSD lleno para que no se pierden datos. En un clúster en funcionamiento, debería aparecer una advertencia cuando el clúster se esté aproximando al máximo de su capacidad. El valor por defecto de <command>mon osd full ratio</command> es de 0,95 o un 95 % de capacidad para impedir la escritura de datos a los clientes. El valor por defecto de <command>mon osd nearfull ratio</command> es de 0,85 o 85 % de capacidad para generar una advertencia de estado.
  </para>

  <para>
   El comando <command>ceph health</command> informa de los nodos OSD llenos:
  </para>

<screen>ceph health
  HEALTH_WARN 1 nearfull osds
  osd.2 is near full at 85%</screen>

  <para>
   O bien
  </para>

<screen>ceph health
  HEALTH_ERR 1 nearfull osds, 1 full osds
  osd.2 is near full at 85%
  osd.3 is full at 97%</screen>

  <para>
   La mejor forma de ocuparse de un clúster lleno es añadir nodos OSD para que el clúster redistribuya los datos en el nuevo espacio de almacenamiento disponible.
  </para>

  <para>
   Si no es posible iniciar un OSD porque está lleno, puede eliminar algunos datos suprimiendo directorios de grupo de colocación en el OSD lleno.
  </para>

  <tip>
   <title>cómo evitar que los OSD se llenen</title>
   <para>
    Si un OSD se llena (usa el 100 % de su espacio de disco), normalmente se bloqueará de inmediato y sin previo aviso. A continuación indicamos algunos consejos que conviene recordar al administrar nodos OSD.
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      El espacio de disco de cada OSD (normalmente montado en <filename>/var/lib/ceph/osd/osd-{1,2..}</filename>) debe colocarse en un disco o en una partición de uso dedicado.
     </para>
    </listitem>
    <listitem>
     <para>
      Compruebe los archivos de configuración de Ceph y asegúrese de que Ceph no almacena el archivo de registro en las particiones o los discos cuyo uso esté dedicado a los OSD.
     </para>
    </listitem>
    <listitem>
     <para>
      Asegúrese de que ningún otro proceso escribe en los discos o en las particiones de uso dedicado de los OSD.
     </para>
    </listitem>
   </itemizedlist>
  </tip>
 </sect1>
 <sect1 xml:id="monitor-monstatus">
  <title>Comprobación del estado de los monitores</title>

  <para>
   Si el clúster tiene varios monitores (una situación muy probable), es recomendable comprobar el estado de quórum de monitores después de iniciar el clúster y antes de que se produzcan operaciones de lectura o escritura de datos. Debe haber un quórum si se ejecutan varios monitores. También debe consultar el estado de los monitores periódicamente para asegurarse de que se están ejecutando.
  </para>

  <para>
   Para ver la asignación de monitores, ejecute lo siguiente:
  </para>

<screen><prompt>root # </prompt>ceph mon stat</screen>

  <para>
   O bien
  </para>

<screen><prompt>root # </prompt>ceph mon dump</screen>

  <para>
   Para comprobar el estado de quórum del clúster de monitores, ejecute lo siguiente:
  </para>

<screen><prompt>root # </prompt>ceph quorum_status</screen>

  <para>
   Ceph devolverá el estado de quórum. Por ejemplo, un clúster de Ceph que consta de tres monitores podría devolver lo siguiente:
  </para>

<screen>{ "election_epoch": 10,
  "quorum": [
        0,
        1,
        2],
  "monmap": { "epoch": 1,
      "fsid": "444b489c-4f16-4b75-83f0-cb8097468898",
      "modified": "2011-12-12 13:28:27.505520",
      "created": "2011-12-12 13:28:27.505520",
      "mons": [
            { "rank": 0,
              "name": "a",
              "addr": "127.0.0.1:6789\/0"},
            { "rank": 1,
              "name": "b",
              "addr": "127.0.0.1:6790\/0"},
            { "rank": 2,
              "name": "c",
              "addr": "127.0.0.1:6791\/0"}
           ]
    }
}</screen>
 </sect1>
 <sect1 xml:id="monitor-pgroupstatus">
  <title>Comprobación del estado de los grupos de colocación</title>

  <para>
   Los grupos de colocación asignan objetos a los OSD. Al supervisar los grupos de colocación, es conveniente que tengan los estados <literal>active</literal> (activo) y <literal>clean</literal> (limpio). Para obtener información detallada, consulte el artículo <link xlink:href="http://docs.ceph.com/docs/master/rados/operations/monitoring-osd-pg">Monitoring OSDs and Placement Groups</link> (Supervisión de OSD y grupos de colocación).
  </para>
 </sect1>
 <sect1 xml:id="monitor-adminsocket">
  <title>Uso del zócalo de administración</title>

  <para>
   <remark role="fixme">Maybe give an example use case? No obvious difference to normal ceph command?!</remark> El zócalo de administración de Ceph permite consultar a un daemon a través de una interfaz de zócalo. Los zócalos de Ceph se encuentran por defecto en <filename>/var/run/ceph</filename>. Para acceder a un daemon a través del zócalo de administración, entrar a la sesión en el host en el que se ejecuta el daemon y utilice el siguiente comando:
  </para>

<screen><prompt>root # </prompt>ceph --admin-daemon /var/run/ceph/<replaceable>socket-name</replaceable></screen>

  <para>
   Para ver los comandos disponibles del zócalo de administrador, ejecute el siguiente comando:
  </para>

<screen><prompt>root # </prompt>ceph --admin-daemon /var/run/ceph/<replaceable>socket-name</replaceable> help</screen>

  <para>
   El comando del zócalo de administración permite mostrar y definir la configuración en tiempo de ejecución. Consulte <link xlink:href="http://docs.ceph.com/docs/master/rados/configuration/ceph-conf#ceph-runtime-config">Viewing a Configuration at Runtime</link> (Visualización de la configuración en tiempo de ejecución) para obtener más información.
  </para>

  <para>
   Además, puede definir directamente los valores de configuración en tiempo de ejecución (el zócalo de administración omite el monitor, a diferencia del comando injectargs <command>ceph tell</command> <replaceable>tipo de daemon</replaceable>.<replaceable>id</replaceable>, que se basa en el monitor, pero no requiere entrar a la sesión directamente en el host en cuestión).
  </para>
 </sect1>
</chapter>
