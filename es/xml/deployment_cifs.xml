<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_cifs.xml" version="5.0" xml:id="cha-ses-cifs">

 <title>Exportación de CephFS mediante Samba</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>editar</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>sí</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  Esta sección describe cómo exportar CephFS mediante un recurso compartido de Samba/CIFS. Los recursos compartidos de Samba se pueden utilizar con clientes de Windows*.
 </para>
 <warning>
  <title>tecnología en fase preliminar</title>
  <para>
   A partir de SUSE Enterprise Storage 5, la exportación de recursos compartidos de Samba se considera una tecnología en fase preliminar y no se admite.
  </para>
 </warning>
 <sect1 xml:id="sec-ses-cifs-example">
  <title>Instalación de ejemplo</title>

  <para>
   La exportación de CephFS es una tecnología en fase preliminar y no se admite. Para exportar un recurso compartido de Samba, deberá instalar manualmente Samba en un nodo de clúster y configurarlo. Es posible proporcionar funcionalidad de failover con CTDB y SUSE Linux Enterprise High Availability Extension.
  </para>

  <procedure>
   <step>
    <para>
     Asegúrese de que ya existe una instancia de CephFS en funcionamiento en el clúster. Para obtener información, consulte: <xref linkend="cha-ceph-as-cephfs"/>.
    </para>
   </step>
   <step>
    <para>
     Cree un anillo de claves específico de la pasarela de Samba en el master de Salt y cópielo en el nodo de pasarela de Samba:
    </para>
<screen><prompt>root@master # </prompt><command>ceph</command> auth get-or-create client.samba.gw mon 'allow r' \
    osd 'allow *' mds 'allow *' -o ceph.client.samba.gw.keyring
<prompt>root@master # </prompt><command>scp</command> ceph.client.samba.gw.keyring <replaceable>SAMBA_NODE</replaceable>:/etc/ceph/</screen>
    <para>
     Sustituya <replaceable>SAMBA_NODE</replaceable> con el nombre del nodo de pasarela de Samba.
    </para>
   </step>
   <step>
    <para>
     Los siguientes pasos se ejecutan en el nodo de pasarela de Samba. Instale el daemon de Samba en el nodo de pasarela de Samba:
    </para>
<screen><prompt>root # </prompt><command>zypper</command> in samba samba-ceph</screen>
   </step>
   <step>
    <para>
     Edite el archivo <filename>/etc/samba/smb.conf</filename> y añada la sección siguiente:
    </para>
<screen>[<replaceable>SHARE_NAME</replaceable>]
        path = /
        vfs objects = ceph
        ceph:config_file = /etc/ceph/ceph.conf
        ceph: user_id = samba.gw
        read only = no</screen>
   </step>
   <step>
    <para>
     Inicie y habilite el daemon de Samba:
    </para>
<screen><prompt>root # </prompt><command>systemctl</command> start smb.service
<prompt>root # </prompt><command>systemctl</command> enable smb.service
<prompt>root # </prompt><command>systemctl</command> start nmb.service
<prompt>root # </prompt><command>systemctl</command> enable nmb.service</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-ses-cifs-ha">
  <title>Configuración de alta disponibilidad</title>

  <para>
   En esta sección se proporciona un ejemplo de cómo establecer una configuración de dos nodos de alta disponibilidad de los servidores de Samba. La configuración requiere SUSE Linux Enterprise High Availability Extension. Los dos nodos se denominan <systemitem class="domainname">earth</systemitem> (<systemitem class="ipaddress">192.168.1.1</systemitem>) y <systemitem class="domainname">mars</systemitem> (<systemitem class="ipaddress">192.168.1.2</systemitem>).
  </para>

  <para>
   Para obtener información sobre SUSE Linux Enterprise High Availability Extension, consulte <link xlink:href="https://www.suse.com/documentation/sle-ha-12/"/>.
  </para>

  <para>
   Asimismo, dos direcciones IP virtuales flotantes permiten a los clientes conectarse al servicio independientemente del nodo físico en el que se estén ejecutando. <systemitem class="ipaddress">192.168.1.10</systemitem> se usa para la administración del clúster con Hawk2 y <systemitem class="ipaddress">192.168.2.1</systemitem> se usa exclusivamente para las exportaciones CIFS. De esta forma es más fácil aplicar más tarde restricciones de seguridad.
  </para>

  <para>
   El procedimiento siguiente describe la instalación de ejemplo. Encontrará más información en <link xlink:href="https://www.suse.com/documentation/sle-ha-12/install-quick/data/install-quick.html"/>.
  </para>

  <procedure xml:id="proc-sec-ses-cifs-ha">
   <step>
    <para>
     Cree un anillo de claves específico de la pasarela de Samba en el master de Salt y cópielo en ambos nodos:
    </para>
<screen><prompt>root@master # </prompt><command>ceph</command> auth get-or-create client.samba.gw mon 'allow r' \
    osd 'allow *' mds 'allow *' -o ceph.client.samba.gw.keyring
<prompt>root@master # </prompt><command>scp</command> ceph.client.samba.gw.keyring <systemitem class="domainname">earth</systemitem>:/etc/ceph/
<prompt>root@master # </prompt><command>scp</command> ceph.client.samba.gw.keyring <systemitem class="domainname">mars</systemitem>:/etc/ceph/</screen>
   </step>
   <step>
    <para>
     Prepare <systemitem class="domainname">earth</systemitem> y <systemitem class="domainname">mars</systemitem> para que alojen el servicio de Samba:
    </para>
    <substeps>
     <step>
      <para>
       Asegúrese de que los paquetes siguientes están instalados antes de continuar:
       <package>ctdb</package>, <package>tdb-tools</package>y
       <package>samba</package> (se necesitan para los recursos smb y nmb).
      </para>
<screen><prompt>root # </prompt><command>zypper</command> in ctdb tdb-tools samba samba-ceph</screen>
     </step>
     <step>
      <para>
       Asegúrese de que los servicios <literal>ctdb</literal>, <literal>smb</literal> y <literal>nmb</literal> están detenidos e inhabilitados:
      </para>
<screen><prompt>root # </prompt><command>systemctl</command> disable ctdb
<prompt>root # </prompt><command>systemctl</command> disable smb
<prompt>root # </prompt><command>systemctl</command> disable nmb
<prompt>root # </prompt><command>systemctl</command> stop smb
<prompt>root # </prompt><command>systemctl</command> stop nmb</screen>
     </step>
     <step>
      <para>
       Abra el puerto <literal>4379</literal> del cortafuegos en todos los nodos. Esto es necesario para que CTDB pueda comunicarse con los demás nodos del clúster.
      </para>
     </step>
     <step>
      <para>
       Cree un directorio para el bloqueo CTDB en el sistema de archivos compartido:
      </para>
<screen><prompt>root # </prompt><command>mkdir</command> -p /srv/samba/</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     En <systemitem class="domainname">earth</systemitem>, cree los archivos de configuración de Samba. Más adelante, se sincronizarán automáticamente con <systemitem class="domainname">mars</systemitem>.
    </para>
    <substeps>
     <step>
      <para>
       En <filename>/etc/ctdb/nodes</filename>, inserte todos los nodos que contienen todas las direcciones IP privadas de cada nodo del clúster:
      </para>
<screen>192.168.1.1
192.168.1.2</screen>
     </step>
     <step>
      <para>
       Configure Samba. Añada las líneas siguientes en la sección <literal>[global]</literal> de <filename>/etc/samba/smb.conf</filename>. Utilice el nombre de host que desee en lugar de "CTDB-SERVER" (en realidad, todos los nodos del clúster aparecerán como un nodo extenso con este nombre):
      </para>
<screen>[global]
    netbios name = CTDB-SERVER
    clustering = yes
    idmap config * : backend = tdb2
    passdb backend = tdbsam
    ctdbd socket = /var/lib/ctdb/ctdb.socket</screen>
      <para>
       Para obtener más información sobre <command>csync2</command>, consulte <link xlink:href="https://www.suse.com/documentation/sle-ha-12/singlehtml/book_sleha/book_sleha.html#pro.ha.installation.setup.csync2.start"/>.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Instale y cargue el clúster de SUSE Linux Enterprise High Availability.
    </para>
    <substeps>
     <step>
      <para>
       Registre SUSE Linux Enterprise High Availability Extension en <systemitem class="domainname">earth</systemitem> y <systemitem class="domainname">mars</systemitem>.
      </para>
<screen><prompt>root@earth # </prompt><command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable></screen>
<screen><prompt>root@mars # </prompt><command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable></screen>
     </step>
     <step>
      <para>
       Instale <package>ha-cluster-bootstrap</package> en ambos nodos:
      </para>
<screen><prompt>root@earth # </prompt><command>zypper</command> in ha-cluster-bootstrap</screen>
<screen><prompt>root@mars # </prompt><command>zypper</command> in ha-cluster-bootstrap</screen>
     </step>
     <step>
      <para>
       Inicialice el clúster en <systemitem class="domainname">earth</systemitem>:
      </para>
<screen>
<prompt>root@earth # </prompt><command>ha-cluster-init</command>
      </screen>
     </step>
     <step>
      <para>
       Deje que <systemitem class="domainname">mars</systemitem> se una al clúster:
      </para>
<screen>
<prompt>root@mars # </prompt><command>ha-cluster-join</command> -c earth
      </screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Compruebe el estado del clúster. Debería observar que se han añadido dos nodos en el clúster:
    </para>
<screen><prompt>root@earth # </prompt><command>crm</command> status
2 nodes configured
1 resource configured

Online: [ earth mars ]

Full list of resources:

 admin-ip       (ocf::heartbeat:IPaddr2):       Started earth</screen>
   </step>
   <step>
    <para>
     Ejecute los comandos siguientes en <systemitem class="domainname">earth</systemitem> para configurar el recurso CTDB:
    </para>

<screen><prompt>root@earth # </prompt><command>crm</command> configure
<prompt>crm(live)configure# </prompt><command>primitive</command> ctdb ocf:heartbeat:CTDB params \
    ctdb_manages_winbind="false" \
    ctdb_manages_samba="false" \
    ctdb_recovery_lock="!/usr/lib64/ctdb/ctdb_mutex_ceph_rados_helper
        ceph client.samba.gw cephfs_metadata ctdb-mutex"
    ctdb_socket="/var/lib/ctdb/ctdb.socket" \
        op monitor interval="10" timeout="20" \
        op start interval="0" timeout="90" \
        op stop interval="0" timeout="100"
<prompt>crm(live)configure# </prompt><command>primitive</command> nmb systemd:nmb \
    op start timeout="60" interval="0" \
    op stop timeout="60" interval="0" \
    op monitor interval="60" timeout="60"
<prompt>crm(live)configure# </prompt><command>primitive</command> smb systemd:smb \
    op start timeout="60" interval="0" \
    op stop timeout="60" interval="0" \
    op monitor interval="60" timeout="60"
<prompt>crm(live)configure# </prompt><command>group</command> g-ctdb ctdb nmb smb
<prompt>crm(live)configure# </prompt><command>clone</command> cl-ctdb g-ctdb meta interleave="true"
<prompt>crm(live)configure# </prompt><command>commit</command></screen>
    <para>
     El archivo binario <command>/usr/lib64/ctdb/ctdb_mutex_ceph_rados_helper</command> de la opción de configuración <literal>ctdb_recovery_lock</literal> tiene los parámetros <replaceable>CLUSTER_NAME</replaceable> <replaceable>CEPHX_USER</replaceable> <replaceable>CEPH_POOL</replaceable> y <replaceable>CEPHX_USER</replaceable>, en este orden.
    </para>
   </step>
   <step>
    <para>
     Añada una dirección IP agrupada en clúster:
    </para>
<screen><prompt>crm(live)configure# </prompt><command>primitive</command> ip ocf:heartbeat:IPaddr2 params ip=192.168.2.1 \
    unique_clone_address="true" \
    op monitor interval="60" \
    meta resource-stickiness="0"
<prompt>crm(live)configure# </prompt><command>clone</command> cl-ip ip \
    meta interleave="true" clone-node-max="2" globally-unique="true"
<prompt>crm(live)configure# </prompt><command>colocation</command> col-with-ctdb 0: cl-ip cl-ctdb
<prompt>crm(live)configure# </prompt><command>order</command> o-with-ctdb 0: cl-ip cl-ctdb
<prompt>crm(live)configure# </prompt><command>commit</command></screen>
    <para>
     Si <literal>unique_clone_address</literal> se define como <literal>true</literal> (verdadero), el agente de recurso IPaddr2 añade un ID de clonación a la dirección especificada, lo que da como resultado que haya tres direcciones IP distintas. Normalmente, no son necesarias, pero ayudan a la hora de equilibrar la carga. Para obtener más información sobre este tema, consulte <link xlink:href="https://www.suse.com/documentation/sle-ha-12/book_sleha/data/cha_ha_lb.html"/>.
    </para>
   </step>
   <step>
    <para>
     Compruebe el resultado:
    </para>
<screen><prompt>root@earth # </prompt><command>crm</command> status
Clone Set: base-clone [dlm]
     Started: [ factory-1 ]
     Stopped: [ factory-0 ]
 Clone Set: cl-ctdb [g-ctdb]
     Started: [ factory-1 ]
     Started: [ factory-0 ]
 Clone Set: cl-ip [ip] (unique)
     ip:0       (ocf:heartbeat:IPaddr2):       Started factory-0
     ip:1       (ocf:heartbeat:IPaddr2):       Started factory-1</screen>
   </step>
   <step>
    <para>
     Realice una prueba desde un equipo cliente. En un cliente Linux, ejecute el comando siguiente para comprobar si puede copiar archivos desde el sistema y en el sistema:
    </para>
<screen><prompt>root # </prompt><command>smbclient</command> <option>//192.168.2.1/myshare</option></screen>
   </step>
  </procedure>
 </sect1>
</chapter>
