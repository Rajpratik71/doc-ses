<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_install_salt.xml" version="5.0" xml:id="ceph-install-saltstack">
 <title>DeepSea/Saltを使用した展開</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:translation>○</dm:translation>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <note>
  <title>SUSE Enterprise Storage 5では<command>ceph-deploy</command>は削除</title>
  <para>
   クラスタ展開ツール<command>ceph-deploy</command>はSUSE Enterprise Storage 4で非推奨になり、SUSE Enterprise Storage 5では完全に削除され、代わりにDeepSeaが推奨されています。
  </para>
 </note>
 <para>
  SaltとDeepSeaは、サーバインフラストラクチャの展開と管理に役立つコンポーネントの「スタック」<emphasis/>です。拡張性が非常に高く高速で、比較的簡単に運用を開始できます。Saltを使用してクラスタの展開を始める前に、以下の考慮事項をお読みください。
 </para>
 <itemizedlist>
  <listitem>
   <para>
    <emphasis/>「Salt Minion」は、Salt Masterと呼ばれる専用のノードによって制御されるノードです。Salt Minionは、Ceph OSD、Ceph Monitor、Ceph Manager、Object Gateway、iSCSI Gateway、NFS Ganeshaなどの役割を持ちます。
   </para>
  </listitem>
  <listitem>
   <para>
    Salt Masterは専用のSalt Minionを実行します。これは特権タスク(キーの作成や権限付与、ミニオンへのキーのコピーなど)を実行するために必要で、その結果、リモートミニオンは特権タスクを実行しなくても済みます。
   </para>
   <tip>
    <title>1つのサーバで複数の役割を共有</title>
    <para>
     各役割を別個のノードに展開すると、Cephクラスタで最適なパフォーマンスを実現できます。しかし、実際の展開では、1つのノードを複数の役割のために共有しなければならない場合があります。パフォーマンスやアップグレード手順で問題が起きないようにするため、Ceph OSD、Metadata Server、またはCeph Monitorの役割はSalt Masterに展開しないでください。
    </para>
   </tip>
  </listitem>
  <listitem>
   <para>
    Salt Minionは、ネットワークでSalt Masterのホスト名を正しく解決する必要があります。Salt Minionは、デフォルトでは<systemitem>salt</systemitem>というホスト名を検索しますが、ネットワーク経由でアクセス可能なほかのホスト名を<filename>/etc/salt/minion</filename>ファイルで指定できます。<xref linkend="ceph-install-stack"/>を参照してください。
   </para>
  </listitem>
 </itemizedlist>
 <sect1 xml:id="cha-ceph-install-relnotes">
  <title>リリースノートの確認</title>

  <para>
   リリースノートには、旧リリースのSUSE Enterprise Storageからの変更点に関する追加情報が記載されています。リリースノートを参照して以下を確認します。
  </para>

  <itemizedlist>
   <listitem>
    <para>
     使用しているハードウェアに特別な配慮が必要かどうか
    </para>
   </listitem>
   <listitem>
    <para>
     使用しているソフトウェアパッケージに大幅な変更があるかどうか
    </para>
   </listitem>
   <listitem>
    <para>
     インストールのために特別な予防措置が必要かどうか
    </para>
   </listitem>
  </itemizedlist>

  <para>
   リリースノートには、マニュアルに記載できなかった情報が記載されています。また、既知の問題に関する注意も記載されています。
  </para>

  <para>
   パッケージ <package>release-notes-ses</package>をインストールすると、リリースノートは、ローカルではディレクトリ<filename>/usr/share/doc/release-notes</filename>に、オンラインでは<link xlink:href="https://www.suse.com/releasenotes/"/>にあります。
  </para>
 </sect1>
 <sect1 xml:id="deepsea-description">
  <title>DeepSeaの概要</title>

  <para>
   DeepSeaの目的は、管理者の時間を節約し、Cephクラスタの複雑な操作を確実に実行できるようにすることです。
  </para>

  <para>
   Cephは、設定を細かく変更可能なソフトウェアソリューションです。Cephはシステム管理者の自由度と対応能力の両方を向上させます。
  </para>

  <para>
   デモンストレーションにはCephの最小セットアップで十分ですが、これでは大量のノードがある場合に発揮されるCephの興味深い機能はわかりません。
  </para>

  <para>
   DeepSeaは、アドレスやデバイス名など、個々のサーバのデータを収集、保存します。Cephのような分散ストレージシステムでは、収集および保存すべきこのような項目が何百個も存在する可能性があります。手動で情報を収集して設定管理ツールにデータを入力するのは、たいへんな労力が必要なだけでなく、ミスも起きがちです。
  </para>

  <para>
   サーバの準備、設定の収集、およびCephの展開に必要な手順はほぼ同じです。ただし、これは別個の機能の管理には対応していません。日々の運用では、特定の機能に簡単にハードウェアを追加したり、ハードウェアを問題なく削除したりできる必要があります。
  </para>

  <para>
   DeepSeaは、管理者の決定事項を1つのファイルに統合するというストラテジーによって、こうした現実の課題に対応します。クラスタの割り当て、役割の割り当て、プロファイルの割り当てなどが決定事項に含まれます。DeepSeaは各タスクセットを収集してシンプルな目標にまとめます。それぞれの目標は「ステージ」です。<emphasis/>
  </para>

  <itemizedlist xml:id="deepsea-stage-description">
   <title>DeepSeaのステージの説明</title>
   <listitem>
    <para>
     <emphasis role="bold"/><emphasis role="bold"/>「ステージ 0」 - 「準備」 - このステージ中に、必要な更新がすべて適用されます。システムが再起動することがあります。
    </para>
    <important>
     <title>Salt Masterの再起動後にステージ0を再実行する</title>
     <para>
      ステージ0の間に、Salt Masterは新しいカーネルバージョンをロードするために再起動するので、もう一度ステージ0を実行する必要があります。そうしないと、ミニオンがターゲットに設定されません。
     </para>
    </important>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold"/><emphasis role="bold"/>「ステージ1」 - 「ディスカバリ」 - ここでは、クラスタ内のすべてのハードウェアを検出して、Cephの設定に必要な情報を収集します。設定の詳細については、<xref linkend="deepsea-pillar-salt-configuration"/>を参照してください。
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold"/><emphasis role="bold"/>「ステージ2」 - 「設定」 - 設定データを特定のフォーマットで準備する必要があります。
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold"/><emphasis role="bold"/>「ステージ3」 - 「展開」 - 必須のCephサービスが含まれる基本的なCephクラスタを作成します。リストについては、<xref linkend="storage-intro-core-nodes"/>を参照してください。
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold"/><emphasis role="bold"/>「ステージ4」 - 「サービス」 - このステージでは、iSCSI、Object Gateway、CephFSなど、Cephの追加機能をインストールできます。各機能はオプションです。
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold"/>「ステージ5」 - 削除ステージ。このステージは必須ではなく、通常、初期セットアップ時には必要ありません。このステージでは、ミニオンの役割のほかにクラスタ設定も削除されます。このステージは、クラスタからストレージノードを削除する必要がある場合に実行する必要があります。詳細については、<xref linkend="salt-node-removing"/>を参照してください。
    </para>
   </listitem>
  </itemizedlist>

  <para>
   DeepSeaの詳細については、<link xlink:href="https://github.com/suse/deepsea/wiki"/>を参照してください。
  </para>

  <sect2 xml:id="deepsea-organisation-locations">
   <title>組織と重要な場所</title>
   <para>
    Saltには、マスタノードで使用される標準の場所と命名規則がいくつか設定されています。
   </para>
   <variablelist>
    <varlistentry>
     <term><filename>/srv/pillar</filename>
     </term>
     <listitem>
      <para>
       このディレクトリには、クラスタミニオンの設定データが保存されます。<emphasis/>「Pillar」は、すべてのクラスタミニオンにグローバル設定値を提供するためのインタフェースです。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/salt/</filename>
     </term>
     <listitem>
      <para>
       このディレクトリには、Saltの状態ファイル(<emphasis/>「sls」ファイルとも呼びます)が保存されます。状態ファイルは、クラスタのあるべき状態を特定のフォーマットで記述したものです。詳細については、<link xlink:href="https://docs.saltstack.com/en/latest/topics/tutorials/starting_states.html">Saltのマニュアル</link>を参照してください。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/module/runners</filename>
     </term>
     <listitem>
      <para>
       このディレクトリには、ランナと呼ばれるPythonスクリプトが保存されます。ランナはマスタノードで実行されます。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/salt/_modules</filename>
     </term>
     <listitem>
      <para>
       このディレクトリには、モジュールと呼ばれるPythonスクリプトが保存されます。モジュールはクラスタ内のすべてのミニオンに適用されます。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/pillar/ceph</filename>
     </term>
     <listitem>
      <para>
       このディレクトリはDeepSeaによって使用されます。収集された設定データはここに保存されます。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/salt/ceph</filename>
     </term>
     <listitem>
      <para>
       DeepSeaによって使用されるディレクトリ。さまざまなフォーマットのslsファイルが保存されますが、各サブディレクトリにslsファイルが含まれます。各サブディレクトリには1種類のslsファイルのみが含まれます。たとえば、<filename>/srv/salt/ceph/stage</filename>には、<command>salt-run state.orchestrate</command>によって実行されるオーケストレーションファイルが含まれます。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ds-minion-targeting">
   <title>ミニオンのターゲット設定</title>
   <para>
    DeepSeaのコマンドはSaltインフラストラクチャ経由で実行されます。<command>salt</command>コマンドを使用する場合、コマンドの対象にするSalt Minionのセットを指定する必要があります。ここでは、そのようなミニオンのセットを<command>salt</command>コマンドの<emphasis/>「ターゲット」と呼びます。以降のセクションでは、ミニオンのターゲットを設定するために使用できる方法について説明します。
   </para>
   <sect3 xml:id="ds-minion-targeting-name">
    <title>ミニオン名の一致</title>
    <para>
     1つのミニオンまたは複数のミニオンのグループを名前で照合してターゲットに設定できます。通常、ミニオンの名前は、ミニオンが実行されているノードの短いホスト名です。これは、Saltの一般的なターゲット設定方法で、DeepSeaには関係ありません。グロブ、正規表現、またはリストを使用して、ミニオン名の範囲を制限できます。一般的な構文は次のとおりです。
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> example.module</screen>
    <tip>
     <title>Cephのみのクラスタ</title>
     <para>
      ユーザの環境内のすべてのSalt Minionがそのユーザ自身のCephクラスタに属する場合は、<replaceable>target</replaceable>を「<literal>*</literal>」に置き換えて、<emphasis/>「すべて」の登録済みミニオンを含めて問題ありません。
     </para>
    </tip>
    <para>
     example.netドメイン内のすべてのミニオンに一致します(ミニオン名がその「完全な」ホスト名と同じであると想定しています)。
    </para>
<screen><prompt>root@master # </prompt>salt '*.example.net' test.ping</screen>
    <para>
     「web1」～「web5」のミニオンに一致します。
    </para>
<screen><prompt>root@master # </prompt>salt 'web[1-5]' test.ping</screen>
    <para>
     正規表現を使用して、「web1-prod」と「web1-devel」の両方のミニオンに一致します。
    </para>
<screen><prompt>root@master # </prompt>salt -E 'web1-(prod|devel)' test.ping</screen>
    <para>
     ミニオンの単純なリストに一致します。
    </para>
<screen><prompt>root@master # </prompt>salt -L 'web1,web2,web3' test.ping</screen>
    <para>
     クラスタ内のすべてのミニオンに一致します。
    </para>
<screen><prompt>root@master # </prompt>salt '*' test.ping</screen>
   </sect3>
   <sect3 xml:id="ds-minion-targeting-grain">
    <title>「deepsea」グレインを使用したターゲット設定</title>
    <para>
     Saltによって管理される異種環境において、Enterprise Storageが他のクラスタソリューションと共にノードのサブセットに展開されている場合、「deepsea」グレインを適用して、関連するミニオンに「マークを付ける」ことをお勧めします。これにより、ミニオン名での一致が難しい環境で、DeepSeaミニオンを簡単にターゲットに設定できます。
    </para>
    <para>
     「deepsea」グレインをミニオンのグループに適用するには、次のコマンドを実行します。
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> grains.append deepsea default</screen>
    <para>
     「deepsea」グレインをミニオンのグループから削除するには、次のコマンドを実行します。
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> grains.delval deepsea destructive=True</screen>
    <para>
     「deepsea」グレインを関連するミニオンに適用した後、次のようにミニオンをターゲットに設定できます。
    </para>
<screen><prompt>root@master # </prompt>salt -G 'deepsea:*' test.ping</screen>
    <para>
     次のコマンドも同等の処理を行います。
    </para>
<screen><prompt>root@master # </prompt>salt -C 'G@deepsea:*' test.ping</screen>
   </sect3>
   <sect3 xml:id="ds-minion-targeting-dsminions">
    <title><option>deepsea_minions</option>オプションの設定</title>
    <para>
     DeepSeaの展開では、<option>deepsea_minions</option>オプションのターゲットを設定する必要があります。DeepSeaは、このオプションを使用して、ステージ実行中にミニオンに命令します(詳細については、<xref linkend="deepsea-stage-description"/>を参照してください)。
    </para>
    <para>
     <option>deepsea_minions</option>オプションを設定または変更するには、Salt Masterで<filename>/srv/pillar/ceph/deepsea_minions.sls</filename>ファイルを編集し、次の行を追加または置換します。
    </para>
<screen>deepsea_minions: <replaceable>target</replaceable></screen>
    <tip>
     <title><option>deepsea_minions</option>のターゲット</title>
     <para>
      <option>deepsea_minions</option>オプションの<replaceable>target</replaceable>として、「<xref linkend="ds-minion-targeting-name" xrefstyle="select: title"/>」および「<xref linkend="ds-minion-targeting-grain" xrefstyle="select: title"/>」の両方のターゲット設定方法を使用できます。
     </para>
     <para>
      クラスタ内のすべてのSalt Minionに一致します。
     </para>
<screen>deepsea_minions: '*'</screen>
     <para>
      「deepsea」グレインを使用してすべてのミニオンに一致します。
     </para>
<screen>deepsea_minions: 'G@deepsea:*'</screen>
    </tip>
   </sect3>
   <sect3>
    <title>その他の情報</title>
    <para>
     Saltインフラストラクチャを使用して、より高度な方法でミニオンをターゲットに設定できます。すべてのターゲット設定手法の説明については、<link xlink:href="https://docs.saltstack.com/en/latest/topics/targeting/"/>を参照してください。
    </para>
    <para>
     さらに、「deepsea-minions」のマニュアルページでは、DeepSeaのターゲット設定が詳しく説明されています(<command>man 7 deepsea_minions</command>)。
    </para>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-install-stack">
  <title>クラスタの展開</title>

  <para>
   クラスタの展開プロセスには複数のフェーズがあります。まず、Saltを設定してクラスタのすべてのノードを準備してから、Cephを展開および設定する必要があります。
  </para>

  <tip>
   <title>OSDプロファイルを定義せずにモニタノードを展開</title>
   <para>
    OSDプロファイルの定義をスキップして、最初にモニタノードを展開する必要がある場合、<option>DEV_ENV</option>変数を設定することにより実行できます。これにより、<filename>profile/</filename>ディレクトリが存在しなくてもモニタを展開できるほか、ストレージノード、モニタノード、およびマネージャノードが少なくとも「1つ」<emphasis/>あればクラスタを展開できます。
   </para>
   <para>
    この環境変数を設定するには、<filename>/srv/pillar/ceph/stack/global.yml</filename>ファイルで設定してグローバルに有効にするか、現在のシェルセッションに対してのみ設定します。
   </para>
<screen><prompt>root@master # </prompt>export DEV_ENV=true</screen>
  </tip>

  <para>
   次の手順では、クラスタの準備について詳しく説明します。
  </para>

  <procedure>
   <step>
    <para>
     クラスタの各ノードにSUSE Linux Enterprise Server 12 SP3とSUSE Enterprise Storage拡張機能をインストールして登録します。
    </para>
   </step>
   <step>
    <para>
     既存のソフトウェアリポジトリを一覧にして、適切な製品がインストールおよび登録されていることを確認します。リストの出力は次のようになります。
    </para>
<screen>
 <prompt>root@minion &gt; </prompt>zypper lr -E
#  | Alias   | Name                              | Enabled | GPG Check | Refresh
---+---------+-----------------------------------+---------+-----------+--------
 4 | [...]   | SUSE-Enterprise-Storage-5-Pool    | Yes     | (r ) Yes  | No
 6 | [...]   | SUSE-Enterprise-Storage-5-Updates | Yes     | (r ) Yes  | Yes
 9 | [...]   | SLES12-SP3-Pool                   | Yes     | (r ) Yes  | No
11 | [...]   | SLES12-SP3-Updates                | Yes     | (r ) Yes  | Yes
</screen>
   </step>
   <step>
    <para>
     ネットワークを設定します。各ノードでDNS名が適切に解決されるようにする設定も含まれます。Salt MasterとすべてのSalt Minionは、お互いをホスト名で解決する必要があります。ネットワークの設定の詳細については、<link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/sec_basicnet_yast.html"/>を参照してください。DNSサーバの設定の詳細については、<link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/cha_dns.html"/>を参照してください。
    </para>
   </step>
   <step>
    <para>
     NTP時刻同期サーバを設定して有効化し、起動します。
    </para>
<screen><prompt>root@master # </prompt>systemctl enable ntpd.service
<prompt>root@master # </prompt>systemctl start ntpd.service</screen>
    <para>
     NTPの設定の詳細については、<link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/sec_netz_xntp_yast.html"/>を参照してください。
    </para>
   </step>
   <step>
    <para>
     AppArmorサービスが実行されているかどうかを確認し、このサービスを各クラスタノードで無効にします。YaST AppArmorモジュールを起動して、<guimenu>Settings (設定)</guimenu>を選択し、<guimenu>Enable Apparmor (Apparmorの有効化)</guimenu>チェックボックスをオフにします。<guimenu>Done (完了)</guimenu>をクリックして確認します。
    </para>
    <para>
     SUSE Enterprise Storageは、AppArmorが有効な状態では「動作しない」<emphasis/>ことに注意してください。
    </para>
   </step>
   <step>
    <para>
     Salt Masterノードに<literal>salt-master</literal>パッケージと<literal>salt-minion</literal>パッケージをインストールします。
    </para>
<screen><prompt>root@master # </prompt>zypper in salt-master salt-minion</screen>
    <para>
     <systemitem>salt-master</systemitem>サービスが有効になっていて起動していることを確認します。必要であれば、サービスを有効にして起動します。
    </para>
<screen><prompt>root@master # </prompt>systemctl enable salt-master.service
<prompt>root@master # </prompt>systemctl start salt-master.service</screen>
   </step>
   <step>
    <para>
     ファイアウォールを使用する場合は、Salt Masterノードのポート4505と4506がすべてのSalt Minionノードに対して開いていることを確認します。これらのポートが閉じている場合は、<command>yast2 firewall</command>コマンドを使用してポートを開き、<guimenu>SaltStack</guimenu>サービスを許可できます。
    </para>
    <warning>
     <title>ファイアウォールがアクティブな場合、DeepSeaのステージが失敗する</title>
     <para>
      ファイアウォールがアクティブな場合(かつ設定されている場合)、DeepSeaの展開ステージが失敗します。ステージを正しく実行するには、次のコマンドを実行してファイアウォールをオフにします。
     </para>
<screen>
<prompt>root@master # </prompt>systemctl stop SuSEfirewall2.service
</screen>
     <para>
      または、<filename>/srv/pillar/ceph/stack/global.yml</filename>の<option>FAIL_ON_WARNING</option>オプションを「False」に設定します。
     </para>
<screen>
FAIL_ON_WARNING: False
</screen>
    </warning>
   </step>
   <step>
    <para>
     パッケージ<literal>salt-minion</literal>をすべてのミニオンノードにインストールします。
    </para>
<screen><prompt>root@minion &gt; </prompt>zypper in salt-minion</screen>
    <para>
     各ノードの「完全修飾ドメイン名」<emphasis/>を他のすべてのノードがパブリックネットワークのIPアドレスに解決できることを確認します。
    </para>
   </step>
   <step>
    <para>
     すべてのミニオン(マスタミニオンを含む)を、マスタに接続するように設定します。ホスト名<literal>salt</literal>でSalt Masterに接続できない場合は、ファイル<filename>/etc/salt/minion</filename>を編集するか、次の内容で新しいファイル<filename>/etc/salt/minion.d/master.conf</filename>を作成します。
    </para>
<screen>master: <replaceable>host_name_of_salt_master</replaceable></screen>
    <para>
     上で説明した設定ファイルを変更した場合は、すべてのSalt MinionのSaltサービスを再起動します。
    </para>
<screen><prompt>root@minion &gt; </prompt>systemctl restart salt-minion.service</screen>
   </step>
   <step>
    <para>
     すべてのノードで<systemitem>salt-minion</systemitem>サービスが有効になっていて起動していることを確認します。必要であれば、次のコマンドを使用して有効にして起動します。
    </para>
<screen><prompt>root@minion &gt; </prompt>systemctl enable salt-minion.service
<prompt>root@minion &gt; </prompt>systemctl start salt-minion.service</screen>
   </step>
   <step>
    <para>
     各Salt Minionの指紋を確認して、指紋が一致する場合、Salt Master上のすべてのSaltキーを受諾します。
    </para>
    <para>
     各ミニオンの指紋を表示します。
    </para>
<screen><prompt>root@minion &gt; </prompt>salt-call --local key.finger
local:
3f:a3:2f:3f:b4:d3:d9:24:49:ca:6b:2c:e1:6c:3f:c3:83:37:f0:aa:87:42:e8:ff...</screen>
    <para>
     すべてのSalt Minionの指紋を収集した後、Salt Master上の、受諾されていない全ミニオンキーの指紋を一覧にします。
    </para>
<screen><prompt>root@master # </prompt>salt-key -F
[...]
Unaccepted Keys:
minion1:
3f:a3:2f:3f:b4:d3:d9:24:49:ca:6b:2c:e1:6c:3f:c3:83:37:f0:aa:87:42:e8:ff...</screen>
    <para>
     ミニオンの指紋が一致する場合、ミニオンを受諾します。
    </para>
<screen><prompt>root@master # </prompt>salt-key --accept-all</screen>
   </step>
   <step>
    <para>
     キーが受諾されたことを確認します。
    </para>
<screen><prompt>root@master # </prompt>salt-key --list-all</screen>
   </step>
   <step xml:id="deploy-wiping-disk">
    <para>
     SUSE Enterprise Storageを展開する前に、以前のクラスタでOSDとして使用されていたディスクがすべて空で、パーティションがないことを確認します。これを確実に行うには、すべてのディスクを手動で消去する必要があります。必ず「X」を正しいディスク文字に置き換えてください。
    </para>
    <substeps>
     <step>
      <para>
       指定したディスクを使用しているすべてのプロセスを停止します。
      </para>
     </step>
     <step>
      <para>
       ディスクのパーティションがマウントされているかどうかを確認し、必要に応じてアンマウントします。
      </para>
     </step>
     <step>
      <para>
       ディスクがLVMによって管理されている場合は、LVMインフラストラクチャ全体を無効にして削除します。詳細については、<link xlink:href="https://www.suse.com/documentation/sles-12/stor_admin/data/cha_lvm.html"/>を参照してください。
      </para>
     </step>
     <step>
      <para>
       ディスクがMD RAIDの一部である場合は、RAIDを無効化します。詳細については、<link xlink:href="https://www.suse.com/documentation/sles-12/stor_admin/data/part_software_raid.html"/>を参照してください。
      </para>
     </step>
     <step>
      <tip>
       <title>サーバの再起動</title>
       <para>
        次の手順の実行中に「partition in use (パーティションが使用中です)」や「kernel can not be updated with the new partition table (カーネルを新しいパーティションテーブルで更新できません)」などのエラーメッセージが表示される場合、サーバを再起動してください。
       </para>
      </tip>
      <para>
       各パーティションの先頭を消去します。
      </para>
<screen>for partition in /dev/sdX[0-9]*
do
  dd if=/dev/zero of=$partition bs=4096 count=1 oflag=direct
done</screen>
     </step>
     <step>
      <para>
       パーティションテーブルを消去します。
      </para>
<screen>sgdisk -Z --clear -g /dev/sdX</screen>
     </step>
     <step>
      <para>
       バックアップパーティションテーブルを消去します。
      </para>
<screen>size=`blockdev --getsz /dev/sdX`
position=$((size/4096 - 33))
dd if=/dev/zero of=/dev/sdX bs=4M count=33 seek=$position oflag=direct</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     DeepSeaをSalt Masterノードにインストールします。
    </para>
<screen><prompt>root@master # </prompt>zypper in deepsea</screen>
   </step>
   <step>
    <para>
     Salt Master上のファイル<filename>/srv/pillar/ceph/master_minion.sls</filename>がSalt Masterを指していることを確認します。ほかのホスト名でもSalt Masterにアクセスできる場合は、Storage Clusterに適したホスト名を使用します。<emphasis/>「ses」ドメインでSalt Masterにデフォルトのホスト名(「salt」<emphasis/>)を使用していた場合、次のようなファイルになります。
    </para>
<screen>master_minion: salt.ses</screen>
   </step>
  </procedure>

  <para>
   続いてCephを展開して設定します。別途明記されていない限り、すべての手順が必須です。
  </para>

  <note>
   <title>Saltコマンドの規則</title>
   <para>
    <command>salt-run state.orch</command>は2つの方法で実行できます。1つは<literal>stage.&lt;stage number&gt;</literal>を使用する方法で、もう1つはステージの名前を使用する方法です。どちらの表記でも効果は同じで、どちらのコマンドを使用するかは完全に好みの問題です。
   </para>
  </note>

  <procedure xml:id="ds-depl-stages">
   <title>展開ステージの実行</title>
   <step>
    <para>
     現在展開中のCephクラスタに属しているSalt Minionを含めます。ミニオンのターゲット設定の詳細については、<xref linkend="ds-minion-targeting-name"/>を参照してください。
    </para>
   </step>
   <step>
    <para>
     クラスタを準備します。詳細については、<xref linkend="deepsea-stage-description"/>を参照してください。
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.0</screen>
    <para>
     または
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.prep</screen>
    <note>
     <title>DeepSea CLIを使用したステージの実行または監視</title>
     <para>
      DeepSea CLIを使用して、ステージの実行進行状況をリアルタイムで把握できます。そのためには、DeepSea CLIをモニタリングモードで実行するか、DeepSea CLIを通じてステージを直接実行します。詳細については、<xref linkend="deepsea-cli"/>を参照してください。
     </para>
    </note>
   </step>
   <step>
    <para>
     <emphasis/>オプション: <filename>/var/lib/ceph/</filename>のBtrfsサブボリュームを作成します。この手順は、DeepSeaの以降のステージの実行が完了する前にのみ実行してください。既存のディレクトリを移行する方法や詳細については、<xref linkend="storage-tips-ceph-btrfs-subvol"/>を参照してください。
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.migrate.subvolume</screen>
   </step>
   <step>
    <para>
     ディスカバリステージでは、すべてのミニオンからデータを収集して、ディレクトリ<filename>/srv/pillar/ceph/proposals</filename>に保存される設定フラグメントを作成します。データはYAMLフォーマットで*.slsファイルまたは*.ymlファイルに保存されます。
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.1</screen>
    <para>
     または
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.discovery</screen>
   </step>
   <step>
    <para>
     前のコマンドが正常に完了した後、<filename>/srv/pillar/ceph/proposals</filename>に<filename>policy.cfg</filename>ファイルを作成します。詳細については、<xref linkend="policy-configuration"/>を参照してください。
    </para>
    <tip>
     <para>
      クラスタのネットワーク設定を変更する必要がある場合は、<filename>/srv/pillar/ceph/stack/ceph/cluster.yml</filename>を編集して、<literal>cluster_network:</literal>および<literal>public_network:</literal>で始まる各行を調整します。
     </para>
    </tip>
   </step>
   <step>
    <para>
     設定ステージにより、<filename>policy.cfg</filename>ファイルが解析され、含まれるファイルが最終的な形態にマージされます。クラスタと役割に関連する内容は<filename>/srv/pillar/ceph/cluster</filename>に保存され、Ceph固有の内容は<filename>/srv/pillar/ceph/stack/default</filename>に配置されます。
    </para>
    <para>
     次のコマンドを実行して、設定ステージをトリガします。
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.2</screen>
    <para>
     または
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.configure</screen>
    <para>
     この設定手順には数秒かかる場合があります。コマンドが完了した後、指定したミニオンのPillarデータを表示できます(たとえば、<literal>ceph_minion1</literal>、<literal>ceph_minion2</literal>という名前のミニオンなど)。このためには、次のコマンドを実行します。
    </para>
<screen><prompt>root@master # </prompt>salt 'ceph_minion*' pillar.items</screen>
    <note>
     <title>デフォルト値の上書き</title>
     <para>
      コマンドが完了したら、すぐにデフォルト設定を参照して、ニーズに合わせて変更できます。詳細については、<xref linkend="ceph-deploy-ds-custom"/>を参照してください。
     </para>
    </note>
   </step>
   <step>
    <para>
     これで展開ステージを実行できます。このステージでは、Pillarを検証し、ストレージノードでモニタデーモンとODSデーモンを起動します。次のコマンドを実行して、ステージを開始します。
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.3</screen>
    <para>
     または
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.deploy
    </screen>
    <para>
     このコマンドには数分かかる場合があります。コマンドが失敗した場合は、問題を修正して前のステージをもう一度実行する必要があります。コマンドが成功したら、次のコマンドを実行して状態を確認します。
    </para>
<screen><prompt>root@master # </prompt>ceph -s</screen>
   </step>
   <step>
    <para>
     Cephクラスタ展開の最後の手順は、「サービス」<emphasis/>ステージです。ここでは、現在サポートされているサービス(iSCSI Gateway、CephFS、Object Gateway、openATTIC、およびNFS Ganesha)のインスタンスを生成します。このステージで、必要なプール、権限付与キーリング、および起動サービスが作成されます。ステージを開始するには、次のコマンドを実行します。
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.4</screen>
    <para>
     または
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.services</screen>
    <para>
     セットアップによっては、このコマンドの実行には数分かかる場合があります。
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="deepsea-cli">
  <title>DeepSea CLI</title>

  <para>
   DeepSeaは、ユーザがステージを監視または実行しながら、実行進行状況をリアルタイムで可視化できるCLIツールも備えています。
  </para>

  <para>
   ステージの実行進行状況を可視化するため、次の2つのモードがサポートされています。
  </para>

  <itemizedlist xml:id="deepsea-cli-modes">
   <title>DeepSea CLIのモード</title>
   <listitem>
    <para>
     モニタリングモード<emphasis role="bold"/>: 別のターミナルセッションで発行された<command>salt-run</command>コマンドによってトリガされたDeepSeaステージの実行進行状況を可視化します。
    </para>
   </listitem>
   <listitem>
    <para>
     スタンドアロンモード<emphasis role="bold"/>: DeepSeaステージを実行すると同時に、その構成要素の実行中にリアルタイムで各手順を可視化します。
    </para>
   </listitem>
  </itemizedlist>

  <important>
   <title>DeepSea CLIコマンド</title>
   <para>
    DeepSea CLIコマンドは、Salt Masterノードで、<systemitem class="username">root</systemitem>特権でのみ実行できます。
   </para>
  </important>

  <sect2 xml:id="deepsea-cli-monitor">
   <title>DeepSea CLI: モニタモード</title>
   <para>
    進行状況モニタは、他のターミナルセッションで<command>salt-run state.orch</command>コマンドを使用して実行されているステージで何が実行されているかをリアルタイムで詳しく可視化します。
   </para>
   <para>
    <command>salt-run state.orch</command>を実行する前に、ステージの実行開始を検出できるようモニタを起動しておく必要があります。
   </para>
   <para>
    <command>salt-run state.orch</command>コマンドの発行後にモニタを起動した場合、実行進行状況は表示されません。
   </para>
   <para>
    次のコマンドを実行して、モニタモードを開始できます。
   </para>
<screen><prompt>root@master # </prompt>deepsea monitor</screen>
   <para>
    <command>deepsea monitor</command>コマンドで利用可能なコマンドラインオプションの詳細については、次のマニュアルページを参照してください。
   </para>
<screen><prompt>root@master # </prompt>man deepsea-monitor</screen>
  </sect2>

  <sect2 xml:id="deepsea-cli-standalone">
   <title>DeepSea CLI: スタンドアロンモード</title>
   <para>
    スタンドアロンモードでは、DeepSea CLIを使用してDeepSeaステージを実行し、その実行をリアルタイムで表示できます。
   </para>
   <para>
    DeepSea CLIからDeepSeaステージを実行するコマンドは次の形式です。
   </para>
<screen><prompt>root@master # </prompt>deepsea stage run <replaceable>stage-name</replaceable></screen>
   <para>
    ここで、<replaceable>stage-name</replaceable>は、Saltオーケストレーション状態ファイルが参照される方法に対応します。たとえば、ステージ<emphasis role="bold"/>「deploy」は、<filename>/srv/salt/ceph/stage/deploy</filename>にあるディレクトリに対応しており、「ceph.stage.deploy」<emphasis role="bold"/>として参照されます。
   </para>
   <para>
    このコマンドは、DeepSeaステージ(またはDeepSeaオーケストレーション状態ファイル)を実行するSaltベースのコマンドの代わりになります。
   </para>
   <para>
    コマンド<command>deepsea stage run ceph.stage.0</command>は、<command>salt-run state.orch ceph.stage.0</command>と同等です。
   </para>
   <para>
    <command>deepsea stage run</command>コマンドで受け付けられる利用可能なコマンドラインオプションの詳細については、次のマニュアルページを参照してください。
   </para>
<screen><prompt>root@master # </prompt>man deepsea-stage run</screen>
   <para>
    次の図に、<emphasis role="underline">ステージ2</emphasis>実行時のDeepSea CLIの出力の例を示します。
   </para>
   <figure>
    <title>DeepSea CLIのステージ実行進行状況の出力</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="deepsea-cli-stage2-screenshot.png" width="70%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="deepsea-cli-stage2-screenshot.png" width="70%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <sect3 xml:id="deepsea-cli-run-alias">
    <title>DeepSea CLI <command>stage run</command>のエイリアス</title>
    <para>
     Saltの上級ユーザ向けに、DeepSeaのステージを実行するためのエイリアスもサポートされています。このエイリアスは、ステージの実行に使用するSaltコマンド(たとえば<command>salt-run state.orch <replaceable>stage-name</replaceable></command>)をDeepSea CLIのコマンドとして取ります。
    </para>
    <para>
     例:
    </para>
<screen><prompt>root@master # </prompt>deepsea salt-run state.orch <replaceable>stage-name</replaceable></screen>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="deepsea-pillar-salt-configuration">
  <title>設定とカスタマイズ</title>

  <sect2 xml:id="policy-configuration">
   <title><filename>policy.cfg</filename>ファイル</title>
   <para>
    <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>設定ファイルは、個々のクラスタノードの役割を決定するために使用されます。たとえば、どのノードがOSDとして機能し、どれがモニタノードとして機能するかを決定します。目的のクラスタセットアップを反映するには、<filename>policy.cfg</filename>を編集します。セクションの順序は任意ですが、古い行の内容の一致するキーは、追加した行の内容で上書きされます。
   </para>
   <tip>
    <title><filename>policy.cfg</filename>の例</title>
    <para>
     <filename>/usr/share/doc/packages/deepsea/examples/</filename>に、完全なポリシーファイルの例がいくつかあります。
    </para>
   </tip>
   <sect3 xml:id="policy-cluster-assignment">
    <title>クラスタの割り当て</title>
    <para>
     「cluster」<emphasis role="bold"/>セクションで、クラスタのミニオンを選択します。すべてのミニオンを選択することも、ミニオンをブラックリスト/ホワイトリストに入れることもできます。次に、「ceph」<emphasis role="bold"/>という名前のクラスタの例を示します。
    </para>
    <para>
     <emphasis role="bold"/>「すべての」ミニオンを含めるには、次の行を追加します。
    </para>
<screen>cluster-ceph/cluster/*.sls</screen>
    <para>
     <emphasis role="bold"/>特定のミニオンを「ホワイトリスト」に入れるには、次の行を追加します。
    </para>
<screen>cluster-ceph/cluster/abc.domain.sls</screen>
    <para>
     ミニオンのグループをホワイトリストに入れるには、シェルグロブ展開による一致を使用できます。
    </para>
<screen>cluster-ceph/cluster/mon*.sls</screen>
    <para>
     <emphasis role="bold"/>ミニオンを「ブラックリスト」に入れるには、該当のミニオンを<literal>unassigned</literal>に設定します。
    </para>
<screen>cluster-unassigned/cluster/client*.sls</screen>
   </sect3>
   <sect3 xml:id="policy-role-assignment">
    <title>役割割り当て</title>
    <para>
     このセクションでは、クラスタノードへの「役割」の割り当てについて詳しく説明します。この文脈の「役割」とは、Ceph Monitor、Object Gateway、iSCSI Gateway、openATTICなど、ノードで実行する必要があるサービスを意味します。役割は自動的には割り当てられません。<command>policy.cfg</command>に追加した役割のみが展開されます。
    </para>
    <para>
     割り当ては次のパターンに従います。
    </para>
<screen>role-<replaceable>ROLE_NAME</replaceable>/<replaceable>PATH</replaceable>/<replaceable>FILES_TO_INCLUDE</replaceable></screen>
    <para>
     ここで、各項目は次の意味と値を持ちます。
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <replaceable>ROLE_NAME</replaceable>は、「master」「admin」「mon」「mgr」「mds」「igw」「rgw」「ganesha」または「openattic」のいずれかです。
      </para>
     </listitem>
     <listitem>
      <para>
       <replaceable>PATH</replaceable>は、.slsファイルまたは.ymlファイルの相対ディレクトリパスです。.slsファイルの場合は通常<filename>cluster</filename>ですが、.ymlファイルは<filename>stack/default/ceph/minions</filename>にあります。
      </para>
     </listitem>
     <listitem>
      <para>
       <replaceable>FILES_TO_INCLUDE</replaceable>は、Salt状態ファイルまたはYAML設定ファイルです。通常は、Salt Minionのホスト名で構成されます。たとえば、<filename>ses5min2.yml</filename>です。シェルグロブ展開を使用して、さらに詳細に一致させることができます。
      </para>
     </listitem>
    </itemizedlist>
    <para>
     次に各役割の例を示します。
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis/>「master」 - このノードは、すべてのCephクラスタに対する管理キーリングを持ちます。現在のところ、1つのCephクラスタのみがサポートされます。<emphasis/>「master」役割は必須であるため、必ず次のような行を追加してください。
      </para>
<screen>role-master/cluster/master*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis/>「admin」 - このミニオンは管理キーリングを持ちます。この役割は次のように定義します。
      </para>
<screen>role-admin/cluster/abc*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis/>「mon」 - このミニオンは、Cephクラスタにモニタリングサービスを提供します。この役割には、割り当てられたミニオンのアドレスが必要です。SUSE Enterprise Storage 5から、パブリックアドレスは動的に計算されるようになり、Salt Pillarに記述する必要はなくなりました。
      </para>
<screen>role-mon/cluster/mon*.sls</screen>
      <para>
       次の例は、モニタリングの役割をミニオンのグループに割り当てます。
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis/>「mgr」 - クラスタ全体からすべての状態情報を収集するCeph Managerデーモン。Ceph Monitorの役割を展開する予定のすべてのミニオンに展開します。
      </para>
<screen>role-mgr/cluster/mgr*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis/>「mds」 - このミニオンは、CephFSをサポートするためのメタデータサービスを提供します。
      </para>
<screen>role-mds/cluster/mds*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis/>「igw」 - このミニオンは、iSCSI Gatewayとして機能します。この役割には、割り当てられたミニオンのアドレスが必要です。そのため、<filename>stack</filename>ディレクトリのファイルも含める必要があります。
      </para>
<screen>role-igw/stack/default/ceph/minions/xyz.domain.yml
role-igw/cluster/*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis/>「rgw」 - このミニオンは、Object Gatewayとして機能します。
      </para>
<screen>role-rgw/cluster/rgw*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis/>「openattic」 - このミニオンは、openATTICサーバとして機能します。
      </para>
<screen>role-openattic/cluster/openattic*.sls</screen>
      <para>
       詳細については、<xref linkend="ceph-oa"/>を参照してください。
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis/>「ganesha」 - このミニオンは、NFS Ganeshaサーバとして機能します。「ganesha」の役割には、クラスタ内で「rgw」または「mds」の役割が必要です。そうしないと、ステージ3で検証に失敗します。
      </para>
      <para>
       NFS Ganeshaを正常にインストールするには、追加の設定が必要です。NFS Ganeshaを使用する場合は、ステージ2および4を実行する前に、<xref linkend="cha-as-ganesha"/>を読んでください。ただし、NFS Ganeshaは後からインストールできます。
      </para>
      <para>
       場合によっては、NFS Ganeshaノードに対してカスタムの役割を定義すると便利です。詳細については、<xref linkend="ceph-nfsganesha-customrole"/>を参照してください。
      </para>
     </listitem>
    </itemizedlist>
    <note>
     <title>クラスタノードの複数の役割</title>
     <para>
      1つノードに複数の役割を割り当てることができます。たとえば、モニタノードにmdsの役割を割り当てることができます。
     </para>
<screen>role-mds/cluster/mon[1,2]*.sls</screen>
    </note>
   </sect3>
   <sect3 xml:id="policy-common-configuration">
    <title>共通設定</title>
    <para>
     共通設定のセクションには、「ディスカバリ(ステージ1)」<emphasis/>中に生成された設定ファイルが記述されています。これらの設定ファイルには、<literal>fsid</literal>や<literal>public_network</literal>などのパラメータが保存されています。必要なCeph共通設定を含めるには、次の行を追加します。
    </para>
<screen>config/stack/default/global.yml
config/stack/default/ceph/cluster.yml</screen>
   </sect3>
   <sect3 xml:id="policy-profile-assignment">
    <title>プロファイルの割り当て</title>
    <para>
     Cephでは、同じハードウェアで利用可能な複数のディスク設定を記述するには、1つのストレージ役割では不十分です。DeepSeaステージ1では、デフォルトのストレージプロファイルの提案が生成されます。デフォルトでは、この提案は<literal>bluestore</literal>プロファイルになっていて、特定のハードウェアセットアップで最もパフォーマンスが高い設定を提案しようとします。たとえば、オブジェクトとメタデータを1つのディスクに含める設定よりも、外部のジャーナルを使用する設定が優先されます。また、回転型ディスクよりもソリッドステートストレージが優先されます。プロファイルは、役割と同じように<filename>policy.cfg</filename>で割り当てます。
    </para>
    <para>
     デフォルトの提案は、profile-defaultディレクトリツリーにあります。これを含めるには、<filename>policy.cfg</filename>に次の2行を追加します。
    </para>
<screen>profile-default/cluster/*.sls
profile-default/stack/default/ceph/minions/*.yml</screen>
    <para>
     提案ランナを使用して、好みに合うようにカスタマイズしたストレージプロファイルを作成することもできます。このランナは、ヘルプ、ピーク、および書き込みの3つの方法を提供します。
    </para>
    <para>
     <command>salt-run proposal.help</command>は、ランナが受け取ったさまざまな引数に関するランナのヘルプテキストを出力します。
    </para>
    <para>
     <command>salt-run proposal.peek</command>は、渡された引数に従って、生成された提案を表示します。
    </para>
    <para>
     <command>salt-run proposal.populate</command>は、提案を<filename>/srv/pillar/ceph/proposals</filename>サブディレクトリに書き込みます。ストレージプロファイルに名前を付けるには、<option>name=myprofile</option>を渡します。これにより、profile-myprofileサブディレクトリが作成されます。
    </para>
    <para>
     ほかのすべての引数については、<command>salt-run proposal.help</command>の出力を参照してください。
    </para>
   </sect3>
   <sect3 xml:id="ds-profile-osd-encrypted">
    <title>暗号化されたOSDの展開</title>
    <para>
     SUSE Enterprise Storage 5から、OSDはデフォルトでFileStoreではなくBlueStoreを使用して展開されるようになりました。BlueStoreは暗号化をサポートしていますが、Ceph OSDはデフォルトでは暗号化されていない状態で展開されます。OSDの展開に使用するデータディスクとWAL/DBディスクはどちらもパーティションのないクリーンな状態であると仮定します。以前に使用したことがあるディスクの場合は、<xref linkend="deploy-wiping-disk"/>で説明する手順に従って消去してください。
    </para>
    <para>
     新しい展開で暗号化されたOSDを使用するには、<literal>proposal.populate</literal>ランナを<option>encryption=dmcrypt</option>引数と共に使用します。
    </para>
<screen>
<prompt>root@master # </prompt>salt-run proposal.populate encryption=dmcrypt
</screen>
   </sect3>
   <sect3 xml:id="deepsea-policy-filtering">
    <title>アイテムのフィルタリング</title>
    <para>
     場合によっては、*.slsグロブを使用して特定のディレクトリからすべてのファイルを含めるのは現実的ではありません。<filename>policy.cfg</filename>ファイルパーサは次のフィルタを理解します。
    </para>
    <warning>
     <title>高度な手法</title>
     <para>
      このセクションでは、上級ユーザ向けのフィルタリング手法について説明します。正しく使用しないと、フィルタリングによって問題が発生する可能性があります。たとえば、ノードの番号付けが変更される場合があります。
     </para>
    </warning>
    <variablelist>
     <varlistentry>
      <term>slice=[start:end]</term>
      <listitem>
       <para>
        sliceフィルタは、「start」<emphasis/>から「end-1」<emphasis/>までのアイテムのみを含める場合に使用します。指定したディレクトリ内のアイテムは英数字順にソートされる点に注意してください。次の行は、<filename>role-mon/cluster/</filename>サブディレクトリから3～5番目のファイルを含めます。
       </para>
<screen>role-mon/cluster/*.sls slice[3:6]</screen>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>re=regexp</term>
      <listitem>
       <para>
        正規表現フィルタは、指定した表現に一致するアイテムのみを含める場合に使用します。次に例を示します。
       </para>
<screen>role-mon/cluster/mon*.sls re=.*1[135]\.subdomainX\.sls$</screen>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="deepsea-example-policy-cfg">
    <title><filename>policy.cfg</filename>ファイルの例</title>
    <para>
     次に、基本的な<filename>policy.cfg</filename>ファイルの例を示します。
    </para>
<screen>## Cluster Assignment
cluster-ceph/cluster/*.sls <co xml:id="co-policy-1"/>

## Roles
# ADMIN
role-master/cluster/examplesesadmin.sls <co xml:id="co-policy-2"/>
role-admin/cluster/sesclient*.sls <co xml:id="co-policy-3"/>

# MON
role-mon/cluster/ses-example-[123].sls <co xml:id="co-policy-5"/>

# MGR
role-mgr/cluster/ses-example-[123].sls <co xml:id="co-policy-mgr"/>

# MDS
role-mds/cluster/ses-example-4.sls <co xml:id="co-policy-6"/>

# IGW
role-igw/stack/default/ceph/minions/ses-example-4.yml <co xml:id="co-policy-7"/>
role-igw/cluster/ses-example-4.sls <co xml:id="co-policy-10"/>

# RGW
role-rgw/cluster/ses-example-4.sls <co xml:id="co-policy-11"/>

# openATTIC
role-openattic/cluster/openattic*.sls <co xml:id="co-policy-oa"/>

# COMMON
config/stack/default/global.yml <co xml:id="co-policy-8"/>
config/stack/default/ceph/cluster.yml <co xml:id="co-policy-13"/>

## Profiles
profile-default/cluster/*.sls <co xml:id="co-policy-9"/>
profile-default/stack/default/ceph/minions/*.yml <co xml:id="co-policy-12"/></screen>
    <calloutlist>
     <callout arearefs="co-policy-1">
      <para>
       Cephクラスタにすべてのミニオンを含めるよう指定します。Cephクラスタに含めたくないミニオンがある場合は、次の行を使用します。
      </para>
<screen>cluster-unassigned/cluster/*.sls
cluster-ceph/cluster/ses-example-*.sls</screen>
      <para>
       最初の行は、すべてのミニオンを未割り当てとしてマークします。2番目の行は、「ses-example-*.sls」に一致するミニオンを上書きして、それらをCephクラスタに割り当てます。
      </para>
     </callout>
     <callout arearefs="co-policy-2">
      <para>
       「examplesesadmin」という名前のミニオンが「master」役割を持ちます。言い換えると、このミニオンはクラスタに対する管理キーを取得します。
      </para>
     </callout>
     <callout arearefs="co-policy-3">
      <para>
       「sesclient*」に一致するすべてのミニオンも管理キーを取得します。
      </para>
     </callout>
     <callout arearefs="co-policy-5">
      <para>
       「ses-example-[123]」に一致するすべてのミニオン(おそらく、ses-example-1、ses-example-2、およびses-example-3の3つのミニオン)がMONノードとして設定されます。
      </para>
     </callout>
     <callout arearefs="co-policy-mgr">
      <para>
       「ses-example-[123]」に一致するすべてのミニオン(この例ではすべてのMONノード)がMGRノードとして設定されます。
      </para>
     </callout>
     <callout arearefs="co-policy-6">
      <para>
       ミニオン「ses-example-4」がMDS役割を持ちます。
      </para>
     </callout>
     <callout arearefs="co-policy-7">
      <para>
       IGWノードのIPアドレスを確実にDeepSeaに把握させます。
      </para>
     </callout>
     <callout arearefs="co-policy-10">
      <para>
       ミニオン「ses-example-4」がIGW役割を持ちます。
      </para>
     </callout>
     <callout arearefs="co-policy-11">
      <para>
       ミニオン「ses-example-4」がRGW役割を持ちます。
      </para>
     </callout>
     <callout arearefs="co-policy-oa">
      <para>
       Cephクラスタを管理するためにopenATTICユーザインタフェースを展開するよう指定します。詳細については、<xref linkend="ceph-oa"/>を参照してください。
      </para>
     </callout>
     <callout arearefs="co-policy-8">
      <para>
       <option>fsid</option>、<option>public_network</option>などの共通設定パラメータでデフォルト値をそのまま使用することを意味します。
      </para>
     </callout>
     <callout arearefs="co-policy-13">
      <para>
       <option>fsid</option>、<option>public_network</option>などの共通設定パラメータでデフォルト値をそのまま使用することを意味します。
      </para>
     </callout>
     <callout arearefs="co-policy-9">
      <para>
       各ミニオンに対してデフォルトのハードウェアプロファイルを使用するようDeepSeaに命令します。デフォルトのハードウェアプロファイルを選択することは、すべての追加ディスク(ルートディスク以外)をOSDにすることを意味します。
      </para>
     </callout>
     <callout arearefs="co-policy-12">
      <para>
       各ミニオンに対してデフォルトのハードウェアプロファイルを使用するようDeepSeaに命令します。デフォルトのハードウェアプロファイルを選択することは、すべての追加ディスク(ルートディスク以外)をOSDにすることを意味します。
      </para>
     </callout>
    </calloutlist>
   </sect3>
  </sect2>

  <sect2>
   <title>カスタムの<filename>ceph.conf</filename>ファイル</title>
   <para>
    <filename>ceph.conf</filename>設定ファイルにカスタム設定を記述する場合は、<xref linkend="ds-custom-cephconf"/>で詳細を参照してください。
   </para>
  </sect2>
 </sect1>
</chapter>
