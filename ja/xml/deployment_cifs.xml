<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_cifs.xml" version="5.0" xml:id="cha-ses-cifs">

 <title>Sambaを介したCephFSのエクスポート</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>編集</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  このセクションでは、Samba/CIFS共有を介してCephFSをエクスポートする方法について説明します。Samba共有はWindows*クライアントで使用できます。
 </para>
 <warning>
  <title>技術プレビュー</title>
  <para>
   SUSE Enterprise Storage 5では、Samba共有のエクスポートは技術プレビューと見なされており、サポートされていません。
  </para>
 </warning>
 <sect1 xml:id="sec-ses-cifs-example">
  <title>インストールの例</title>

  <para>
   CephFSのエクスポートは技術プレビューで、サポートされていません。Samba共有をエクスポートするには、1つのクラスタノードに手動でSambaをインストールして設定する必要があります。フェールオーバー機能は、CTDBとSUSE Linux Enterprise High Availability Extensionで提供できます。
  </para>

  <procedure>
   <step>
    <para>
     クラスタ内に動作中のCephFSがすでに存在することを確認します。詳細については、<xref linkend="cha-ceph-as-cephfs"/>を参照してください。
    </para>
   </step>
   <step>
    <para>
     Salt Master上にSambaゲートウェイに固有のキーリングを作成して、Sambaゲートウェイノードにコピーします。
    </para>
<screen><prompt>root@master # </prompt><command>ceph</command> auth get-or-create client.samba.gw mon 'allow r' \
    osd 'allow *' mds 'allow *' -o ceph.client.samba.gw.keyring
<prompt>root@master # </prompt><command>scp</command> ceph.client.samba.gw.keyring <replaceable>SAMBA_NODE</replaceable>:/etc/ceph/</screen>
    <para>
     <replaceable>SAMBA_NODE</replaceable>は、Sambaゲートウェイノードの名前に置き換えます。
    </para>
   </step>
   <step>
    <para>
     Sambaゲートウェイノードで次の手順を実行します。SambaゲートウェイノードにSambaデーモンをインストールします。
    </para>
<screen><prompt>root # </prompt><command>zypper</command> in samba samba-ceph</screen>
   </step>
   <step>
    <para>
     <filename>/etc/samba/smb.conf</filename>を編集して次のセクションを追加します。
    </para>
<screen>[<replaceable>SHARE_NAME</replaceable>]
        path = /
        vfs objects = ceph
        ceph:config_file = /etc/ceph/ceph.conf
        ceph: user_id = samba.gw
        read only = no</screen>
   </step>
   <step>
    <para>
     Sambaデーモンを起動して有効にします。
    </para>
<screen><prompt>root # </prompt><command>systemctl</command> start smb.service
<prompt>root # </prompt><command>systemctl</command> enable smb.service
<prompt>root # </prompt><command>systemctl</command> start nmb.service
<prompt>root # </prompt><command>systemctl</command> enable nmb.service</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-ses-cifs-ha">
  <title>高可用性のための環境設定</title>

  <para>
   このセクションでは、Sambaサーバの2ノード高可用性設定の方法について例を使って説明します。このセットアップでは、SUSE Linux Enterprise High Availability Extensionが必要です。これら2つのノードは、<systemitem class="domainname">earth</systemitem> (<systemitem class="ipaddress">192.168.1.1</systemitem>)および<systemitem class="domainname">mars</systemitem> (<systemitem class="ipaddress">192.168.1.2</systemitem>)という名前です。
  </para>

  <para>
   SUSE Linux Enterprise High Availability Extensionの詳細については、<link xlink:href="https://www.suse.com/documentation/sle-ha-12/"/>を参照してください。
  </para>

  <para>
   さらに、2つの浮動仮想IPアドレスにより、実行している物理ノードがどれであれ、クライアントからの該当サービスへの接続が可能になります。Hawk2でのクラスタ管理には<systemitem class="ipaddress">192.168.1.10</systemitem>を使用し、CIFSエクスポートには<systemitem class="ipaddress">192.168.2.1</systemitem>を排他的に使用します。これにより、後で簡単にセキュリティ制約を適用できます。
  </para>

  <para>
   次の手順では、インストールの例について説明します。詳細については、<link xlink:href="https://www.suse.com/documentation/sle-ha-12/install-quick/data/install-quick.html"/>を参照してください。
  </para>

  <procedure xml:id="proc-sec-ses-cifs-ha">
   <step>
    <para>
     Salt Master上にSambaゲートウェイに固有のキーリングを作成して、両方のノードにコピーします。
    </para>
<screen><prompt>root@master # </prompt><command>ceph</command> auth get-or-create client.samba.gw mon 'allow r' \
    osd 'allow *' mds 'allow *' -o ceph.client.samba.gw.keyring
<prompt>root@master # </prompt><command>scp</command> ceph.client.samba.gw.keyring <systemitem class="domainname">earth</systemitem>:/etc/ceph/
<prompt>root@master # </prompt><command>scp</command> ceph.client.samba.gw.keyring <systemitem class="domainname">mars</systemitem>:/etc/ceph/</screen>
   </step>
   <step>
    <para>
     <systemitem class="domainname">earth</systemitem>および<systemitem class="domainname">mars</systemitem>を、Sambaサービスをホストするように準備します。
    </para>
    <substeps>
     <step>
      <para>
       次のパッケージがインストールされていることを確認してから進んでください: 
       <package>ctdb</package>、 <package>tdb-tools</package>、および
       <package>samba</package> (smbおよびnmbリソースに必要)。
      </para>
<screen><prompt>root # </prompt><command>zypper</command> in ctdb tdb-tools samba samba-ceph</screen>
     </step>
     <step>
      <para>
       <literal>ctdb</literal>、<literal>smb</literal>、および<literal>nmb</literal>の各サービスを停止して無効にします。
      </para>
<screen><prompt>root # </prompt><command>systemctl</command> disable ctdb
<prompt>root # </prompt><command>systemctl</command> disable smb
<prompt>root # </prompt><command>systemctl</command> disable nmb
<prompt>root # </prompt><command>systemctl</command> stop smb
<prompt>root # </prompt><command>systemctl</command> stop nmb</screen>
     </step>
     <step>
      <para>
       すべてのノードのファイアウォールのポート<literal>4379</literal>を開きます。これは、CTDBが他のクラスタノードと通信するために必要です。
      </para>
     </step>
     <step>
      <para>
       共有ファイルシステムにCTDBロックのディレクトリを作成します。
      </para>
<screen><prompt>root # </prompt><command>mkdir</command> -p /srv/samba/</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     <systemitem class="domainname">earth</systemitem>上にSambaの設定ファイルを作成します。これらは、後で自動的に<systemitem class="domainname">mars</systemitem>に同期されます。
    </para>
    <substeps>
     <step>
      <para>
       <filename>/etc/ctdb/nodes</filename>に、クラスタ内の各ノードの全プライベートIPアドレスを含むすべてのノードを挿入します。
      </para>
<screen>192.168.1.1
192.168.1.2</screen>
     </step>
     <step>
      <para>
       Sambaを設定します。<literal>/etc/samba/smb.conf</literal>の<filename>[global]</filename>セクションに次の行を追加します。「CTDB-SERVER」の代わりに、選択したホスト名を使用します(クラスタ内のすべてのノードは、この名前を持つ1つの大きなノードとして表示されます)。
      </para>
<screen>[global]
    netbios name = CTDB-SERVER
    clustering = yes
    idmap config * : backend = tdb2
    passdb backend = tdbsam
    ctdbd socket = /var/lib/ctdb/ctdb.socket</screen>
      <para>
       <command>csync2</command>の詳細については、<link xlink:href="https://www.suse.com/documentation/sle-ha-12/singlehtml/book_sleha/book_sleha.html#pro.ha.installation.setup.csync2.start"/>を参照してください。
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     SUSE Linux Enterprise High Availabilityクラスタをインストールして起動します。
    </para>
    <substeps>
     <step>
      <para>
       <systemitem class="domainname">earth</systemitem>および<systemitem class="domainname">mars</systemitem>でSUSE Linux Enterprise High Availability Extensionを登録します。
      </para>
<screen><prompt>root@earth # </prompt><command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable></screen>
<screen><prompt>root@mars # </prompt><command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable></screen>
     </step>
     <step>
      <para>
       両方のノードに <package>ha-cluster-bootstrap</package> をインストールします。
      </para>
<screen><prompt>root@earth # </prompt><command>zypper</command> in ha-cluster-bootstrap</screen>
<screen><prompt>root@mars # </prompt><command>zypper</command> in ha-cluster-bootstrap</screen>
     </step>
     <step>
      <para>
       <systemitem class="domainname">earth</systemitem>でクラスタを初期化します。
      </para>
<screen>
<prompt>root@earth # </prompt><command>ha-cluster-init</command>
      </screen>
     </step>
     <step>
      <para>
       <systemitem class="domainname">mars</systemitem>をクラスタに参加させます。
      </para>
<screen>
<prompt>root@mars # </prompt><command>ha-cluster-join</command> -c earth
      </screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     クラスタの状態を確認します。クラスタに2つのノードが追加されたことがわかります。
    </para>
<screen><prompt>root@earth # </prompt><command>crm</command> status
2 nodes configured
1 resource configured

Online: [ earth mars ]

Full list of resources:

 admin-ip       (ocf::heartbeat:IPaddr2):       Started earth</screen>
   </step>
   <step>
    <para>
     <systemitem class="domainname">earth</systemitem>で次のコマンドを実行して、CTDBリソースを設定します。
    </para>

<screen><prompt>root@earth # </prompt><command>crm</command> configure
<prompt>crm(live)configure# </prompt><command>primitive</command> ctdb ocf:heartbeat:CTDB params \
    ctdb_manages_winbind="false" \
    ctdb_manages_samba="false" \
    ctdb_recovery_lock="!/usr/lib64/ctdb/ctdb_mutex_ceph_rados_helper
        ceph client.samba.gw cephfs_metadata ctdb-mutex"
    ctdb_socket="/var/lib/ctdb/ctdb.socket" \
        op monitor interval="10" timeout="20" \
        op start interval="0" timeout="90" \
        op stop interval="0" timeout="100"
<prompt>crm(live)configure# </prompt><command>primitive</command> nmb systemd:nmb \
    op start timeout="60" interval="0" \
    op stop timeout="60" interval="0" \
    op monitor interval="60" timeout="60"
<prompt>crm(live)configure# </prompt><command>primitive</command> smb systemd:smb \
    op start timeout="60" interval="0" \
    op stop timeout="60" interval="0" \
    op monitor interval="60" timeout="60"
<prompt>crm(live)configure# </prompt><command>group</command> g-ctdb ctdb nmb smb
<prompt>crm(live)configure# </prompt><command>clone</command> cl-ctdb g-ctdb meta interleave="true"
<prompt>crm(live)configure# </prompt><command>commit</command></screen>
    <para>
     設定オプション<literal>ctdb_recovery_lock</literal>のバイナリ<command>/usr/lib64/ctdb/ctdb_mutex_ceph_rados_helper</command>には、パラメータ<replaceable>CLUSTER_NAME</replaceable>
     <replaceable>CEPHX_USER</replaceable> <replaceable>CEPH_POOL</replaceable>
     <replaceable>CEPHX_USER</replaceable>がこの順序で指定されています。
    </para>
   </step>
   <step>
    <para>
     クラスタ対応のIPアドレスを追加します。
    </para>
<screen><prompt>crm(live)configure# </prompt><command>primitive</command> ip ocf:heartbeat:IPaddr2 params ip=192.168.2.1 \
    unique_clone_address="true" \
    op monitor interval="60" \
    meta resource-stickiness="0"
<prompt>crm(live)configure# </prompt><command>clone</command> cl-ip ip \
    meta interleave="true" clone-node-max="2" globally-unique="true"
<prompt>crm(live)configure# </prompt><command>colocation</command> col-with-ctdb 0: cl-ip cl-ctdb
<prompt>crm(live)configure# </prompt><command>order</command> o-with-ctdb 0: cl-ip cl-ctdb
<prompt>crm(live)configure# </prompt><command>commit</command></screen>
    <para>
     <literal>unique_clone_address</literal>が<literal>true</literal>に設定されている場合、IPaddr2リソースエージェントはクローンIDを指定のアドレスに追加し、3つの異なるIPアドレスを設定します。これらは通常必要とされませんが、負荷分散に役立ちます。この項目の詳細については、<link xlink:href="https://www.suse.com/documentation/sle-ha-12/book_sleha/data/cha_ha_lb.html"/>を参照してください。
    </para>
   </step>
   <step>
    <para>
     結果を確認します。
    </para>
<screen><prompt>root@earth # </prompt><command>crm</command> status
Clone Set: base-clone [dlm]
     Started: [ factory-1 ]
     Stopped: [ factory-0 ]
 Clone Set: cl-ctdb [g-ctdb]
     Started: [ factory-1 ]
     Started: [ factory-0 ]
 Clone Set: cl-ip [ip] (unique)
     ip:0       (ocf:heartbeat:IPaddr2):       Started factory-0
     ip:1       (ocf:heartbeat:IPaddr2):       Started factory-1</screen>
   </step>
   <step>
    <para>
     クライアントコンピュータからテストを行います。次のコマンドをLinuxクライアントで実行して、システムからファイルをコピーしたり、システムにファイルをコピーできるかどうか確認します。
    </para>
<screen><prompt>root # </prompt><command>smbclient</command> <option>//192.168.2.1/myshare</option></screen>
   </step>
  </procedure>
 </sect1>
</chapter>
