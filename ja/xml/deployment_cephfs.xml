<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_cephfs.xml" version="5.0" xml:id="cha-ceph-as-cephfs">

 <title>CephFSのインストール</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>編集</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  Ceph File System (CephFS)は、Ceph Storage Clusterを使用してデータを保存するPOSIX互換ファイルシステムです。CephFSは、Ceph Block Deviceと同じクラスタシステム、CephオブジェクトストレージとS3およびSwift API、またはネイティブのバインディング(<systemitem>librados</systemitem>)を使用します。
 </para>
 <para>
  CephFSを使用するには、動作しているCeph Storage Clusterと、動作している「Ceph Metadata Server」<emphasis/>が少なくとも1つ必要です。
 </para>
 <sect1 xml:id="ceph-cephfs-limitations">
  <title>CephFSでサポートされるシナリオとガイド</title>

  <para>
   SUSE Enterprise StorageでSUSEは、CephFSのスケールアウトと分散コンポーネントを使用するさまざまなシナリオを正式にサポートしました。このエントリでは、ハード制限について説明し、推奨される使用事例のガイドを提供します。
  </para>

  <para>
   サポートされるCephFSの展開は、次の要件を満足する必要があります。
  </para>

  <itemizedlist>
   <listitem>
    <para>
     1つ以上のMetadata Server。SUSEでは、MDSの役割を持つノードを複数展開することをお勧めします。そのうち1つだけが「アクティブ」になり、残りは「パッシブ」になります。クライアントからCephFSをマウントする際は、必ず<command>mount</command>コマンドですべてのMDSノードを指定するようにしてください。
    </para>
   </listitem>
   <listitem>
    <para>
     CephFSスナップショットは無効になっており(デフォルト)、このバージョンではサポートされません。
    </para>
   </listitem>
   <listitem>
    <para>
     クライアントは、<literal>cephfs</literal>カーネルモジュールドライバを使用する、SUSE Linux Enterprise Server 12 SP2またはSP3ベースです。FUSEモジュールはサポートされません。
    </para>
   </listitem>
   <listitem>
    <para>
     CephFSのクォータはSUSE Enterprise Storageではサポートされません。クォータのサポートはFUSEクライアントにのみ実装されるためです。
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="http://docs.ceph.com/docs/jewel/cephfs/file-layouts/"/>で説明されているように、CephFSはファイルレイアウトの変更をサポートします。ただし、ファイルシステムがいずれかのクライアントによってマウントされている場合は、既存のCephFSファイルシステムに新しいデータプールを追加することはできません(<literal>ceph mds add_data_pool</literal>)。新しいデータプールはファイルシステムがアンマウントされているときにのみ追加できます。
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="ceph-cephfs-mds">
  <title>Ceph Metadata Server</title>

  <para>
   Ceph MDS (メタデータサーバ)は、CephFSのメタデータを保存します。Ceph Block Device とCephオブジェクトストレージはMDSを「使用しません」<emphasis/>。MDSにより、POSIXファイルシステムのユーザは、Ceph Storage Clusterに多大な負荷を掛けることなく基本的なコマンド(<command>ls</command>や<command>find</command>など)を実行できます。
  </para>

  <sect2 xml:id="ceph-cephfs-mdf-add">
   <title>Metadata Serverの追加</title>
   <para>
    MDSは、最初のクラスタの展開プロセス中に展開することも(<xref linkend="ceph-install-stack"/>を参照)、すでに展開済みのクラスタに追加することもできます(<xref linkend="salt-adding-nodes"/>を参照)。
   </para>
   <para>
    MDSの展開後、MDSを展開したサーバのファイアウォール設定で<literal>Ceph OSD/MDS</literal>サービスを許可します。<literal>yast</literal>を起動し、<menuchoice> <guimenu>Security and Users (セキュリティとユーザ)</guimenu> <guimenu>Firewall (ファイアウォール)</guimenu> <guimenu>Allowed Services (許可されたサービス)</guimenu> </menuchoice>へ移動して、<guimenu>Service to Allow (許可するサービス)</guimenu>ドロップダウンメニューで<guimenu>Ceph OSD/MDS</guimenu>を選択します。Ceph MDSノードに完全なトラフィックが許可されていない場合、ほかの操作が正常に機能してもファイルシステムのマウントは失敗します。
   </para>
  </sect2>

  <sect2 xml:id="ceph-cephfs-mds-config">
   <title>Metadata Serverの設定</title>
   <para>
    <filename>ceph.conf</filename>設定ファイルに関連オプションを挿入することによって、MDSの動作を微調整できます。
   </para>
   <variablelist>
    <title>MDSのキャッシュサイズ</title>
    <varlistentry>
     <term><option>mds cache memory limit</option>
     </term>
     <listitem>
      <para>
       MDSがキャッシュに強制するソフトメモリ制限(バイト単位)。管理者は、古い<option>mds cache size</option>設定ではなく、このオプションを使用する必要があります。デフォルトは1GBです。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>mds cache reservation</option>
     </term>
     <listitem>
      <para>
       MDSキャッシュが維持するキャッシュ予約(メモリまたはiノード単位)。MDSは、予約の修正を始める場合、キャッシュサイズが縮小して予約が復元されるまで、クライアントの状態を取り消します。デフォルトは0.05です。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    MDS関連の設定オプションの詳しいリストについては、<link xlink:href="http://docs.ceph.com/docs/master/cephfs/mds-config-ref/"/>を参照してください。
   </para>
   <para>
    MDSのジャーナラ関連の設定オプションの詳しいリストについては、<link xlink:href="http://docs.ceph.com/docs/master/cephfs/journaler/"/>を参照してください。
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs">
  <title>CephFS</title>

  <para>
   正常なCeph Storage Clusterと1つ以上のCeph Metadata Serverが用意できたら、Ceph File Systemを作成してマウントできます。クライアントがネットワークに接続されていて、適切な認証キーリングを持っていることを確認します。
  </para>

  <sect2 xml:id="ceph-cephfs-cephfs-create">
   <title>CephFSの作成</title>
   <para>
    CephFSには、「データ」<emphasis/>と「メタデータ」<emphasis/>にそれぞれ1つずつ、少なくとも2つのRADOSプールが必要です。これらのプールを設定する際には、次の点を考慮する必要があります。
   </para>
   <itemizedlist>
    <listitem>
     <para>
      メタデータプールに他より高いレプリケーションレベルを使用する。このプールのデータが失われると、ファイルシステム全体がアクセス不可能になるおそれがあるためです。
     </para>
    </listitem>
    <listitem>
     <para>
      メタデータプールのSSDに他よりレイテンシの低いストレージを使用する。これによって、クライアント上でのファイルシステム操作の体感レイテンシが向上するためです。
     </para>
    </listitem>
   </itemizedlist>
   <para>
    <filename>policy.cfg</filename>で<literal>role-mds</literal>を割り当てると、必要なプールは自動的に作成されます。パフォーマンスを手動で調整する場合、Metadata Serverを設定する前に、プール<literal>cephfs_data</literal>および<literal>cephfs_metadata</literal>を手動で作成できます。これらのプールがすでに存在する場合、DeepSeaはプールを作成しません。
   </para>
   <para>
    プールの管理の詳細については、<xref linkend="ceph-pools"/>を参照してください。
   </para>
   <para>
    2つの必須のプール(たとえば「cephfs_data」と「cephfs_metadata」)をCephFSで使用するためにデフォルト設定で作成するには、次のコマンドを実行します。
   </para>
<screen><prompt>root # </prompt>ceph osd pool create cephfs_data <replaceable>pg_num</replaceable>
<prompt>root # </prompt>ceph osd pool create cephfs_metadata <replaceable>pg_num</replaceable></screen>
   <para>
    複製プールの代わりにECプールを使用できます。ECプールは、パフォーマンス要件が低く、ランダムアクセスが少ない用途でのみ使用することをお勧めします。たとえば、クラウドストレージやバックアップ、アーカイブなどです。ECプール上のCephFSでBlueStoreを有効にする必要があります。また、プールに<literal>allow_ec_overwrite</literal>オプションが設定されている必要があります。このオプションは、<command>ceph osd pool set ec_pool allow_ec_overwrites true</command>を実行して設定できます。
   </para>
   <para>
    イレージャコーディングでは、ファイルシステムの操作に、特に細かい更新によって多大なオーバーヘッドが追加されます。このオーバーヘッドは、イレージャコーディングを耐障害性メカニズムとして使用する場合につきものです。このペナルティは、ストレージ領域へのオーバーヘッドが大幅に削減されることのトレードオフです。
   </para>
   <para>
    プールが作成されたら、<command>ceph fs new</command>コマンドを使用してファイルシステムを有効にできます。
   </para>
<screen><prompt>root # </prompt>ceph fs new <replaceable>fs_name</replaceable> <replaceable>metadata_pool_name</replaceable> <replaceable>data_pool_name</replaceable></screen>
   <para>
    次に例を示します。
   </para>
<screen><prompt>root # </prompt>ceph fs new cephfs cephfs_metadata cephfs_data</screen>
   <para>
    ファイルシステムが作成されたかどうかを確認するには、利用可能なすべてのCephFSを一覧にします。
   </para>
<screen><prompt>root # </prompt><command>ceph</command> <option>fs ls</option>
 name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]</screen>
   <para>
    ファイルシステムが作成されている場合、MDSを「アクティブ」<emphasis/>状態にすることができます。たとえば、1つのMDSシステムの場合、次のようになります。
   </para>
<screen><prompt>root # </prompt><command>ceph</command> <option>mds stat</option>
e5: 1/1/1 up</screen>
   <tip>
    <title>その他のトピック</title>
    <para>
     マウント、アンマウント、CephFSの高度な設定など、特定のタスクの詳細情報は、<xref linkend="cha-ceph-cephfs"/>に記載されています。
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-cephfs-multimds">
   <title>MDSのクラスタサイズ</title>
   <para>
    1つのCephFSインスタンスに複数のアクティブなMDSデーモンでサービスを提供できます。CephFSインスタンスに割り当てられているすべてのアクティブなMDSデーモンは、デーモン間でシステムのディレクトリツリーを分散して同時クライアントの負荷を分散します。アクティブなMDSデーモンをCephFSインスタンスに追加するには、スペアのスタンバイが必要です。追加のデーモンを起動するか、既存のスタンバイインスタンスを使用します。
   </para>
   <para>
    次のコマンドは、アクティブなMDSデーモンとパッシブなデーモンの現在の数を表示します。
   </para>
<screen><prompt>root # </prompt>ceph mds stat</screen>
   <para>
    次のコマンドは、1つのファイルシステムインスタンスでアクティブなMDSの数を2に設定します。
   </para>
<screen><prompt>root # </prompt>ceph fs set <replaceable>fs_name</replaceable> max_mds 2</screen>
   <para>
    更新前にMDSクラスタを縮小するには、2つの手順が必要です。最初に、<option>max_mds</option>を設定し、1つのインスタンスだけが残るようにします。
   </para>
<screen><prompt>root # </prompt>ceph fs set <replaceable>fs_name</replaceable> max_mds 1</screen>
   <para>
    その後、他のアクティブなMDSデーモンを明示的に無効にします。
   </para>
<screen><prompt>root # </prompt>ceph mds deactivate <replaceable>fs_name</replaceable>:<replaceable>rank</replaceable></screen>
   <para>
    ここで、<replaceable>rank</replaceable>は、ファイルシステムインスタンスのアクティブなMDSデーモンの数で、0～<option>max_mds</option>-1の範囲になります。詳細については、<link xlink:href="http://docs.ceph.com/docs/luminous/cephfs/multimds/"/>を参照してください。
   </para>
  </sect2>

  <sect2 xml:id="ceph-cephfs-multimds-updates">
   <title>MDSクラスタと更新</title>
   <para>
    Cephの更新中に、ファイルシステムインスタンスの機能フラグが変更されることがあります(通常、新機能の追加によって発生します)。互換性のないデーモン(古いバージョンなど)は、互換性のない機能セットでは機能できず、起動を拒否します。つまり、1つのデーモンを更新して再起動すると、まだ更新されていない他のデーモンがすべて停止して起動を拒否する可能性があります。このような理由から、Cephを更新する前に、アクティブなMDSクラスタのサイズを1に縮小して、スタンバイデーモンをすべて停止することをお勧めします。この更新手順を手動で実行するための手順は次のとおりです。
   </para>
   <procedure>
    <step>
     <para>
      <command>zypper</command>を使用してCeph関連パッケージを更新します。
     </para>
    </step>
    <step>
     <para>
      上の説明のように、アクティブなMDSクラスタを1インスタンスに減らし、他のすべてのノードで<systemitem class="daemon">systemd</systemitem>ユニットを使用してスタンバイMDSデーモンをすべて停止します。
     </para>
<screen><prompt>root # </prompt>systemctl stop ceph-mds\*.service ceph-mds.target</screen>
    </step>
    <step>
     <para>
      その後、残っている1つのMDSデーモンを再起動し、更新されたバイナリで再起動させます。
     </para>
<screen><prompt>root # </prompt>systemctl restart ceph-mds\*.service ceph-mds.target</screen>
    </step>
    <step>
     <para>
      他のすべてのMDSデーモンを再起動して、必要な<option>max_mds</option>設定を再設定します。
     </para>
<screen><prompt>root # </prompt>systemctl start ceph-mds.target</screen>
    </step>
   </procedure>
   <para>
    DeepSeaを使用している場合、
    <package>ceph</package> パッケージがステージ0～4の間に更新されていれば、DeepSeaはこの手順に従います。この手順は、クライアントがCephFSインスタンスをマウントしていてI/Oが実行中であっても行うことができます。ただし、アクティブなMDSの再起動中はI/Oが短時間一時停止するので注意してください。クライアントは自動的に回復します。
   </para>
   <para>
    MDSクラスタを更新する前に、できる限りI/O負荷を削減することをお勧めします。アイドル状態のMDSクラスタでは、この更新手順はより短時間で完了します。逆に、複数のMDSデーモンが存在する非常に負荷の高いクラスタでは、実行中のI/Oによって1つのMDSデーモンが圧迫されるのを避けるため、前もって負荷を軽減しておくことが不可欠です。
   </para>
  </sect2>
 </sect1>
</chapter>
