<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_operating_pools.xml" version="5.0" xml:id="ceph-pools">
 <title>ストレージプールの管理</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:translation>○</dm:translation>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  Cephはデータをプール内に保存します。プールは、オブジェクトを保存するための論理グループです。プールを作成せずに初めてクラスタを展開した場合、Cephはデフォルトのプールを使用してデータを保存します。プールは次の機能を提供します。
 </para>
 <itemizedlist mark="bullet" spacing="normal">
  <listitem>
   <para>
    「災害耐性」<emphasis/>: いくつのOSDに障害が発生してもデータが失われないようにするかを設定できます。複製プールの場合、これはオブジェクトに必要なコピー/レプリカの数になります。新しいプールは、デフォルトのレプリカ数が3に設定された状態で作成されます。一般的な設定では1つのオブジェクトと1つの追加コピーが保存されるため、レプリカ数を2に設定する必要があります。イレージャコーディングプールの場合、これはコーディングチャンクの数になります(すなわち、イレージャコードプロファイルで<emphasis>m=2</emphasis>)。
   </para>
  </listitem>
  <listitem>
   <para>
    「配置グループ」<emphasis/>: 複数のOSDにわたるプールにデータを保存するための内部的なデータ構造です。CephがデータをPGに保存する方法はCRUSHマップで定義します。プールの配置グループの数を設定できます。一般的な設定では、OSDあたり約100個の配置グループを使用し、大量のコンピューティングリソースを使用することなく最適なバランスを提供します。複数のプールを設定する場合は、プールとクラスタ全体の両方にとって適切な数の配置グループを設定するよう注意してください。
   </para>
  </listitem>
  <listitem>
   <para>
    「CRUSHルール」<emphasis/>: データをプールに保存する際、そのプールにマップされたCRUSHルールセットにより、CRUSHは、クラスタ内にオブジェクトとそのレプリカ(イレージャコーディングプールの場合はチャンク)を配置するためのルールを識別できます。ご使用のプールに対してカスタムCRUSHルールを作成できます。
   </para>
  </listitem>
  <listitem>
   <para>
    「スナップショット」<emphasis/>: <command>ceph osd pool mksnap</command>を使用してスナップショットを作成すると、特定のプールのスナップショットが効果的に作成されます。
   </para>
  </listitem>
  <listitem>
   <para>
    「所有権の設定」<emphasis/>: 特定のユーザIDをプールの所有者として設定できます。
   </para>
  </listitem>
 </itemizedlist>
 <para>
  データをプールに編成するために、プールを一覧、作成、および削除できます。各プールの使用量統計を表示することもできます。
 </para>
 <sect1 xml:id="ceph-pools-associate">
  <title>プールとアプリケーションの関連付け</title>

  <para>
   プールを使用する前に、プールをアプリケーションに関連付ける必要があります。CephFSで使用されるプール、またはObject Gatewayによって自動的に作成されるプールは自動的に関連付けられます。RBDで使用する予定のプールは、<command>rbd</command>ツールを使用して初期化する必要があります(詳細については、<xref linkend="ceph-rbd-commands"/>を参照してください)。
  </para>

  <para>
   それ以外の場合は、自由な形式のアプリケーション名を手動でプールに関連付けることができます。
  </para>

<screen><prompt>root # </prompt>ceph osd pool application enable <replaceable>pool_name</replaceable> <replaceable>application_name</replaceable></screen>

  <tip>
   <title>デフォルトのアプリケーション名</title>
   <para>
    アプリケーション名として、CephFSは<literal>cephfs</literal>、RADOS Block Deviceは<literal>rbd</literal>、Object Gatewayは<literal>rgw</literal>をそれぞれ使用します。
   </para>
  </tip>

  <para>
   1つのプールを複数のアプリケーションに関連付けて、各アプリケーションで専用のメタデータを使用できます。次のコマンドを使用して、指定したプールのアプリケーションのメタデータを表示できます。
  </para>

<screen><prompt>root # </prompt>ceph osd pool application get <replaceable>pool_name</replaceable></screen>
 </sect1>
 <sect1 xml:id="ceph-pools-operate">
  <title>プールの操作</title>

  <para>
   このセクションでは、プールで基本的なタスクを実行するための実用的な情報を紹介します。プールの一覧、作成、削除の方法と、プールの統計の表示方法、プールのスナップショットの管理方法を理解できます。
  </para>

  <sect2>
   <title>プールの一覧</title>
   <para>
    クラスタのプールを一覧にするには、次のコマンドを実行します。
   </para>
<screen><prompt>root # </prompt>ceph osd lspools
0 rbd, 1 photo_collection, 2 foo_pool,</screen>
  </sect2>

  <sect2 xml:id="ceph-pools-operate-add-pool">
   <title>プールの作成</title>
   <para>
    複製プールを作成するには、次のコマンドを実行します。
   </para>
<screen><prompt>root # </prompt>ceph osd pool create <replaceable>pool_name</replaceable> <replaceable>pg_num</replaceable> <replaceable>pgp_num</replaceable> replicated <replaceable>crush_ruleset_name</replaceable> \
<replaceable>expected_num_objects</replaceable></screen>
   <para>
    イレージャコーディングプールを作成するには、次のコマンドを実行します。
   </para>
<screen><prompt>root # </prompt>ceph osd pool create <replaceable>pool_name</replaceable> <replaceable>pg_num</replaceable> <replaceable>pgp_num</replaceable> erasure <replaceable>erasure_code_profile</replaceable> \
 <replaceable>crush_ruleset_name</replaceable> <replaceable>expected_num_objects</replaceable></screen>
   <para>
    OSDあたりの配置グループの制限を超える場合、<command>ceph osd pool create</command>は失敗する可能性があります。この制限はオプション<option>mon_max_pg_per_osd</option>で設定します。
   </para>
   <variablelist>
    <varlistentry>
     <term>pool_name</term>
     <listitem>
      <para>
       プールの名前。固有である必要があります。このオプションは必須です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       プールの配置グループの合計数。このオプションは必須です。デフォルト値は8です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       配置目的の配置グループの合計数。配置グループ分割シナリオ以外では、配置グループの合計数と等しい必要があります。このオプションは必須です。デフォルト値は8です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_type</term>
     <listitem>
      <para>
       プールタイプ。オブジェクトのコピーを複数保持することによってOSDの損失から回復するには「replicated」<emphasis/>、一種の汎用RAID5機能を利用するには「erasure」<emphasis/>を指定できます。複製プールの場合、必要な未加工ストレージが増えますが、Cephのすべての操作が実装されます。イレージャコーディングプールの場合、必要な未加工ストレージは減りますが、利用可能な操作のサブセットのみが実装されます。デフォルトは「replicated」です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crush_ruleset_name</term>
     <listitem>
      <para>
       このプールのCRUSHルールセットの名前。指定したルールセットが存在しない場合、複製プールの作成は-ENOENTで失敗します。ただし、複製プールは、指定した名前を持つ新しいイレージャルールセットを作成します。イレージャコーディングプールのデフォルト値は「erasure-code」です。複製プールに対しては、Ceph設定変数<option>osd_pool_default_crush_replicated_ruleset</option>を選択します。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>erasure_code_profile=profile</term>
     <listitem>
      <para>
       イレージャコーディングプール専用。イレージャコードプロファイルを使用します。<command>osd erasure-code-profile set</command>で定義した既存のプロファイルである必要があります。
      </para>
      <para>
       プールを作成する際、配置グループの数を適切な値(たとえば100)に設定します。OSDあたりの配置グループの合計数も考慮してください。配置グループは計算コストが高いため、多数の配置グループが含まれるプールが大量にあると(たとえば、50個のプールと、それぞれに100個の配置グループ)、パフォーマンスが低下します。増加に応じた効果が得られなくなるポイントは、OSDホストの能力によって異なります。
      </para>
      <para>
       プールに適した配置グループ数の計算の詳細については、「<link xlink:href="http://docs.ceph.com/docs/master/rados/operations/placement-groups/">Placement Groups</link>」を参照してください。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>expected_num_objects</term>
     <listitem>
      <para>
       このプールの想定オブジェクト数。この値を設定すると、プールの作成時にPGフォルダが分割されます。これにより、ランタイム時のフォルダ分割によるレイテンシの影響が避けられます。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2>
   <title>プールクォータの設定</title>
   <para>
    最大バイト数、またはプールあたりのオブジェクトの最大数に対してプールクォータを設定できます。
   </para>
<screen><prompt>root # </prompt>ceph osd pool set-quota <replaceable>pool-name</replaceable> <replaceable>max_objects</replaceable> <replaceable>obj-count</replaceable> <replaceable>max_bytes</replaceable> <replaceable>bytes</replaceable></screen>
   <para>
    次に例を示します。
   </para>
<screen><prompt>root # </prompt>ceph osd pool set-quota data max_objects 10000</screen>
   <para>
    クォータを削除するには、値を0に設定します。
   </para>
  </sect2>

  <sect2 xml:id="ceph-pools-operate-del-pool">
   <title>プールの削除</title>
   <warning>
    <title>プールの削除は元に戻せない</title>
    <para>
     プールには重要なデータが収められている場合があります。プールを削除すると、プール内のすべてのデータが消え、回復する方法はありません。
    </para>
   </warning>
   <para>
    誤ってプールを削除することはきわめて危険であるため、Cephには、プールの削除を防止するメカニズムが2つ実装されています。プールを削除するには、両方のメカニズムを無効にする必要があります。
   </para>
   <para>
    1つ目のメカニズムは<literal>NODELETE</literal>フラグです。各プールにこのフラグがあり、デフォルト値は「false」です。プールのこのフラグのデフォルト値を確認するには、次のコマンドを実行します。
   </para>
<screen><prompt>root # </prompt>ceph osd pool get <replaceable>pool_name</replaceable> nodelete</screen>
   <para>
    <literal>nodelete: true</literal>が出力される場合、次のコマンドを使用してフラグを変更しない限り、プールを削除できません。
   </para>
<screen>ceph osd pool set <replaceable>pool_name</replaceable> nodelete false</screen>
   <para>
    2つ目のメカニズムは、クラスタ全体の設定パラメータ<option>mon allow pool delete</option>で、デフォルトは「false」です。つまり、デフォルトではプールを削除できません。表示されるエラーメッセージは次のとおりです。
   </para>
<screen>Error EPERM: pool deletion is disabled; you must first set the
mon_allow_pool_delete config option to true before you can destroy a pool</screen>
   <para>
    この安全設定に関係なくプールを削除するには、<option>mon allow pool delete</option>を一時的に「true」に設定してプールを削除し、その後、パラメータを「false」に戻します。
   </para>
<screen><prompt>root # </prompt>ceph tell mon.* injectargs --mon-allow-pool-delete=true
<prompt>root # </prompt>ceph osd pool delete pool_name pool_name --yes-i-really-really-mean-it
<prompt>root # </prompt>ceph tell mon.* injectargs --mon-allow-pool-delete=false</screen>
   <para>
    <command>injectargs</command>コマンドを実行すると、次のメッセージが表示されます。
   </para>
<screen>injectargs:mon_allow_pool_delete = 'true' (not observed, change may require restart)</screen>
   <para>
    これは単にコマンドが正常に実行されたことを確認するものです。エラーではありません。
   </para>
   <para>
    作成したプール用に独自のルールセットとルールを作成した場合、プールが必要なくなったらルールセットとルールを削除することをお勧めします。存在しなくなったプール専用の許可を持つユーザを作成した場合は、それらのユーザの削除も検討することをお勧めします。
   </para>
  </sect2>

  <sect2>
   <title>プールの名前変更</title>
   <para>
    プールの名前を変更するには、次のコマンドを実行します。
   </para>
<screen><prompt>root # </prompt>ceph osd pool rename <replaceable>current-pool-name</replaceable> <replaceable>new-pool-name</replaceable></screen>
   <para>
    プールの名前を変更する場合に、認証ユーザ用のプールごとのケーパビリティがあるときは、そのユーザのケーパビリティを新しいプール名で更新する必要があります。
   </para>
  </sect2>

  <sect2>
   <title>プール統計の表示</title>
   <para>
    プールの使用量統計を表示するには、次のコマンドを実行します。
   </para>
<screen><prompt>root # </prompt>rados df
pool name  category  KB  objects   lones  degraded  unfound  rd  rd KB  wr  wr KB
cold-storage    -   228   1         0      0          0       0   0      1   228
data            -    1    4         0      0          0       0   0      4    4
hot-storage     -    1    2         0      0          0       15  10     5   231
metadata        -    0    0         0      0          0       0   0      0    0
pool1           -    0    0         0      0          0       0   0      0    0
rbd             -    0    0         0      0          0       0   0      0    0
total used          266268          7
total avail       27966296
total space       28232564</screen>
  </sect2>

  <sect2 xml:id="ceph-pools-values">
   <title>プールの値の設定</title>
   <para>
    プールに値を設定するには、次のコマンドを実行します。
   </para>
<screen><prompt>root # </prompt>ceph osd pool set <replaceable>pool-name</replaceable> <replaceable>key</replaceable> <replaceable>value</replaceable></screen>
   <para>
    次のキーの値を設定できます。
   </para>
   <variablelist>
    <varlistentry>
     <term>size</term>
     <listitem>
      <para>
       プール内のオブジェクトのレプリカ数を設定します。詳細については、<xref linkend="ceph-pools-options-num-of-replicas"/>を参照してください。複製プール専用です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>min_size</term>
     <listitem>
      <para>
       I/Oに必要なレプリカの最小数を設定します。詳細については、<xref linkend="ceph-pools-options-num-of-replicas"/>を参照してください。複製プール専用です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crash_replay_interval</term>
     <listitem>
      <para>
       確認済みであるもののコミットされていない要求の再生をクライアントに許可する秒数。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       プールの配置グループの数。OSDをクラスタに追加する場合、配置グループの値を増やす必要があります。詳細については、<xref linkend="storage-bp-cluster-mntc-add-pgnum"/>を参照してください。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       データ配置を計算する際に使用する配置グループの有効数。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crush_ruleset</term>
     <listitem>
      <para>
       クラスタ内のオブジェクト配置のマッピングに使用するルールセット。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hashpspool</term>
     <listitem>
      <para>
       指定したプールに対してHASHPSPOOLフラグを設定(1)または設定解除(0)します。このフラグを有効にすると、PGをOSDに効率的に分散するためにアルゴリズムが変更されます。HASHPSPOOLフラグが0に設定されたプールでこのフラグを有効にすると、クラスタは、すべてのPGをもう一度正しく配置するためにバックフィルを開始します。これはクラスタに多大なI/O負荷をかける可能性があるので、非常に負荷が高い運用クラスタでは適切な計画が必要です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nodelete</term>
     <listitem>
      <para>
       プールの削除を防止します。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nopgchange</term>
     <listitem>
      <para>
       プールの<option>pg_num</option>および<option>pgp_num</option>の変更を防止します。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nosizechange</term>
     <listitem>
      <para>
       プールのサイズの変更を防止します。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>write_fadvise_dontneed</term>
     <listitem>
      <para>
       指定したプールに対して<literal>WRITE_FADVISE_DONTNEED</literal>フラグを設定/設定解除します。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>noscrub、nodeep-scrub</term>
     <listitem>
      <para>
       I/Oの一時的な高負荷を解決するため、特定のプールに対してデータの(詳細)スクラブを無効にします。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_type</term>
     <listitem>
      <para>
       キャッシュプールのヒットセットの追跡を有効にします。詳細については、「<link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">Bloom Filter</link>」を参照してください。このオプションに設定できる値は、<literal>bloom</literal>、<literal>explicit_hash</literal>、または<literal>explicit_object</literal>です。デフォルトは<literal>bloom</literal>で、他の値はテスト専用です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_count</term>
     <listitem>
      <para>
       キャッシュプールに関して保存するヒットセットの数。値を増やすほど、<systemitem>ceph-osd</systemitem>デーモンのRAM消費量が増えます。デフォルトは<literal>0</literal>です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_period</term>
     <listitem>
      <para>
       キャッシュプールのヒットセットの期間(秒単位)。値を増やすほど、<systemitem>ceph-osd</systemitem>デーモンのRAM消費量が増えます。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_fpp</term>
     <listitem>
      <para>
       bloomヒットセットタイプの誤検知確率。詳細については、「<link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">Bloom Filter</link>」を参照してください。有効な範囲は0.0～1.0で、デフォルトは<literal>0.05</literal>です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>use_gmt_hitset</term>
     <listitem>
      <para>
       キャッシュ階層化のヒットセットを作成する際に、GMT (グリニッジ標準時)のタイムスタンプを使用するようOSDに強制します。これにより、異なるタイムゾーンにあるノードが同じ結果を返すようにします。デフォルトは<literal>1</literal>です。この値は変更できません。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_dirty_ratio</term>
     <listitem>
      <para>
       キャッシュプールに含まれる変更済みオブジェクトの割合で、この割合を超えると、キャッシュ階層化エージェントは変更済み(ダーティ)オブジェクトをバッキングストレージプールにフラッシュします。デフォルトは<literal>.4</literal>です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_dirty_high_ratio</term>
     <listitem>
      <para>
       キャッシュプールに含まれる変更済みオブジェクトの割合で、この割合を超えると、キャッシュ階層化エージェントは変更済み(ダーティ)オブジェクトをより高速なバッキングストレージプールにフラッシュします。デフォルトは<literal>.6</literal>です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_full_ratio</term>
     <listitem>
      <para>
       キャッシュプールに含まれる未変更オブジェクトの割合で、この割合を超えると、キャッシュ階層化エージェントは未変更(クリーン)オブジェクトをキャッシュプールから削除します。デフォルトは<literal>.8</literal>です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>target_max_bytes</term>
     <listitem>
      <para>
       <option>max_bytes</option>のしきい値がトリガされた場合、Cephはオブジェクトのフラッシュまたは削除を開始します。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>target_max_objects</term>
     <listitem>
      <para>
       <option>max_objects</option>のしきい値がトリガされた場合、Cephはオブジェクトのフラッシュまたは削除を開始します。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_grade_decay_rate</term>
     <listitem>
      <para>
       連続する2つの<literal>hit_set</literal>間の温度減衰率。デフォルトは<literal>20</literal>です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_search_last_n</term>
     <listitem>
      <para>
       温度を計算するために、<literal>hit_set</literal>内で最大<literal>N</literal>個の出現をカウントします。デフォルトは<literal>1</literal>です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_min_flush_age</term>
     <listitem>
      <para>
       キャッシュ階層化エージェントがオブジェクトをキャッシュプールからストレージプールへフラッシュするまでの時間(秒単位)。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_min_evict_age</term>
     <listitem>
      <para>
       キャッシュ階層化エージェントがオブジェクトをキャッシュプールから削除するまでの時間(秒単位)。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>fast_read</term>
     <listitem>
      <para>
       イレージャコーディングプールでこのフラグが有効な場合、読み込み要求は、すべてのシャードに対してサブ読み込みを発行し、クライアントの要求を実行するためにデコードする十分なシャードを受け取るまで待機します。イレージャプラグインが<emphasis>jerasure</emphasis>および<emphasis>isa</emphasis>の場合、最初の<literal>K</literal>個の応答が返された時点で、これらの応答からデコードされたデータを使用してただちにクライアントの要求が実行されます。これは、パフォーマンスを向上させるために若干のリソースを取得するのに役立ちます。現在のところ、このフラグはイレージャコーディングプールでのみサポートされます。デフォルトは<literal>0</literal>です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>scrub_min_interval</term>
     <listitem>
      <para>
       クラスタの負荷が低い場合にプールをスクラブする最小間隔(秒単位)。デフォルトの<literal>0</literal>は、Ceph設定ファイルの<option>osd_scrub_min_interval</option>の値が使用されることを意味します。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>scrub_max_interval</term>
     <listitem>
      <para>
       クラスタの負荷に関係なくプールをスクラブする最大間隔(秒単位)。デフォルトの<literal>0</literal>は、Ceph設定ファイルの<option>osd_scrub_max_interval</option>の値が使用されることを意味します。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>deep_scrub_interval</term>
     <listitem>
      <para>
       プールの「詳細」<emphasis/>スクラブの間隔(秒単位)。デフォルトの<literal>0</literal>は、Ceph設定ファイルの<option>osd_deep_scrub</option>の値が使用されることを意味します。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2>
   <title>プールの値の取得</title>
   <para>
    プールから値を取得するには、次のコマンドを実行します。
   </para>
<screen><prompt>root # </prompt>ceph osd pool get <replaceable>pool-name</replaceable> <replaceable>key</replaceable></screen>
   <para>
    <xref linkend="ceph-pools-values"/>に示すキーと、次のキーの値を取得できます。
   </para>
   <variablelist>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       プールの配置グループの数。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       データ配置を計算する際に使用する配置グループの有効数。有効な範囲は<literal>pg_num</literal>以下です。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph-pools-options-num-of-replicas">
   <title>オブジェクトレプリカの数の設定</title>
   <para>
    複製プール上のオブジェクトレプリカの数を設定するには、次のコマンドを実行します。
   </para>
<screen><prompt>root # </prompt>ceph osd pool set <replaceable>poolname</replaceable> size <replaceable>num-replicas</replaceable></screen>
   <para>
    <replaceable>num-replicas</replaceable>にはオブジェクトそのものも含まれます。たとえば、オブジェクトとそのオブジェクトの2つのコピーで合計3つのオブジェクトインスタンスが必要な場合、3を指定します。
   </para>
   <para>
    <replaceable>num-replicas</replaceable>を2に設定した場合、データのコピーは「1つ」<emphasis/>だけになります。1つのオブジェクトインスタンスが失われた場合、たとえば回復中の前回の<link xlink:href="http://ceph.com/docs/master/rados/configuration/osd-config-ref/#scrubbing">スクラブ</link>以降に、他のコピーが壊れていないことを信頼する必要があります。
   </para>
   <para>
    プールを1つのレプリカに設定することは、プール内にデータオブジェクトのインスタンスが「1つ」<emphasis/>だけ存在することを意味します。OSDに障害発生すると、データは失われます。レプリカが1つのプールの使用法としては、一時データを短時間保存することが考えられます。
   </para>
   <para>
    1つのプールにレプリカを4つ以上設定しても信頼性の向上はごくわずかで、まれな事例にしか適さない場合があります。レプリカを増やすほど、オブジェクトのコピーを保存するために必要なディスク領域が増えることを覚えておいてください。最高のデータセキュリティが必要な場合は、イレージャコーディングプールを使用することをお勧めします。詳細については、<xref linkend="cha-ceph-erasure"/>を参照してください。
   </para>
   <warning>
    <title>3つ以上のレプリカを推奨</title>
    <para>
     2つだけのレプリカは使用しないことを強くお勧めします。一方のOSDに障害が発生した場合、回復中の高いワークロードによって2つ目のOSDにもほぼ確実に障害が発生します。
    </para>
   </warning>
   <para>
    次に例を示します。
   </para>
<screen><prompt>root # </prompt>ceph osd pool set data size 3</screen>
   <para>
    各プールに対してこのコマンドを実行できます。
   </para>
   <note>
    <para>
     1つのオブジェクトが、機能低下モードにおいてレプリカが<literal>pool size</literal>未満の状態でI/Oを受け付ける場合があります。I/Oに必要なレプリカの最小数を設定するには、<literal>min_size</literal>設定を使用する必要があります。次に例を示します。
    </para>
<screen><prompt>root # </prompt>ceph osd pool set data min_size 2</screen>
    <para>
     これにより、データプール内のオブジェクトはレプリカが<literal>min_size</literal>未満の場合、I/Oを受け取らなくなります。
    </para>
   </note>
  </sect2>

  <sect2>
   <title>オブジェクトレプリカの数の取得</title>
   <para>
    オブジェクトレプリカの数を取得するには、次のコマンドを実行します。
   </para>
<screen><prompt>root # </prompt>ceph osd dump | grep 'replicated size'</screen>
   <para>
    <literal>replicated size</literal>属性が強調表示された状態でプールが一覧にされます。デフォルトでは、Cephはオブジェクトのレプリカを2つ作成します(合計で3つのコピー、またはサイズ3)。
   </para>
  </sect2>

  <sect2 xml:id="storage-bp-cluster-mntc-add-pgnum">
   <title>配置グループの数の増加</title>
   <para>
    新しいプールを作成する際、プールの配置グループの数を指定します(<xref linkend="ceph-pools-operate-add-pool"/>を参照してください)。パフォーマンスおよびデータの持続性の理由から、通常、クラスタに別のOSDを追加した後、配置グループの数も増やす必要があります。OSDおよびMonitorノードには、配置グループごとに常にメモリ、ネットワーク、およびCPUが必要で、回復時にはそれ以上の量が必要です。したがって、配置グループの数を最小限に抑えると、リソースの量を大幅に節約できます。
   </para>
   <warning>
    <title>高すぎる<option>pg_num</option>の値</title>
    <para>
     プールの<option>pg_num</option>の値を変更すると、配置グループの新しい数が許可されている制限を超えることがあります。次に例を示します。
    </para>
<screen><prompt>root # </prompt>ceph osd pool set rbd pg_num 4096
 Error E2BIG: specified pg_num 3500 is too large (creating 4096 new PGs \
 on ~64 OSDs exceeds per-OSD max of 32)</screen>
    <para>
     この制限は、配置グループが極端に分割されるのを防ぐもので、<option>mon_osd_max_split_count</option>の値から派生されます。
    </para>
   </warning>
   <para>
    サイズを変更したクラスタの配置グループに適した新しい数を判断するのは複雑なタスクです。1つの方法は、クラスタのパフォーマンスが最適な状態になるまで、配置グループの数を連続的に増やしていく方法です。新しく増やした配置グループ数を判断するには、<option>mon_osd_max_split_count</option>パラメータの値を取得して、それを配置グループの現在の数に追加する必要があります。基本的な考え方を理解するため、次のスクリプトを見てください。
   </para>
<screen><prompt>cephadm &gt; </prompt>max_inc=`ceph daemon mon.a config get mon_osd_max_split_count 2&gt;&amp;1 \
  | tr -d '\n ' | sed 's/.*"\([[:digit:]]\+\)".*/\1/'`
<prompt>cephadm &gt; </prompt>pg_num=`ceph osd pool get rbd pg_num | cut -f2 -d: | tr -d ' '`
<prompt>cephadm &gt; </prompt>echo "current pg_num value: $pg_num, max increment: $max_inc"
<prompt>cephadm &gt; </prompt>next_pg_num="$(($pg_num+$max_inc))"
<prompt>cephadm &gt; </prompt>echo "allowed increment of pg_num: $next_pg_num"</screen>
   <para>
    配置グループの次の数がわかったら、次のコマンドを使用して数を増やします。
   </para>
<screen><prompt>root # </prompt>ceph osd pool set <replaceable>pool_name</replaceable> pg_num <replaceable>next_pg_num</replaceable></screen>
  </sect2>

  <sect2 xml:id="storage-bp-cluster-mntc-add-pool">
   <title>プールの追加</title>
   <para>
    初めてクラスタを展開した後、Cephはデフォルトのプールを使用してデータを保存します。後で、次のコマンドを使用して新しいプールを作成できます。
   </para>
<screen><prompt>root # </prompt>ceph osd pool create</screen>
   <para>
    クラスタプールの作成の詳細については、<xref linkend="ceph-pools-operate-add-pool"/>を参照してください。
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="pools-migration">
  <title>プールのマイグレーション</title>

  <para>
   プールを作成する際(<xref linkend="ceph-pools-operate-add-pool"/>を参照)、プールのタイプや配置グループの数など、初期パラメータを指定する必要があります。後日、プールをデータに配置した後にこれらのパラメータを変更する場合、プールのデータを、ご使用の展開環境に適したパラメータが設定された別のプールに移行する必要があります。
  </para>

  <para>
   プールのマイグレーションには複数の方法があります。「キャッシュ層」<emphasis/>を使用することをお勧めします。この方法は透過的で、クラスタのダウンタイムを短縮し、すべてのプールのデータが重複するのを避けられます。
  </para>

  <sect2 xml:id="pool-migrate-cache-tier">
   <title>キャッシュ層を使用した移行</title>
   <para>
    原理は単純で、移行する必要があるプールを逆の順番でキャッシュ層に含めます。キャッシュ層の詳細については、<xref linkend="cha-ceph-tiered"/>を参照してください。たとえば、「testpool」という名前の複製プールをイレージャコーディングプールに移行するには、次の手順に従います。
   </para>
   <procedure>
    <title>複製プールからイレージャコーディングプールへの移行</title>
    <step>
     <para>
      「newpool」という名前の新しいイレージャコーディングプールを作成します。
     </para>
<screen>
<prompt>root@minion &gt; </prompt>ceph osd pool create newpool 4096 4096 erasure default
</screen>
     <para>
      これでプールが2つできました。データが入った元の複製プール「testpool」と、新しい空のイレージャコーディングプール「newpool」です。
     </para>
     <figure>
      <title>マイグレーション前のプール</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate1.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate1.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      キャッシュ層をセットアップして、複製プール「testpool」をキャッシュプールとしてセットアップします。
     </para>
<screen>
<prompt>root@minion &gt; </prompt>ceph osd tier add newpool testpool --force-nonempty
<prompt>root@minion &gt; </prompt>ceph osd cache-mode testpool forward
</screen>
     <para>
      これ以降、新しいオブジェクトはすべて新しいプールに作成されます。
     </para>
     <figure>
      <title>キャッシュ層のセットアップ</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate2.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate2.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      キャッシュプールからすべてのオブジェクトを新しいプールに強制的に移動します。
     </para>
<screen>
<prompt>root@minion &gt; </prompt>rados -p testpool cache-flush-evict-all
</screen>
     <figure>
      <title>データのフラッシュ</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate3.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate3.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      すべてのクライアントを新しいプールに切り替えます。すべてのデータが新しいイレージャコーディングプールにフラッシュされるまでは、オーバーレイを指定してオブジェクトが古いプールで検索されるようにする必要があります。
     </para>
<screen>
<prompt>root@minion &gt; </prompt>ceph osd tier set-overlay newpool testpool
</screen>
     <para>
      このオーバーレイにより、すべての操作が古い複製プール「testpool」に転送されます。
     </para>
     <figure>
      <title>オーバーレイの設定</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate4.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate4.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      これで、新しいプールのオブジェクトにアクセスするようすべてのクライアントを切り替えることができます。
     </para>
    </step>
    <step>
     <para>
      すべてのデータがイレージャコーディングプール「newpool」に移行されたら、オーバーレイと古いキャッシュプール「testpool」を削除します。
     </para>
<screen>
<prompt>root@minion &gt; </prompt>ceph osd tier remove-overlay newpool
<prompt>root@minion &gt; </prompt>ceph osd tier remove newpool testpool
</screen>
     <figure>
      <title>マイグレーションの完了</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate5.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate5.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="cha-ceph-snapshots-pool">
  <title>プールのスナップショット</title>

  <para>
   プールのスナップショットは、Cephのプール全体の状態のスナップショットです。プールのスナップショットにより、プールの状態の履歴を保持できます。プールのサイズによっては、プールのスナップショットの作成に大量のストレージ領域が必要になる場合があります。プールのスナップショットを作成する前に、必ず関連するストレージに十分なディスク領域があることを確認してください。
  </para>

  <sect2>
   <title>プールのスナップショットの作成</title>
   <para>
    プールのスナップショットを作成するには、次のコマンドを実行します。
   </para>
<screen><prompt>root # </prompt>ceph osd pool mksnap <replaceable>pool-name</replaceable> <replaceable>snap-name</replaceable></screen>
   <para>
    次に例を示します。
   </para>
<screen><prompt>root # </prompt>ceph osd pool mksnap pool1 snapshot1
created pool pool1 snap snapshot1</screen>
  </sect2>

  <sect2>
   <title>プールのスナップショットの削除</title>
   <para>
    プールのスナップショットを削除するには、次のコマンドを実行します。
   </para>
<screen><prompt>root # </prompt>ceph osd pool rmsnap <replaceable>pool-name</replaceable> <replaceable>snap-name</replaceable></screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-ceph-pool-compression">
  <title>データ圧縮</title>

  <para>
   SUSE Enterprise Storage 5から、BlueStoreはディスク領域の節約のためにオンザフライのデータ圧縮を提供しています。
  </para>

  <sect2 xml:id="sec-ceph-pool-compression-enable">
   <title>圧縮の有効化</title>
   <para>
    次のコマンドを使用して、プールのデータ圧縮を有効にできます。
   </para>
<screen><prompt>root # </prompt><command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> ompression_algorithm snappy
<prompt>root # </prompt><command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> compression_mode aggressive</screen>
   <para>
    <replaceable>POOL_NAME</replaceable>は、圧縮を有効にするプールに置き換えてください。
   </para>
  </sect2>

  <sect2 xml:id="sec-ceph-pool-compression-options">
   <title>プール圧縮オプション</title>
   <para>
    次に、すべての圧縮設定のリストを示します。
   </para>
   <variablelist>
    <varlistentry>
     <term>compression_algorithm</term>
     <listitem>
      <para>
       値: <literal>none</literal>、<literal>zstd</literal>、<literal>snappy</literal>。デフォルト: <literal>snappy</literal>。
      </para>
      <para>
       どの圧縮アルゴリズムを使用するかは、特定の使用事例によって異なります。次に推奨事項をいくつか示します。
      </para>
      <itemizedlist>
       <listitem>
        <para>
         <literal>zlib</literal>は使用しないでください。他のアルゴリズムの方が効率的です。
        </para>
       </listitem>
       <listitem>
        <para>
         高い圧縮率が必要な場合は、<literal>zstd</literal>を使用します。小容量データを圧縮する際のCPUオーバーヘッドが高いため、BlueStoreでは<literal>zstd</literal>は推奨されないことに注意してください。
        </para>
       </listitem>
       <listitem>
        <para>
         低いCPU使用量が必要な場合は、<literal>lz4</literal>または<literal>snappy</literal>を使用します。
        </para>
       </listitem>
       <listitem>
        <para>
         クラスタのCPUとメモリの使用量に注意しながら、実際のデータのサンプルに対してこれらのアルゴリズムのベンチマークを実行します。
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_mode</term>
     <listitem>
      <para>
       値: <literal>none</literal>、<literal>aggressive</literal>、<literal>passive</literal>、<literal>force</literal>。デフォルト: <literal>none</literal>。
      </para>
      <itemizedlist>
       <listitem>
        <para>
         <literal>none</literal>: 圧縮しません。
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>passive</literal>: <literal>COMPRESSIBLE</literal>と表示されている場合、圧縮します。
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>aggressive</literal>: <literal>INCOMPRESSIBLE</literal>と表示されている場合以外、圧縮します。
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>force</literal>: 常に圧縮します。
        </para>
       </listitem>
      </itemizedlist>
      <para>
       <literal>COMPRESSIBLE</literal>または<literal>INCOMPRESSIBLE</literal>のフラグを設定する方法の詳細については、<link xlink:href="http://docs.ceph.com/docs/doc-12.2.0-major-changes/rados/api/librados/#rados_set_alloc_hint"/>を参照してください。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_required_ratio</term>
     <listitem>
      <para>
       値: 倍精度、比率= SIZE_COMPRESSED / SIZE_ORIGINAL。デフォルト: <literal>.875</literal>。
      </para>
      <para>
       この率を上回るオブジェクトは、圧縮効果が低いため圧縮状態では保存されません。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_max_blob_size</term>
     <listitem>
      <para>
       値: 符号なし整数、バイト単位のサイズ。デフォルト: <literal>0</literal>。
      </para>
      <para>
       圧縮されるオブジェクトの最大サイズ。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_min_blob_size</term>
     <listitem>
      <para>
       値: 符号なし整数、バイト単位のサイズ。デフォルト: <literal>0</literal>。
      </para>
      <para>
       圧縮されるオブジェクトの最小サイズ。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="sec-ceph-pool-bluestore-compression-options">
   <title>グローバル圧縮オプション</title>
   <para>
    次の設定オプションはCeph設定で指定でき、1つのプールだけでなくすべてのOSDに適用されます。<xref linkend="sec-ceph-pool-compression-options"/>に一覧にされているプール固有の設定が優先されます。
   </para>
   <variablelist>
    <varlistentry>
     <term>bluestore_compression_algorithm</term>
     <listitem>
      <para>
       値: <literal>none</literal>、<literal>zstd</literal>、<literal>snappy</literal>、<literal>zlib</literal>。デフォルト: <literal>snappy</literal>。
      </para>
      <para>
       どの圧縮アルゴリズムを使用するかは、特定の使用事例によって異なります。次に推奨事項をいくつか示します。
      </para>
      <itemizedlist>
       <listitem>
        <para>
         <literal>zlib</literal>は使用しないでください。他のアルゴリズムの方が効率的です。
        </para>
       </listitem>
       <listitem>
        <para>
         高い圧縮率が必要な場合は、<literal>zstd</literal>を使用します。小容量データを圧縮する際のCPUオーバーヘッドが高いため、BlueStoreでは<literal>zstd</literal>は推奨されないことに注意してください。
        </para>
       </listitem>
       <listitem>
        <para>
         低いCPU使用量が必要な場合は、<literal>lz4</literal>または<literal>snappy</literal>を使用します。
        </para>
       </listitem>
       <listitem>
        <para>
         クラスタのCPUとメモリの使用量に注意しながら、実際のデータのサンプルに対してこれらのアルゴリズムのベンチマークを実行します。
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_mode</term>
     <listitem>
      <para>
       値: <literal>none</literal>、<literal>aggressive</literal>、<literal>passive</literal>、<literal>force</literal>。デフォルト: <literal>none</literal>。
      </para>
      <itemizedlist>
       <listitem>
        <para>
         <literal>none</literal>: 圧縮しません。
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>passive</literal>: <literal>COMPRESSIBLE</literal>と表示されている場合、圧縮します。
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>aggressive</literal>: <literal>INCOMPRESSIBLE</literal>と表示されている場合以外、圧縮します。
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>force</literal>: 常に圧縮します。
        </para>
       </listitem>
      </itemizedlist>
      <para>
       <literal>COMPRESSIBLE</literal>または<literal>INCOMPRESSIBLE</literal>のフラグを設定する方法の詳細については、<link xlink:href="http://docs.ceph.com/docs/doc-12.2.0-major-changes/rados/api/librados/#rados_set_alloc_hint"/>を参照してください。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_required_ratio</term>
     <listitem>
      <para>
       値: 倍精度、比率= SIZE_COMPRESSED / SIZE_ORIGINAL。デフォルト: <literal>.875</literal>。
      </para>
      <para>
       この率を上回るオブジェクトは、圧縮効果が低いため圧縮状態では保存されません。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size</term>
     <listitem>
      <para>
       値: 符号なし整数、バイト単位のサイズ。デフォルト: <literal>0</literal>。
      </para>
      <para>
       圧縮されるオブジェクトの最小サイズ。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size</term>
     <listitem>
      <para>
       値: 符号なし整数、バイト単位のサイズ。デフォルト: <literal>0</literal>。
      </para>
      <para>
       圧縮されるオブジェクトの最大サイズ。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size_ssd</term>
     <listitem>
      <para>
       値: 符号なし整数、バイト単位のサイズ。デフォルト: <literal>8K</literal>。
      </para>
      <para>
       圧縮してソリッドステートドライブに保存されるオブジェクトの最小サイズ。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size_ssd</term>
     <listitem>
      <para>
       値: 符号なし整数、バイト単位のサイズ。デフォルト: <literal>64K</literal>。
      </para>
      <para>
       圧縮してソリッドステートドライブに保存されるオブジェクトの最大サイズ。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size_hdd</term>
     <listitem>
      <para>
       値: 符号なし整数、バイト単位のサイズ。デフォルト: <literal>128K</literal>。
      </para>
      <para>
       圧縮してハードディスクに保存されるオブジェクトの最小サイズ。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size_hdd</term>
     <listitem>
      <para>
       値: 符号なし整数、バイト単位のサイズ。デフォルト: <literal>512K</literal>。
      </para>
      <para>
       圧縮してハードディスクに保存されるオブジェクトの最大サイズ。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
</chapter>
