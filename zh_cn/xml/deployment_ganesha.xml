<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_ganesha.xml" version="5.0" xml:id="cha-as-ganesha">

 <title>安装 NFS Ganesha</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>编辑</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  使用 NFS Ganesha 可通过 NFS 访问对象网关或 CephFS。SUSE Enterprise Storage 5 支持 NFS 版本 3 和 4。NFS Ganesha 在用户空间而不是内核空间中运行，直接与对象网关或 CephFS 交互。
 </para>
 <sect1 xml:id="sec-as-ganesha-preparation">
  <title>准备</title>

  <sect2 xml:id="sec-as-ganesha-preparation-general">
   <title>一般信息</title>
   <para>
    要成功部署 NFS Ganesha，需要将 <literal>role-ganesha</literal> 添加到 <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>。有关详细信息，请参见<xref linkend="policy-configuration"/>。要使用 NFS Ganesha，还需要在 <filename>policy.cfg</filename> 中指定 <literal>role-rgw</literal> 或 <literal>role-mds</literal>。
   </para>
   <para>
    尽管可以在现有的 Ceph 节点上安装并运行 NFS Ganesha 服务器，但建议在能够访问 Ceph 集群的专用主机上运行该服务器。客户端主机通常不是集群的一部分，但需要能够通过网络访问 NFS Ganesha 服务器。
   </para>
   <para>
    要在完成初始安装后随时启用 NFS Ganesha 服务器，请将 <literal>role-ganesha</literal> 添加到 <filename>policy.cfg</filename>，并至少重新运行 DeepSea 阶段 2 和 4。有关详细信息，请参见<xref linkend="ceph-install-stack"/>。
   </para>
   <para>
    NFS Ganesha 是通过 NFS Ganesha 节点上的 <filename>/etc/ganesha/ganesha.conf</filename> 文件配置的。但是，每次执行 DeepSea 阶段 4，都会重写此文件。因此，建议编辑 Salt 使用的模板，即 Salt Master 上的 <filename>/srv/salt/ceph/ganesha/files/ganesha.conf.j2</filename> 文件。有关配置文件的详细信息，请参见<xref linkend="ceph-nfsganesha-config"/>。
   </para>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-preparation-requirements">
   <title>要求摘要</title>
   <para>
    在执行 DeepSea 阶段 2 和 4 来安装 NFS Ganesha 之前，必须满足以下要求：
   </para>
   <itemizedlist>
    <listitem>
     <para>
      至少为一个节点指定 <literal>role-ganesha</literal>。
     </para>
    </listitem>
    <listitem>
     <para>
      对于每个 Minion，只能定义一个 <literal>role-ganesha</literal>。
     </para>
    </listitem>
    <listitem>
     <para>
      NFS Ganesha 需要对象网关或 CephFS 才能正常工作。
     </para>
    </listitem>
    <listitem>
     <para>
      如果 NFS Ganesha 预期要使用对象网关来连接集群，则需要填充 Salt Master 上的 <filename>/srv/pillar/ceph/rgw.sls</filename>。
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-basic-example">
  <title>示例安装</title>

  <para>
   此过程提供了一个安装示例，该示例使用 NFS Ganesha 的对象网关和 CephFS 文件系统抽象层 (FSAL)。
  </para>

  <procedure>
   <step>
    <para>
     先执行 DeepSea 阶段 0 和 1（如果尚未这样做），然后继续执行此过程。
    </para>
<screen><prompt>root # </prompt><command>salt-run</command> state.orch ceph.stage.0
<prompt>root # </prompt><command>salt-run</command> state.orch ceph.stage.1</screen>
   </step>
   <step>
    <para>
     执行 DeepSea 的阶段 1 之后，编辑 <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> 并添加下行
    </para>
<screen>role-ganesha/cluster/<replaceable>NODENAME</replaceable></screen>
    <para>
     将 <replaceable>NODENAME</replaceable> 替换为集群中某个节点的名称。
    </para>
    <para>
     另外，请确保已指定 <literal>role-mds</literal> 和 <literal>role-rgw</literal>。
    </para>
   </step>
   <step>
    <para>
     创建文件 <filename>/srv/pillar/ceph/rgw.sls</filename> 并插入以下内容：
    </para>
<screen>rgw_configurations:
  rgw:
    users:
      - { uid: "demo", name: "Demo", email: "demo@demo.nil" }
      - { uid: "demo1", name: "Demo1", email: "demo1@demo.nil" }</screen>
    <para>
     系统稍后会将这些用户创建为对象网关用户，并生成 API 密钥。稍后，您可在对象网关节点上运行 <command>radosgw-admin user list</command> 列出所有已创建的用户，并运行 <command>radosgw-admin user info --uid=demo</command> 获取有关单个用户的详细信息。
    </para>
    <para>
     DeepSea 确保对象网关和 NFS Ganesha 都能接收 <filename>rgw.sls</filename> 的 <literal>rgw</literal> 段落中所列全部用户的身份凭证。
    </para>
    <para>
     导出的 NFS 将在文件系统的第一级别上使用这些用户名；在本示例中，将导出路径 <filename>/demo</filename> 和 <filename>/demo1</filename>。
    </para>
   </step>
   <step>
    <para>
     至少执行 DeepSea 的阶段 2 和 4。建议运行中间的阶段 3。
    </para>
<screen><prompt>root # </prompt><command>salt-run</command> state.orch ceph.stage.2
<prompt>root # </prompt><command>salt-run</command> state.orch ceph.stage.3 # optional but recommended
<prompt>root # </prompt><command>salt-run</command> state.orch ceph.stage.4</screen>
   </step>
   <step>
    <para>
     通过从客户端节点装入 NFS 共享来校验 NFS Ganesha 是否正常工作：
    </para>
<screen><prompt>root # </prompt><command>mount</command> -o sync -t nfs <replaceable>GANESHA_NODE</replaceable>:/ /mnt
<prompt>root # </prompt><command>ls</command> /mnt
cephfs  demo  demo1</screen>
    <para>
     <filename>/mnt</filename> 应包含所有已导出的路径。CephFS 和两个对象网关用户的目录应该存在。对于用户拥有的每个桶，将导出路径 <filename>/mnt/<replaceable>USERNAME</replaceable>/<replaceable>BUCKETNAME</replaceable></filename>。
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-ha-ap">
  <title>高可用性主动-被动配置</title>

  <para>
   本节提供一个示例来说明如何设置 NFS Ganesha 服务器的双节点主动-被动配置。该设置需要 SUSE Linux Enterprise High Availability Extension。两个节点分别名为 <systemitem class="domainname">earth</systemitem> 和 <systemitem class="domainname">mars</systemitem>。
  </para>

  <para>
   有关 SUSE Linux Enterprise High Availability Extension 的详细信息，请参见 <link xlink:href="https://www.suse.com/documentation/sle-ha-12/"/>。
  </para>

  <sect2 xml:id="sec-as-ganesha-ha-ap-basic">
   <title>基本安装</title>
   <para>
    在此设置中，<systemitem class="domainname">earth</systemitem> 的 IP 地址为 <systemitem class="ipaddress">192.168.1.1</systemitem>，<systemitem class="domainname">mars</systemitem> 的地址为 <systemitem class="ipaddress">192.168.1.2</systemitem>。
   </para>
   <para>
    此外，使用了两个浮动虚拟 IP 地址，这样无论服务在哪个物理节点上运行，客户端都可连接到服务。<systemitem class="ipaddress">192.168.1.10</systemitem> 用于通过 Hawk2 进行集群管理，<systemitem class="ipaddress">192.168.2.1</systemitem> 专门用于 NFS 导出项。这样，以后便可更轻松地应用安全限制。
   </para>
   <para>
    以下过程介绍示例安装。<link xlink:href="https://www.suse.com/documentation/sle-ha-12/install-quick/data/install-quick.html"/> 上提供了更多详细信息。
   </para>
   <procedure xml:id="proc-as-ganesha-ha-ap">
    <step>
     <para>
      在 Salt Master 上准备 NFS Ganesha 节点：
     </para>
     <substeps>
      <step>
       <para>
        在 Salt Master 上运行 DeepSea 阶段 0 和 1。
       </para>
<screen>
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.0
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.1
</screen>
      </step>
      <step>
       <para>
        在 <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> 中为节点 <systemitem class="domainname">earth</systemitem> 和 <systemitem class="domainname">mars</systemitem> 指定 <literal>role-ganesha</literal>：
       </para>
<screen>role-ganesha/cluster/earth*.sls
role-ganesha/cluster/mars*.sls</screen>
      </step>
      <step>
       <para>
        在 Salt Master 上运行 DeepSea 阶段 3 和 4。
       </para>
<screen><prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.3
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.4</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      在 <systemitem class="domainname">earth</systemitem> 和 <systemitem class="domainname">mars</systemitem> 上注册 SUSE Linux Enterprise High Availability Extension。
     </para>
<screen>
<prompt>root # </prompt><command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable>
</screen>
    </step>
    <step>
     <para>
      在两个节点上安装 <package>ha-cluster-bootstrap</package> ：
     </para>
<screen><prompt>root # </prompt><command>zypper</command> in ha-cluster-bootstrap</screen>
    </step>
    <step>
     <substeps>
      <step>
       <para>
        在 <systemitem class="domainname">earth</systemitem> 上初始化集群：
       </para>
<screen><prompt>root@earth # </prompt><command>ha-cluster-init</command></screen>
      </step>
      <step>
       <para>
        让 <systemitem class="domainname">mars</systemitem> 加入该集群：
       </para>
<screen><prompt>root@mars # </prompt><command>ha-cluster-join</command> -c earth</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      检查集群的状态。您应该会看到两个节点都已添加到集群中：
     </para>
<screen><prompt>root@earth # </prompt><command>crm</command> status</screen>
    </step>
    <step>
     <para>
      在这两个节点上，禁用引导时自动启动 NFS Ganesha 服务的功能：
     </para>
<screen><prompt>root # </prompt><command>systemctl</command> disable nfs-ganesha</screen>
    </step>
    <step>
     <para>
      在 <systemitem class="domainname">earth</systemitem> 上启动 <command>crm</command> 外壳：
     </para>
<screen><prompt>root@earth # </prompt><command>crm</command> configure</screen>
     <para>
      后续命令在 crm 外壳中执行。
     </para>
    </step>
    <step>
     <para>
      在 <systemitem class="domainname">earth</systemitem> 上，运行 crm 外壳来执行以下命令，以便将 NFS Ganesha 守护进程的资源配置为 systemd 资源类型的克隆：
     </para>
<screen>
<prompt>crm(live)configure# </prompt>primitive nfs-ganesha-server systemd:nfs-ganesha \
op monitor interval=30s
<prompt>crm(live)configure# </prompt>clone nfs-ganesha-clone nfs-ganesha-server meta interleave=true
<prompt>crm(live)configure# </prompt>commit
<prompt>crm(live)configure# </prompt>status
    2 nodes configured
    2 resources configured

    Online: [ earth mars ]

    Full list of resources:
         Clone Set: nfs-ganesha-clone [nfs-ganesha-server]
         Started:  [ earth mars ]</screen>
    </step>
    <step>
     <para>
      使用 crm 外壳创建一个原始 IPAddr2：
     </para>
<screen>
<prompt>crm(live)configure# </prompt>primitive ganesha-ip IPaddr2 \
params ip=192.168.2.1 cidr_netmask=24 nic=eth0 \
op monitor interval=10 timeout=20

<prompt>crm(live)# </prompt>status
Online: [ earth mars  ]
Full list of resources:
 Clone Set: nfs-ganesha-clone [nfs-ganesha-server]
     Started: [ earth mars ]
 ganesha-ip    (ocf::heartbeat:IPaddr2):    Started earth</screen>
    </step>
    <step>
     <para>
      为了在 NFS Ganesha 服务器与浮动虚拟 IP 之间建立关系，我们使用了共置和顺序约束。
     </para>
<screen>
<prompt>crm(live)configure# </prompt>colocation ganesha-ip-with-nfs-ganesha-server inf: ganesha-ip nfs-ganesha-clone
<prompt>crm(live)configure# </prompt>order ganesha-ip-after-nfs-ganesha-server Mandatory: nfs-ganesha-clone ganesha-ip
</screen>
    </step>
    <step>
     <para>
      从客户端使用 <command>mount</command> 命令，以确保完成集群设置：
     </para>
<screen><prompt>root # </prompt><command>mount</command> -t nfs -v -o sync,nfsvers=4 192.168.2.1:/ /mnt</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-ha-ap-cleanup">
   <title>清理资源</title>
   <para>
    如果其中一个节点（例如 <systemitem class="domainname">earth</systemitem>）上发生 NFS Ganesha 故障，请解决问题并清理资源。如果 NFS Ganesha 在 <systemitem class="domainname">mars</systemitem> 上发生故障，则只有在清理资源之后，该资源才能故障回复到 <systemitem class="domainname">earth</systemitem>。
   </para>
   <para>
    要清理资源，请执行以下命令：
   </para>
<screen><prompt>root@earth # </prompt><command>crm</command> resource cleanup nfs-ganesha-clone earth
<prompt>root@earth # </prompt><command>crm</command> resource cleanup ganesha-ip earth</screen>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-ha-ap-ping-resource">
   <title>设置 Ping 资源</title>
   <para>
    有时，服务器可能由于网络问题而无法访问客户端。Ping 资源可以检测并缓解此问题。配置此资源的操作是可选的。
   </para>
   <procedure>
    <step>
     <para>
      定义 ping 资源：
     </para>
<screen><prompt>crm(live)configure# </prompt>primitive ganesha-ping ocf:pacemaker:ping \
        params name=ping dampen=3s multiplier=100 host_list="<replaceable>CLIENT1</replaceable> <replaceable>CLIENT2</replaceable>" \
        op monitor interval=60 timeout=60 \
        op start interval=0 timeout=60 \
        op stop interval=0 timeout=60</screen>
     <para>
      <literal>host_list</literal> 是以空格分隔的 IP 地址列表。系统将会定期 ping 这些 IP 地址，以检查网络中断问题。如果某个客户端必须始终能够访问 NFS 服务器，请将该客户端添加到 <literal>host_list</literal>。
     </para>
    </step>
    <step>
     <para>
      创建克隆资源：
     </para>
<screen><prompt>crm(live)configure# </prompt>clone ganesha-ping-clone ganesha-ping \
        meta interleave=true</screen>
    </step>
    <step>
     <para>
      以下命令会创建 NFS Ganesha 服务的约束。当 <literal>host_list</literal> 不可访问时，此约束会强制服务转移到另一节点。
     </para>
<screen><prompt>crm(live)configure# </prompt>location nfs-ganesha-server-with-ganesha-ping
        nfs-ganesha-clone \
        rule -inf: not_defined ping or ping lte 0</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ganesha-ha-deepsea">
   <title>NFS Ganesha HA 和 DeepSea</title>
   <para>
    DeepSea 不支持配置 NFS Ganesha HA。为了防止在配置 NFS Ganesha HA 之后 DeepSea 发生故障，请从 DeepSea 阶段 4 中排除 NFS Ganesha 服务的启动和停止操作：
   </para>
   <procedure>
    <step>
     <para>
      将 <filename>/srv/salt/ceph/ganesha/default.sls</filename> 复制到 <filename>/srv/salt/ceph/ganesha/ha.sls</filename>。
     </para>
    </step>
    <step>
     <para>
      从 <filename>/srv/salt/ceph/ganesha/ha.sls</filename> 中删除 <literal>.service</literal> 项，使文件内容如下所示：
     </para>
<screen>include:
- .keyring
- .install
- .configure</screen>
    </step>
    <step>
     <para>
      将下行添加到 <filename>/srv/pillar/ceph/stack/global.yml</filename>：
     </para>
<screen>ganesha_init: ha</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-info">
  <title>更多信息</title>

  <para>
   更多信息可以在<xref linkend="cha-ceph-nfsganesha"/>中找到。
  </para>
 </sect1>
</chapter>
