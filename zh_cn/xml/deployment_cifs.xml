<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_cifs.xml" version="5.0" xml:id="cha-ses-cifs">

 <title>通过 Samba 导出 CephFS</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>编辑</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  本章介绍如何通过 Samba/CIFS 共享导出 CephFS。可在 Windows* 客户端中使用 Samba 共享。
 </para>
 <warning>
  <title>技术预览</title>
  <para>
   从 SUSE Enterprise Storage 5 开始，导出 Samba 共享被视为一项技术预览，不再受支持。
  </para>
 </warning>
 <sect1 xml:id="sec-ses-cifs-example">
  <title>示例安装</title>

  <para>
   导出 CephFS 是一项预览技术，不支持该功能。要导出 Samba 共享，需要在一个集群节点上手动安装 Samba 并对其进行配置。可以通过 CTDB 和 SUSE Linux Enterprise High Availability Extension 提供故障转移功能。
  </para>

  <procedure>
   <step>
    <para>
     请确保集群中已存在一个正常工作的 CephFS。有关详细信息，请参见<xref linkend="cha-ceph-as-cephfs"/>。
    </para>
   </step>
   <step>
    <para>
     在 Salt Master 上创建一个特定于 Samba 网关的密钥环，并将其复制到 Samba 网关节点：
    </para>
<screen><prompt>root@master # </prompt><command>ceph</command> auth get-or-create client.samba.gw mon 'allow r' \
    osd 'allow *' mds 'allow *' -o ceph.client.samba.gw.keyring
<prompt>root@master # </prompt><command>scp</command> ceph.client.samba.gw.keyring <replaceable>SAMBA_NODE</replaceable>:/etc/ceph/</screen>
    <para>
     将 <replaceable>SAMBA_NODE</replaceable> 替换为 Samba 网关节点的名称。
    </para>
   </step>
   <step>
    <para>
     在 Samba 网关节点上执行以下步骤。在 Samba 网关节点上安装 Samba 守护进程：
    </para>
<screen><prompt>root # </prompt><command>zypper</command> in samba samba-ceph</screen>
   </step>
   <step>
    <para>
     编辑 <filename>/etc/samba/smb.conf</filename> 并添加以下段落：
    </para>
<screen>[<replaceable>SHARE_NAME</replaceable>]
        path = /
        vfs objects = ceph
        ceph:config_file = /etc/ceph/ceph.conf
        ceph: user_id = samba.gw
        read only = no</screen>
   </step>
   <step>
    <para>
     启动并启用 Samba 守护进程：
    </para>
<screen><prompt>root # </prompt><command>systemctl</command> start smb.service
<prompt>root # </prompt><command>systemctl</command> enable smb.service
<prompt>root # </prompt><command>systemctl</command> start nmb.service
<prompt>root # </prompt><command>systemctl</command> enable nmb.service</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-ses-cifs-ha">
  <title>高可用性配置</title>

  <para>
   本节提供一个示例来说明如何设置 Samba 服务器的双节点高可用性配置。该设置需要 SUSE Linux Enterprise High Availability Extension。两个节点分别名为 <systemitem class="domainname">earth</systemitem> (<systemitem class="ipaddress">192.168.1.1</systemitem>) 和 <systemitem class="domainname">mars</systemitem> (<systemitem class="ipaddress">192.168.1.2</systemitem>)。
  </para>

  <para>
   有关 SUSE Linux Enterprise High Availability Extension 的详细信息，请参见 <link xlink:href="https://www.suse.com/documentation/sle-ha-12/"/>。
  </para>

  <para>
   此外，使用两个浮动虚拟 IP 地址可让客户端连接到服务，不管该服务在哪个物理节点上运行均如此。<systemitem class="ipaddress">192.168.1.10</systemitem> 用于通过 Hawk2 进行集群管理，<systemitem class="ipaddress">192.168.2.1</systemitem> 专门用于 CIFS 导出。这样，以后便可更轻松地应用安全限制。
  </para>

  <para>
   以下过程介绍示例安装。<link xlink:href="https://www.suse.com/documentation/sle-ha-12/install-quick/data/install-quick.html"/> 上提供了更多详细信息。
  </para>

  <procedure xml:id="proc-sec-ses-cifs-ha">
   <step>
    <para>
     在 Salt Master 上创建一个特定于 Samba 网关的密钥环，并将其复制到上述两个节点：
    </para>
<screen><prompt>root@master # </prompt><command>ceph</command> auth get-or-create client.samba.gw mon 'allow r' \
    osd 'allow *' mds 'allow *' -o ceph.client.samba.gw.keyring
<prompt>root@master # </prompt><command>scp</command> ceph.client.samba.gw.keyring <systemitem class="domainname">earth</systemitem>:/etc/ceph/
<prompt>root@master # </prompt><command>scp</command> ceph.client.samba.gw.keyring <systemitem class="domainname">mars</systemitem>:/etc/ceph/</screen>
   </step>
   <step>
    <para>
     准备好 <systemitem class="domainname">earth</systemitem> 和 <systemitem class="domainname">mars</systemitem>，以托管 Samba 服务：
    </para>
    <substeps>
     <step>
      <para>
       在继续下一步之前，请确保已安装以下包：
       <package>ctdb</package>、 <package>tdb-tools</package> 和 
       <package>samba</package> （smb 和 nmb 资源需要）。
      </para>
<screen><prompt>root # </prompt><command>zypper</command> in ctdb tdb-tools samba samba-ceph</screen>
     </step>
     <step>
      <para>
       确保服务 <literal>ctdb</literal>、<literal>smb</literal> 和 <literal>nmb</literal> 已停止且未启用：
      </para>
<screen><prompt>root # </prompt><command>systemctl</command> disable ctdb
<prompt>root # </prompt><command>systemctl</command> disable smb
<prompt>root # </prompt><command>systemctl</command> disable nmb
<prompt>root # </prompt><command>systemctl</command> stop smb
<prompt>root # </prompt><command>systemctl</command> stop nmb</screen>
     </step>
     <step>
      <para>
       在所有节点上打开防火墙的端口 <literal>4379</literal>。这是为了使 CTDB 能够与其他集群节点通讯。
      </para>
     </step>
     <step>
      <para>
       在共享文件系统上为 CTDB 锁定创建一个目录：
      </para>
<screen><prompt>root # </prompt><command>mkdir</command> -p /srv/samba/</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     在 <systemitem class="domainname">earth</systemitem> 上创建 Samba 的配置文件。这些文件稍后将自动同步到 <systemitem class="domainname">mars</systemitem>。
    </para>
    <substeps>
     <step>
      <para>
       在 <filename>/etc/ctdb/nodes</filename> 中插入包含集群中每个节点的所有私用 IP 地址的所有节点：
      </para>
<screen>192.168.1.1
192.168.1.2</screen>
     </step>
     <step>
      <para>
       配置 Samba。在 <filename>/etc/samba/smb.conf</filename> 的 <literal>[global]</literal> 部分中添加以下行。使用所选的主机名取代“CTDB-SERVER”（集群中的所有节点将显示为一个此名称的大节点，以方便操作）：
      </para>
<screen>[global]
    netbios name = CTDB-SERVER
    clustering = yes
    idmap config * : backend = tdb2
    passdb backend = tdbsam
    ctdbd socket = /var/lib/ctdb/ctdb.socket</screen>
      <para>
       有关 <command>csync2</command> 的详细信息，请参见 <link xlink:href="https://www.suse.com/documentation/sle-ha-12/singlehtml/book_sleha/book_sleha.html#pro.ha.installation.setup.csync2.start"/>。
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     安装并引导 SUSE Linux Enterprise High Availability 集群。
    </para>
    <substeps>
     <step>
      <para>
       在 <systemitem class="domainname">earth</systemitem> 和 <systemitem class="domainname">mars</systemitem> 上注册 SUSE Linux Enterprise High Availability Extension。
      </para>
<screen><prompt>root@earth # </prompt><command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable></screen>
<screen><prompt>root@mars # </prompt><command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable></screen>
     </step>
     <step>
      <para>
       在两个节点上安装 <package>ha-cluster-bootstrap</package> ：
      </para>
<screen><prompt>root@earth # </prompt><command>zypper</command> in ha-cluster-bootstrap</screen>
<screen><prompt>root@mars # </prompt><command>zypper</command> in ha-cluster-bootstrap</screen>
     </step>
     <step>
      <para>
       在 <systemitem class="domainname">earth</systemitem> 上初始化集群：
      </para>
<screen>
<prompt>root@earth # </prompt><command>ha-cluster-init</command>
      </screen>
     </step>
     <step>
      <para>
       让 <systemitem class="domainname">mars</systemitem> 加入该集群：
      </para>
<screen>
<prompt>root@mars # </prompt><command>ha-cluster-join</command> -c earth
      </screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     检查集群的状态。您应该会看到两个节点都已添加到集群中：
    </para>
<screen><prompt>root@earth # </prompt><command>crm</command> status
2 nodes configured
1 resource configured

Online: [ earth mars ]

Full list of resources:

 admin-ip       (ocf::heartbeat:IPaddr2):       Started earth</screen>
   </step>
   <step>
    <para>
     在 <systemitem class="domainname">earth</systemitem> 上执行以下命令，以配置 CTDB 资源：
    </para>

<screen><prompt>root@earth # </prompt><command>crm</command> configure
<prompt>crm(live)configure# </prompt><command>primitive</command> ctdb ocf:heartbeat:CTDB params \
    ctdb_manages_winbind="false" \
    ctdb_manages_samba="false" \
    ctdb_recovery_lock="!/usr/lib64/ctdb/ctdb_mutex_ceph_rados_helper
        ceph client.samba.gw cephfs_metadata ctdb-mutex"
    ctdb_socket="/var/lib/ctdb/ctdb.socket" \
        op monitor interval="10" timeout="20" \
        op start interval="0" timeout="90" \
        op stop interval="0" timeout="100"
<prompt>crm(live)configure# </prompt><command>primitive</command> nmb systemd:nmb \
    op start timeout="60" interval="0" \
    op stop timeout="60" interval="0" \
    op monitor interval="60" timeout="60"
<prompt>crm(live)configure# </prompt><command>primitive</command> smb systemd:smb \
    op start timeout="60" interval="0" \
    op stop timeout="60" interval="0" \
    op monitor interval="60" timeout="60"
<prompt>crm(live)configure# </prompt><command>group</command> g-ctdb ctdb nmb smb
<prompt>crm(live)configure# </prompt><command>clone</command> cl-ctdb g-ctdb meta interleave="true"
<prompt>crm(live)configure# </prompt><command>commit</command></screen>
    <para>
     配置选项 <literal>ctdb_recovery_lock</literal> 中的二进制文件 <command>/usr/lib64/ctdb/ctdb_mutex_ceph_rados_helper</command> 依序包含参数 <replaceable>CLUSTER_NAME</replaceable>
     <replaceable>CEPHX_USER</replaceable> <replaceable>CEPH_POOL</replaceable>
     <replaceable>CEPHX_USER</replaceable>。
    </para>
   </step>
   <step>
    <para>
     添加集群 IP 地址：
    </para>
<screen><prompt>crm(live)configure# </prompt><command>primitive</command> ip ocf:heartbeat:IPaddr2 params ip=192.168.2.1 \
    unique_clone_address="true" \
    op monitor interval="60" \
    meta resource-stickiness="0"
<prompt>crm(live)configure# </prompt><command>clone</command> cl-ip ip \
    meta interleave="true" clone-node-max="2" globally-unique="true"
<prompt>crm(live)configure# </prompt><command>colocation</command> col-with-ctdb 0: cl-ip cl-ctdb
<prompt>crm(live)configure# </prompt><command>order</command> o-with-ctdb 0: cl-ip cl-ctdb
<prompt>crm(live)configure# </prompt><command>commit</command></screen>
    <para>
     如果 <literal>unique_clone_address</literal> 设置为 <literal>true</literal>，IPaddr2 资源代理将向指定的地址添加一个克隆 ID，从而导致出现三个不同的 IP 地址。这些地址通常是不需要的，但有助于实现负载平衡。有关此主题的更多信息，请参见<link xlink:href="https://www.suse.com/documentation/sle-ha-12/book_sleha/data/cha_ha_lb.html"/>。
    </para>
   </step>
   <step>
    <para>
     检查结果：
    </para>
<screen><prompt>root@earth # </prompt><command>crm</command> status
Clone Set: base-clone [dlm]
     Started: [ factory-1 ]
     Stopped: [ factory-0 ]
 Clone Set: cl-ctdb [g-ctdb]
     Started: [ factory-1 ]
     Started: [ factory-0 ]
 Clone Set: cl-ip [ip] (unique)
     ip:0       (ocf:heartbeat:IPaddr2):       Started factory-0
     ip:1       (ocf:heartbeat:IPaddr2):       Started factory-1</screen>
   </step>
   <step>
    <para>
     从客户端计算机进行测试。在 Linux 客户端上运行以下命令，以检查能否从系统复制文件以及将文件复制到系统：
    </para>
<screen><prompt>root # </prompt><command>smbclient</command> <option>//192.168.2.1/myshare</option></screen>
   </step>
  </procedure>
 </sect1>
</chapter>
