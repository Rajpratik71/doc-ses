<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_datamgm.xml" version="5.0" xml:id="cha-storage-datamgm">
 <title>存储的数据管理</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>编辑</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  CRUSH 算法通过计算数据存储位置来确定如何存储和检索数据。使用 CRUSH，Ceph 客户端无需通过中心服务器或中介程序，即可直接与 OSD 通讯。借助算法确定的数据存储和检索方法，Ceph 可避免单一故障点、性能瓶颈和可伸缩性物理限制。
 </para>
 <para>
  CRUSH 需要获取集群的地图，它使用 CRUSH 地图以伪随机的方式在 OSD 中存储和检索数据，并以一致的方式在整个集群中分布数据。
 </para>
 <para>
  CRUSH 地图包含一个 OSD 列表、一个用于将设备聚合到物理位置的“桶”列表，以及一个告知 CRUSH 应如何在 Ceph 集群存储池中复制数据的规则列表。通过反映安装的底层物理组织，CRUSH 可对相关设备故障的潜在根源建模，从而解决故障的根源。典型的根源包括物理接近、共用电源和共用网络。通过将这些信息编码到集群地图中，CRUSH 归置策略可将对象副本分隔在不同的故障域中，同时维持所需的分布方式。例如，为了消除可能的并发故障，可能需要确保数据副本位于使用不同机架、机柜、电源、控制器和/或物理位置的设备上。
 </para>
 <para>
  部署 Ceph 集群后，将会生成默认的 CRUSH 地图。这种模式适合 Ceph 沙箱环境。但是，在部署大规模的数据集群时，强烈建议您考虑创建自定义 CRUSH 地图，因为这样做有助于管理 Ceph 集群、提高性能并确保数据安全。
 </para>
 <para>
  例如，如果某个 OSD 停机，而您需要使用现场支持或更换硬件，则 CRUSH 地图可帮助您定位到发生 OSD 故障的主机所在的物理数据中心、机房、设备排和机柜。
 </para>
 <para>
  同样，CRUSH 可以帮助您更快地确定故障。例如，如果特定机柜中的所有 OSD 同时停机，故障可能是由某个网络交换机或者机柜或网络交换机的电源所致，而不是发生在 OSD 自身上。
 </para>
 <para>
  当与故障主机关联的归置组处于降级状态时，自定义 CRUSH 地图还可帮助您确定 Ceph 存储数据冗余副本的物理位置。
 </para>
 <para>
  CRUSH 地图包括三个主要部分。
 </para>
 <itemizedlist mark="bullet" spacing="normal">
  <listitem>
   <para>
    <xref linkend="datamgm-devices" xrefstyle="select: title"/>由任何对象存储设备（即与 <systemitem>ceph-osd</systemitem> 守护进程对应的硬盘）组成。
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="datamgm-buckets" xrefstyle="select: title"/>由存储位置（例如设备排、机柜、主机等）及为其指定的权重的分层聚合组成。
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="datamgm-rules" xrefstyle="select: title"/>由桶选择方式组成。
   </para>
  </listitem>
 </itemizedlist>
 <sect1 xml:id="datamgm-devices">
  <title>设备数</title>

  <para>
   为了将归置组映射到 OSD，CRUSH 地图需要 OSD 设备（OSD 守护进程的名称）的列表。设备列表显示在 CRUSH 地图的最前面。
  </para>

<screen>#devices
device <replaceable>num</replaceable> <replaceable>osd.name</replaceable></screen>

  <para>
   例如：
  </para>

<screen>#devices
device 0 osd.0
device 1 osd.1
device 2 osd.2
device 3 osd.3</screen>

  <para>
   一般而言，一个 OSD 守护进程映射到一个磁盘。
  </para>
 </sect1>
 <sect1 xml:id="datamgm-buckets">
  <title>桶</title>

  <para>
   CRUSH 地图包含 OSD 的列表，可将这些 OSD 组织成“桶”，以便将设备聚合到物理位置。
  </para>

  <informaltable frame="none">
   <tgroup cols="3">
    <colspec colwidth="10*"/>
    <colspec colwidth="30*"/>
    <colspec colwidth="70*"/>
    <tbody>
     <row>
      <entry>
       <para>
        0
       </para>
      </entry>
      <entry>
       <para>
        OSD
       </para>
      </entry>
      <entry>
       <para>
        OSD 守护进程（osd.1、osd.2 等）。
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        1
       </para>
      </entry>
      <entry>
       <para>
        主机
       </para>
      </entry>
      <entry>
       <para>
        包含一个或多个 OSD 的主机名。
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        2
       </para>
      </entry>
      <entry>
       <para>
        机箱
       </para>
      </entry>
      <entry>
       <para>
        组成机柜的机箱。
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        3
       </para>
      </entry>
      <entry>
       <para>
        机柜
       </para>
      </entry>
      <entry>
       <para>
        计算机机柜。默认值为 <literal>unknownrack</literal>。
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        4
       </para>
      </entry>
      <entry>
       <para>
        设备排
       </para>
      </entry>
      <entry>
       <para>
        由一系列机柜组成的设备排。
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        5
       </para>
      </entry>
      <entry>
       <para>
        Pdu
       </para>
      </entry>
      <entry>
       <para>
        电源分配单元。
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        6
       </para>
      </entry>
      <entry>
       <para>
        Pod
       </para>
      </entry>
      <entry>
       <para/>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        7
       </para>
      </entry>
      <entry>
       <para>
        机房
       </para>
      </entry>
      <entry>
       <para>
        包含主机机柜和主机排的机房。
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        8
       </para>
      </entry>
      <entry>
       <para>
        数据中心
       </para>
      </entry>
      <entry>
       <para>
        包含机房的物理数据中心。
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        9
       </para>
      </entry>
      <entry>
       <para>
        地区
       </para>
      </entry>
      <entry>
       <para/>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        10
       </para>
      </entry>
      <entry>
       <para>
        根
       </para>
      </entry>
      <entry>
       <para/>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </informaltable>

  <tip>
   <para>
    可以删除这些类型，并创建自己的桶类型。
   </para>
  </tip>

  <para>
   Ceph 的部署工具可生成 CRUSH 地图，其中包含每个主机的桶，以及名为“default”的存储池（可用于默认的 <literal>rbd</literal> 池）。剩余的桶类型提供了一种存储有关节点/桶的物理位置信息的方法，当 OSD、主机或网络硬件发生故障，并且管理员需要访问物理硬件时，这种方法可大大简化集群管理工作。
  </para>

  <para>
   桶具有类型、唯一的名称（字符串）、以负整数表示的唯一 ID、相对于其项目的总容量/功能的权重、桶算法（默认为 <literal>straw</literal>）和哈希（默认为 <literal>0</literal>，反映 CRUSH 哈希 <literal>rjenkins1</literal>）。一个桶可以包含一个或多个项目。项目可由其他桶或 OSD 组成。项目可能会有一个权重来反映该项目的相对权重。
  </para>

<screen>[bucket-type] [bucket-name] {
  id [a unique negative numeric ID]
  weight [the relative capacity/capability of the item(s)]
  alg [the bucket type: uniform | list | tree | straw ]
  hash [the hash type: 0 by default]
  item [item-name] weight [weight]
}</screen>

  <para>
   下面的示例说明如何使用桶来聚合存储池，以及诸如数据中心、机房、机柜和设备排的物理位置。
  </para>

<screen>host ceph-osd-server-1 {
        id -17
        alg straw
        hash 0
        item osd.0 weight 1.00
        item osd.1 weight 1.00
}

row rack-1-row-1 {
        id -16
        alg straw
        hash 0
        item ceph-osd-server-1 weight 2.00
}

rack rack-3 {
        id -15
        alg straw
        hash 0
        item rack-3-row-1 weight 2.00
        item rack-3-row-2 weight 2.00
        item rack-3-row-3 weight 2.00
        item rack-3-row-4 weight 2.00
        item rack-3-row-5 weight 2.00
}

rack rack-2 {
        id -14
        alg straw
        hash 0
        item rack-2-row-1 weight 2.00
        item rack-2-row-2 weight 2.00
        item rack-2-row-3 weight 2.00
        item rack-2-row-4 weight 2.00
        item rack-2-row-5 weight 2.00
}

rack rack-1 {
        id -13
        alg straw
        hash 0
        item rack-1-row-1 weight 2.00
        item rack-1-row-2 weight 2.00
        item rack-1-row-3 weight 2.00
        item rack-1-row-4 weight 2.00
        item rack-1-row-5 weight 2.00
}

room server-room-1 {
        id -12
        alg straw
        hash 0
        item rack-1 weight 10.00
        item rack-2 weight 10.00
        item rack-3 weight 10.00
}

datacenter dc-1 {
        id -11
        alg straw
        hash 0
        item server-room-1 weight 30.00
        item server-room-2 weight 30.00
}

pool data {
        id -10
        alg straw
        hash 0
        item dc-1 weight 60.00
        item dc-2 weight 60.00
}</screen>
 </sect1>
 <sect1 xml:id="datamgm-rules">
  <title>规则组</title>

  <para>
   CRUSH 地图支持“CRUSH 规则”概念，这些规则确定存储池的数据归置。对于大型集群，您可能会创建许多存储池，其中每个存储池各自可能具有自己的 CRUSH 规则组和规则。默认的 CRUSH 地图对每个存储池使用一个规则，并为每个默认存储池指定一个规则组。
  </para>

  <note>
   <para>
    大多数情况下，无需修改默认规则。创建新存储池时，该存储池的默认规则组为 0。
   </para>
  </note>

  <para>
   规则采用以下格式：
  </para>

<screen>rule <replaceable>rulename</replaceable> {

        ruleset <replaceable>ruleset</replaceable>
        type <replaceable>type</replaceable>
        min_size <replaceable>min-size</replaceable>
        max_size <replaceable>max-size</replaceable>
        step <replaceable>step</replaceable>

}</screen>

  <variablelist>
   <varlistentry>
    <term>ruleset</term>
    <listitem>
     <para>
      一个整数。将规则分类，使其属于一个规则组。通过在存储池中设置规则组来激活。必须指定此选项。默认值是 <literal>0</literal>。
     </para>
     <important>
      <para>
       需要从默认值 0 开始连续递增规则组编号，否则相关的监视器可能会崩溃。
      </para>
     </important>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>type</term>
    <listitem>
     <para>
      一个字符串。描述硬盘 (replicated) 或 RAID 的规则。必须指定此选项。默认值为 <literal>replicated</literal>。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>min_size</term>
    <listitem>
     <para>
      一个整数。如果归置组创建的副本数小于此数字，CRUSH 将不选择此规则。必须指定此选项。默认值是 <literal>2</literal>。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>max_size</term>
    <listitem>
     <para>
      一个整数。如果归置组创建的副本数大于此数字，CRUSH 将不选择此规则。必须指定此选项。默认值是 <literal>10</literal>。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>step take <replaceable>bucket</replaceable>
    </term>
    <listitem>
     <para>
      采用某个桶名称，并开始在树中向下迭代。必须指定此选项。有关在树中迭代的说明，请参见<xref linkend="datamgm-rules-step-iterate"/>。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>step <replaceable>target</replaceable><replaceable>mode</replaceable><replaceable>num</replaceable> type <replaceable>bucket-type</replaceable>
    </term>
    <listitem>
     <para>
      <replaceable>target</replaceable> 可以是 <literal>choose</literal> 或 <literal>chooseleaf</literal>。如果设置为 <literal>choose</literal>，则会选择许多桶。<literal>chooseleaf</literal> 直接从桶集的每个桶的子树中选择 OSD（叶节点）。
     </para>
     <para>
      <replaceable>mode</replaceable> 可以是 <literal>firstn</literal> 或 <literal>indep</literal>。请参见<xref linkend="datamgm-rules-step-mode"/>。
     </para>
     <para>
      选择给定类型的桶的数量。其中，N 是可用选项的数量，如果 <replaceable>num</replaceable> &gt; 0 且 &lt; N，则选择该数量的桶；如果 <replaceable>num</replaceable> &lt; 0，则表示 N - <replaceable>num</replaceable>；如果 <replaceable>num</replaceable> == 0，则选择 N 个桶（全部可用）。跟在 <literal>step take</literal> 或 <literal>step choose</literal> 后使用。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>step emit</term>
    <listitem>
     <para>
      输出当前值并清空堆栈。通常在规则的末尾使用，但也可在同一规则中用来构成不同的树。跟在 <literal>step choose</literal> 后使用。
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <important>
   <para>
    要将一个或多个具有共同规则组编号的规则构成某个存储池，请将规则组编号设置为该存储池。
   </para>
  </important>

  <sect2 xml:id="datamgm-rules-step-iterate">
   <title>在节点树中迭代</title>
   <para>
    可采用节点树的形式来查看使用桶定义的结构。在此树中，桶是节点，OSD 是叶。
   </para>
   <para>
    CRUSH 地图中的规则定义如何从此树中选择 OSD。规则从某个节点开始，然后在树中向下迭代，以返回一组 OSD。无法定义需要选择哪个分支。CRUSH 算法可确保 OSD 集能够满足复制要求并均衡分布数据。
   </para>
   <para>
    使用 <literal>step take </literal> <replaceable>bucket</replaceable> 时，节点树中的迭代从给定的桶（而不是桶类型）开始。如果要返回树中所有分支上的 OSD，该桶必须是根桶。否则，以下步骤只会在子树中迭代。
   </para>
   <para>
    完成 <literal>step take</literal> 后，接下来会执行规则定义中的一个或多个 <literal>step choose</literal> 项。每个 <literal>step choose</literal> 项从前面选定的上层节点中选择定义数量的节点（或分支）。
   </para>
   <para>
    最后，使用 <literal>step emit</literal> 返回选定的 OSD。
   </para>
   <para>
    <literal>step chooseleaf</literal> 是一个便捷函数，可直接从给定桶的分支中选择 OSD。
   </para>
   <para>
    <xref linkend="datamgm-rules-step-iterate-figure"/>中提供了说明如何使用 <literal>step</literal> 在树中迭代的示例。在下面的规则定义中，橙色箭头和数字与 <literal>example1a</literal> 和 <literal>example1b</literal> 对应，蓝色箭头和数字与 <literal>example2</literal> 对应。
   </para>
   <figure xml:id="datamgm-rules-step-iterate-figure">
    <title>示例树</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="crush-step.svg" width="100%" format="SVG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="crush-step.png" width="100%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
<screen># orange arrows
rule example1a {
        ruleset 0
        type replicated
        min_size 2
        max_size 10
        # orange (1)
        step take rack1
        # orange (2)
        step choose firstn 0 host
        # orange (3)
        step choose firstn 1 osd
        step emit
}

rule example1b {
        ruleset 0
        type replicated
        min_size 2
        max_size 10
        # orange (1)
        step take rack1
        # orange (2) + (3)
        step chooseleaf firstn 0 host
        step emit
}

# blue arrows
rule example2 {
        ruleset 0
        type replicated
        min_size 2
        max_size 10
        # blue (1)
        step take room1
        # blue (2)
        step chooseleaf firstn 0 rack
        step emit
}</screen>
  </sect2>

  <sect2 xml:id="datamgm-rules-step-mode">
   <title>firstn 和 indep</title>
   <para>
    CRUSH 规则定义有故障节点或 OSD 的替换项（请参见<xref linkend="datamgm-rules"/>）。关键字 <literal>step</literal> 要求使用 <literal>firstn</literal> 或 <literal>indep</literal> 参数。<xref linkend="datamgm-rules-step-mode-indep-figure"/>提供了示例。
   </para>
   <para>
    <literal>firstn</literal> 将替换节点添加到活动节点列表的末尾。如果某个节点发生故障，其后的正常节点会移位到左侧，以填充有故障节点留下的空缺。这是<emphasis>副本池</emphasis>的默认方法，也是需要采取的方法，因为次要节点已包含所有数据，因此可立即接管主要节点的职责。
   </para>
   <para>
    <literal>indep</literal> 为每个活动节点选择固定的替换节点。替换有故障节点不会更改剩余节点的顺序。这对于<emphasis>纠删码池</emphasis>而言是所需的行为。在纠删码池中，节点上存储的数据取决于在选择节点时它所在的位置。如果节点的顺序发生变化，受影响节点上的所有数据都需要重新放置。
   </para>
   <note>
    <title>纠删池</title>
    <para>
     确保针对每个<emphasis>纠删码池</emphasis>设置使用 <literal>indep</literal> 的规则。
    </para>
   </note>
   <figure xml:id="datamgm-rules-step-mode-indep-figure">
    <title>节点替换方法</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="crush-firstn-indep.svg" width="100%" format="SVG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="crush-firstn-indep.png" width="100%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>
 </sect1>
 <sect1 xml:id="op-crush">
  <title>CRUSH 地图操作</title>

  <para>
   本节介绍基本的 CRUSH 地图操作方法，例如编辑 CRUSH 地图、更改 CRUSH 地图参数，以及添加/移动/删除 OSD。
  </para>

  <sect2>
   <title>编辑 CRUSH 地图</title>
   <para>
    要编辑现有的 CRUSH 地图，请执行以下操作：
   </para>
   <procedure>
    <step>
     <para>
      获取 CRUSH 地图。要获取集群的 CRUSH 地图，请执行以下命令：
     </para>
<screen><prompt>root # </prompt>ceph osd getcrushmap -o <replaceable>compiled-crushmap-filename</replaceable></screen>
     <para>
      Ceph 会将编译的 CRUSH 地图输出 (<option>-o</option>) 到您指定名称的文件。由于该 CRUSH 地图采用编译格式，您必须先将其反编译，然后才能对其进行编辑。
     </para>
    </step>
    <step>
     <para>
      反编译 CRUSH 地图。要反编译 CRUSH 地图，请执行以下命令：
     </para>
<screen><prompt>cephadm &gt; </prompt>crushtool -d <replaceable>compiled-crushmap-filename</replaceable> \
 -o <replaceable>decompiled-crushmap-filename</replaceable></screen>
     <para>
      Ceph 将对已编译的 CRUSH 地图进行反编译 (<option>-d</option>)，并将其输出 (<option>-o</option>) 到您指定名称的文件。
     </para>
    </step>
    <step>
     <para>
      至少编辑“设备”、“桶”和“规则”中的其中一个参数。
     </para>
    </step>
    <step>
     <para>
      编译 CRUSH 地图。要编译 CRUSH 地图，请执行以下命令：
     </para>
<screen><prompt>cephadm &gt; </prompt>crushtool -c <replaceable>decompiled-crush-map-filename</replaceable> \
 -o <replaceable>compiled-crush-map-filename</replaceable></screen>
     <para>
      Ceph 会将编译的 CRUSH 地图存储到您指定名称的文件。
     </para>
    </step>
    <step>
     <para>
      设置 CRUSH 地图。要设置集群的 CRUSH 地图，请执行以下命令：
     </para>
<screen><prompt>root # </prompt>ceph osd setcrushmap -i <replaceable>compiled-crushmap-filename</replaceable></screen>
     <para>
      Ceph 将输入您所指定文件名的已编译 CRUSH 地图，作为集群的 CRUSH 地图。
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="op-crush-addosd">
   <title>添加/移动 OSD</title>
   <para>
    要在运行中集群的 CRUSH 地图中添加或移动 OSD，请执行以下命令：
   </para>
<screen><prompt>root # </prompt>ceph osd crush set <replaceable>id_or_name</replaceable> <replaceable>weight</replaceable> root=<replaceable>pool-name</replaceable>
<replaceable>bucket-type</replaceable>=<replaceable>bucket-name</replaceable> ...</screen>
   <variablelist>
    <varlistentry>
     <term>id</term>
     <listitem>
      <para>
       一个整数。OSD 的数字 ID。必须指定此选项。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>name</term>
     <listitem>
      <para>
       一个字符串。OSD 的全名。必须指定此选项。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>weight</term>
     <listitem>
      <para>
       一个双精度值。OSD 的 CRUSH 权重。必须指定此选项。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pool</term>
     <listitem>
      <para>
       一个键/值对。默认情况下，CRUSH 层次结构包含 default 存储池作为根。必须指定此选项。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bucket-type</term>
     <listitem>
      <para>
       键/值对。可在 CRUSH 层次结构中指定 OSD 的位置。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    下面的示例将 <literal>osd.0</literal> 添加到层次结构，或移动之前某个位置的 OSD。
   </para>
<screen><prompt>root # </prompt>ceph osd crush set osd.0 1.0 root=data datacenter=dc1 room=room1 \
row=foo rack=bar host=foo-bar-1</screen>
  </sect2>

  <sect2 xml:id="op-crush-osdweight">
   <title>调整 OSD 的 CRUSH 权重</title>
   <para>
    要在运行中集群的 CRUSH 地图中调整 OSD 的 CRUSH 权重，请执行以下命令：
   </para>
<screen><prompt>root # </prompt>ceph osd crush reweight <replaceable>name</replaceable> <replaceable>weight</replaceable></screen>
   <variablelist>
    <varlistentry>
     <term>name</term>
     <listitem>
      <para>
       一个字符串。OSD 的全名。必须指定此选项。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>weight</term>
     <listitem>
      <para>
       一个双精度值。OSD 的 CRUSH 权重。必须指定此选项。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="op-crush-osdremove">
   <title>删除 OSD</title>
   <para>
    要从运行中集群的 CRUSH 地图中删除 OSD，请执行以下命令：
   </para>
<screen><prompt>root # </prompt>ceph osd crush remove <replaceable>name</replaceable></screen>
   <variablelist>
    <varlistentry>
     <term>name</term>
     <listitem>
      <para>
       一个字符串。OSD 的全名。必须指定此选项。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="op-crush-movebucket">
   <title>移动桶</title>
   <para>
    要将某个桶移到 CRUSH 地图层次结构中的不同位置，请执行以下命令：
   </para>
<screen><prompt>root # </prompt>ceph osd crush move <replaceable>bucket-name</replaceable> <replaceable>bucket-type</replaceable>=<replaceable>bucket-name</replaceable>, ...</screen>
   <variablelist>
    <varlistentry>
     <term>bucket-name</term>
     <listitem>
      <para>
       一个字符串。要移动/重新定位的桶的名称。必须指定此选项。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bucket-type</term>
     <listitem>
      <para>
       键/值对。可在 CRUSH 层次结构中指定桶的位置。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="scrubbing">
  <title>整理 (Scrub)</title>

  <para>
   除了复制对象的多个副本外，Ceph 还需通过<emphasis>整理</emphasis> (scrub) 归置组来确保数据完整性。Ceph 的整理 (scrub) 类似于在对象存储层运行 <command>fsck</command>。对于每个归置组，Ceph 都会生成一个包含所有对象的编目，并比较每个主对象及其副本，以确保不会有缺失或不匹配的对象。每天的浅层整理 (light scrub) 会检查对象大小和属性，而每周的深层整理 (deep scrub) 则会读取数据并使用校验和来确保数据完整性。
  </para>

  <para>
   整理 (scrub) 对于维护数据完整性非常重要，但该操作可能会降低性能。您可以通过调整以下设置来增加或减少整理 (scrub) 操作：
  </para>

  <variablelist>
   <varlistentry>
    <term><option>osd max scrubs</option>
    </term>
    <listitem>
     <para>
      针对 Ceph OSD 同时执行的最大整理 (scrub) 操作数量。默认值是 1。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub begin hour</option>、<option>osd scrub end hour</option>
    </term>
    <listitem>
     <para>
      按小时定义一天内可以执行整理 (scrub) 的时间段（0 到 24）。默认开始时间为 0，结束时间为 24。
     </para>
     <important>
      <para>
       如果归置组的整理 (scrub) 间隔超出 <option>osd scrub max interval</option> 设置的值，则无论您定义的整理 (scrub) 时间段为何，都将执行整理 (scrub)。
      </para>
     </important>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub during recovery</option>
    </term>
    <listitem>
     <para>
      允许恢复期间执行整理 (scrub)。如果将此选项设置为“false”，则当存在活动的恢复进程时，将禁止安排新的整理 (scrub)。已在运行的整理 (scrub) 将继续执行。此选项有助于降低忙碌集群上的负载。默认值为“true”。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub thread timeout</option>
    </term>
    <listitem>
     <para>
      整理 (scrub) 线程超时前的最长时间（以秒为单位）。默认值是 60。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub finalize thread timeout</option>
    </term>
    <listitem>
     <para>
      整理 (scrub) 完成线程超时前的最长时间（以秒为单位）。默认值为 60*10。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub load threshold</option>
    </term>
    <listitem>
     <para>
      规范化的最大负载。当系统负载（由 <literal>getloadavg()</literal> 与 <literal>online cpus</literal> 数量之比定义）高于此数字时，Ceph 将不会执行整理 (scrub)。默认值是 0.5。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub min interval</option>
    </term>
    <listitem>
     <para>
      当 Ceph 集群负载较低时整理 (scrub) Ceph OSD 的最短间隔（以秒为单位）。默认值为 60*60*24（一天一次）。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub max interval</option>
    </term>
    <listitem>
     <para>
      无论集群负载如何都整理 (scrub) Ceph OSD 的最长间隔（以秒为单位）。7*60*60*24（一周一次）。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub chunk min</option>
    </term>
    <listitem>
     <para>
      单个操作期间要整理 (scrub) 的对象存储块的最小数量。整理 (scrub) 期间，Ceph 会阻止向单个块写入数据。默认值是 5。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub chunk max</option>
    </term>
    <listitem>
     <para>
      单个操作期间要整理 (scrub) 的对象存储块的最大数量。默认值是 25。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub sleep</option>
    </term>
    <listitem>
     <para>
      整理 (scrub) 下一组存储块之前休眠的时间。增大此值会降低整个整理 (scrub) 操作的速度，但对客户端操作的影响较小。默认值是 0。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd deep scrub interval</option>
    </term>
    <listitem>
     <para>
      深层整理 (deep scrub)（完整读取所有数据）的间隔。<option>osd scrub load threshold</option> 选项不会影响此设置。默认值为 60*60*24*7（一周一次）。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub interval randomize ratio</option>
    </term>
    <listitem>
     <para>
      在安排归置组的下一次整理 (scrub) 作业时，为 <option>osd scrub min interval</option> 值增加一个随机延迟。该延迟为一个随机的值，小于 <option>osd scrub min interval</option> * <option>osd scrub interval randomized ratio</option> 所得结果。因此，该默认设置实际上是将整理 (scrub) 随机地安排在允许的时间段 [1, 1.5] * <option>osd scrub min interval</option> 内执行。默认值为 0.5
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd deep scrub stride</option>
    </term>
    <listitem>
     <para>
      执行深层整理 (deep scrub) 时读取的大小。默认值为 524288 (512 kB)。
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="op-mixed-ssd-hdd">
  <title>在同一个节点上混用 SSD 和 HDD</title>

  <para>
   有时，用户可能需要配置这样一个 Ceph 集群：在每个节点上混用 SSD 和 HDD，并将一个存储池放在速度较快的 SSD 上，将另一个存储池放在速度较慢的 HDD 上。要实现此目的，需要编辑 CRUSH 地图。
  </para>

  <para>
   默认的 CRUSH 地图采用简单的层次结构，其中，默认根包含主机，而主机包含 OSD，例如：
  </para>

<screen><prompt>root # </prompt>ceph osd tree
 ID CLASS WEIGHT   TYPE NAME      STATUS REWEIGHT PRI-AFF
 -1       83.17899 root default
 -4       23.86200     host cpach
 2   hdd  1.81898         osd.2      up  1.00000 1.00000
 3   hdd  1.81898         osd.3      up  1.00000 1.00000
 4   hdd  1.81898         osd.4      up  1.00000 1.00000
 5   hdd  1.81898         osd.5      up  1.00000 1.00000
 6   hdd  1.81898         osd.6      up  1.00000 1.00000
 7   hdd  1.81898         osd.7      up  1.00000 1.00000
 8   hdd  1.81898         osd.8      up  1.00000 1.00000
 15  hdd  1.81898         osd.15     up  1.00000 1.00000
 10  nvme 0.93100         osd.10     up  1.00000 1.00000
 0   ssd  0.93100         osd.0      up  1.00000 1.00000
 9   ssd  0.93100         osd.9      up  1.00000 1.00000 </screen>

  <para>
   这样，就无法区分磁盘类型。要将 OSD 分为 SSD 和 HDD，需在 CRUSH 地图中创建另一个层次结构：
  </para>

<screen><prompt>root # </prompt>ceph osd crush add-bucket ssd root</screen>

  <para>
   为 SSD 创建新根后，需在此根中添加主机。这意味着需要创建新的主机项。但是，由于同一个主机名不能在 CRUSH 地图中出现多次，此处使用了虚设的主机名。这些虚设的主机名无需由 DNS 解析。CRUSH 不关心主机名是什么，只需创建适当的层次结构即可。要支持虚设的主机名，<emphasis>真正</emphasis>需要进行的一项更改就是必须设置
  </para>

<screen>osd crush update on start = false</screen>

  <para>
   （在 <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/global.conf</filename> 文件中），然后运行 DeepSea 阶段 3 以分发该项更改（有关详细信息，请参见<xref linkend="ds-custom-cephconf"/>）：
  </para>

<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
</screen>

  <para>
   否则，您移动的 OSD 稍后将重设置到其在默认根中的原始位置，并且集群不会按预期方式工作。
  </para>

  <para>
   更改该设置后，请将新的虚设主机添加到 SSD 的根中：
  </para>

<screen><prompt>root # </prompt>ceph osd crush add-bucket node1-ssd host
<prompt>root # </prompt>ceph osd crush move node1-ssd root=ssd
<prompt>root # </prompt>ceph osd crush add-bucket node2-ssd host
<prompt>root # </prompt>ceph osd crush move node2-ssd root=ssd
<prompt>root # </prompt>ceph osd crush add-bucket node3-ssd host
<prompt>root # </prompt>ceph osd crush move node3-ssd root=ssd</screen>

  <para>
   最后，针对每个 SSD OSD，将 OSD 移到 SSD 的根中。在本示例中，我们假设 osd.0、osd.1 和 osd.2 实际托管在 SSD 上：
  </para>

<screen><prompt>root # </prompt>ceph osd crush add osd.0 1 root=ssd
<prompt>root # </prompt>ceph osd crush set osd.0 1 root=ssd host=node1-ssd
<prompt>root # </prompt>ceph osd crush add osd.1 1 root=ssd
<prompt>root # </prompt>ceph osd crush set osd.1 1 root=ssd host=node2-ssd
<prompt>root # </prompt>ceph osd crush add osd.2 1 root=ssd
<prompt>root # </prompt>ceph osd crush set osd.2 1 root=ssd host=node3-ssd</screen>

  <para>
   CRUSH 层次结构现在应类似下方所示：
  </para>

<screen><prompt>root # </prompt>ceph osd tree
ID WEIGHT  TYPE NAME                   UP/DOWN REWEIGHT PRIMARY-AFFINITY
-5 3.00000 root ssd
-6 1.00000     host node1-ssd
 0 1.00000         osd.0                    up  1.00000          1.00000
-7 1.00000     host node2-ssd
 1 1.00000         osd.1                    up  1.00000          1.00000
-8 1.00000     host node3-ssd
 2 1.00000         osd.2                    up  1.00000          1.00000
-1 0.11096 root default
-2 0.03699     host node1
 3 0.01849         osd.3                    up  1.00000          1.00000
 6 0.01849         osd.6                    up  1.00000          1.00000
-3 0.03699     host node2
 4 0.01849         osd.4                    up  1.00000          1.00000
 7 0.01849         osd.7                    up  1.00000          1.00000
-4 0.03699     host node3
 5 0.01849         osd.5                    up  1.00000          1.00000
 8 0.01849         osd.8                    up  1.00000          1.00000</screen>

  <para>
   现在，创建一个针对 SSD 根的 CRUSH 规则：
  </para>

<screen><prompt>root # </prompt>ceph osd crush rule create-simple ssd_replicated_ruleset ssd host</screen>

  <para>
   原始默认规则组 <option>replicated_ruleset</option>（ID 为 0）针对的是 HDD。新规则组 <option>ssd_replicated_ruleset</option>（ID 为 1）针对的是 SSD。
  </para>

  <para>
   所有现有存储池仍会使用 HDD，因为它们位于 CRUSH 地图的默认层次结构中。可以创建一个仅使用 SSD 的新存储池：
  </para>

<screen><prompt>root # </prompt>ceph osd pool create ssd-pool 64 64
<prompt>root # </prompt>ceph osd pool set ssd-pool crush_rule ssd_replicated_ruleset</screen>
 </sect1>
</chapter>
